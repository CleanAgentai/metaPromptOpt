{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Model Comparative Analysis\n",
        "\n",
        "## Comprehensive comparison of AI models vs each other and vs human raters\n",
        "\n",
        "### Models Analyzed:\n",
        "1. **GPT-4o** (OpenAI's flagship model)\n",
        "2. **GPT-3.5-turbo** (OpenAI's fast model)\n",
        "3. **Llama-3.1-8B** (Open source model)\n",
        "4. **Human Recruiter** (Professional recruiter)\n",
        "5. **Service Owner** (Domain expert)\n",
        "\n",
        "### Analysis Types:\n",
        "- **Model-to-Model:** GPT-4o vs GPT-3.5 vs Llama\n",
        "- **Model-to-Human:** Each AI vs Recruiter and Service Owner\n",
        "- **Performance:** Cost, speed, quality tradeoffs\n",
        "\n",
        "### Outputs:\n",
        "- 10 pairwise comparisons (5 raters √ó 4 pairs each)\n",
        "- Model performance dashboard\n",
        "- Deployment recommendations"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup & Data Loading"
      ],
      "metadata": {
        "id": "setup_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install pandas numpy scipy matplotlib seaborn pingouin scikit-learn -q"
      ],
      "metadata": {
        "id": "install"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import pearsonr, spearmanr, wilcoxon\n",
        "import pingouin as pg\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from typing import Dict, List, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['savefig.dpi'] = 300"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mount"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure file paths for all raters\n",
        "# UPDATE THESE PATHS TO YOUR FILES!\n",
        "\n",
        "FILE_PATHS = {\n",
        "    'gpt4o': '/content/drive/MyDrive/mvp/final_gpt-4o_TIMESTAMP.csv',\n",
        "    'gpt35': '/content/drive/MyDrive/mvp/final_gpt-3.5-turbo_TIMESTAMP.csv',\n",
        "    'llama': '/content/drive/MyDrive/mvp/finalScores20251024-000950.csv',\n",
        "    'recruiter': '/content/drive/MyDrive/mvp/candidates_v2_recruiter_graded.csv',\n",
        "    'owner': '/content/drive/MyDrive/mvp/candidates_v2_service_owner_graded.csv'\n",
        "}\n",
        "\n",
        "# Define which raters are AI vs human\n",
        "AI_MODELS = ['gpt4o', 'gpt35', 'llama']\n",
        "HUMAN_RATERS = ['recruiter', 'owner']\n",
        "ALL_RATERS = AI_MODELS + HUMAN_RATERS\n",
        "\n",
        "# Metric definitions\n",
        "METRICS = [\"cognitive_ability\", \"experience\", \"problem_solving\", \n",
        "           \"reliability\", \"professionalism\", \"communication\"]\n",
        "SCORE_COLS = [f\"{m}_score\" for m in METRICS]\n",
        "\n",
        "print(\"Configuration loaded:\")\n",
        "print(f\"  AI Models: {AI_MODELS}\")\n",
        "print(f\"  Human Raters: {HUMAN_RATERS}\")\n",
        "print(f\"  Metrics: {len(METRICS)}\")"
      ],
      "metadata": {
        "id": "config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load all data\n",
        "print(\"Loading data...\")\n",
        "\n",
        "dataframes = {}\n",
        "\n",
        "# Load AI model scores (already in correct format)\n",
        "for model in AI_MODELS:\n",
        "    if model in FILE_PATHS:\n",
        "        try:\n",
        "            df = pd.read_csv(FILE_PATHS[model])\n",
        "            dataframes[model] = df\n",
        "            print(f\"  ‚úì {model}: {len(df)} interviews\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"  ‚ö†Ô∏è {model}: File not found - will be excluded from analysis\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå {model}: Error loading - {e}\")\n",
        "\n",
        "# Load human scores (need parsing)\n",
        "METRICS_HUMAN = [\"Cognitive ability\", \"Experience\", \"Problem Solving\", \n",
        "                 \"Reliability\", \"Professionalism\", \"Communication\"]\n",
        "\n",
        "def extract_score_and_justification(df, metric_cols):\n",
        "    \"\"\"Extract numeric scores from human rating format.\"\"\"\n",
        "    out = df.copy()\n",
        "    for metric in metric_cols:\n",
        "        base = metric.lower().replace(\" \", \"_\")\n",
        "        score_col = f\"{base}_score\"\n",
        "        just_col = f\"{base}_justification\"\n",
        "        if metric in df.columns:\n",
        "            out[score_col] = df[metric].str.extract(r'(\\d+)').astype(float)\n",
        "            out[just_col] = df[metric].str.replace(r'^\\s*\\d+\\s*[‚Äì-]\\s*', '', regex=True).str.strip()\n",
        "    return out\n",
        "\n",
        "for rater in HUMAN_RATERS:\n",
        "    if rater in FILE_PATHS:\n",
        "        try:\n",
        "            df = pd.read_csv(FILE_PATHS[rater])\n",
        "            df = extract_score_and_justification(df, METRICS_HUMAN)\n",
        "            dataframes[rater] = df\n",
        "            print(f\"  ‚úì {rater}: {len(df)} interviews\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"  ‚ö†Ô∏è {rater}: File not found\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå {rater}: Error loading - {e}\")\n",
        "\n",
        "print(f\"\\n‚úì Loaded {len(dataframes)} raters successfully\")"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create unified comparison dataframe\n",
        "def create_master_comparison_df(dataframes_dict):\n",
        "    \"\"\"Merge all rater scores into one dataframe.\"\"\"\n",
        "    \n",
        "    # Start with first available rater\n",
        "    first_rater = list(dataframes_dict.keys())[0]\n",
        "    master_df = dataframes_dict[first_rater][['interview_id'] + SCORE_COLS].copy()\n",
        "    master_df.columns = ['interview_id'] + [f\"{c}_{first_rater}\" for c in SCORE_COLS]\n",
        "    \n",
        "    # Merge remaining raters\n",
        "    for rater in list(dataframes_dict.keys())[1:]:\n",
        "        df = dataframes_dict[rater][['interview_id'] + SCORE_COLS].copy()\n",
        "        df.columns = ['interview_id'] + [f\"{c}_{rater}\" for c in SCORE_COLS]\n",
        "        master_df = master_df.merge(df, on='interview_id', how='outer')\n",
        "    \n",
        "    return master_df\n",
        "\n",
        "master_df = create_master_comparison_df(dataframes)\n",
        "\n",
        "print(f\"‚úì Created master comparison dataframe\")\n",
        "print(f\"  Total interviews: {len(master_df)}\")\n",
        "print(f\"  Total columns: {len(master_df.columns)}\")\n",
        "print(f\"\\nAvailable comparisons by interview:\")\n",
        "\n",
        "for rater in dataframes.keys():\n",
        "    col = f\"{SCORE_COLS[0]}_{rater}\"\n",
        "    count = master_df[col].notna().sum()\n",
        "    print(f\"  {rater}: {count} interviews\")\n",
        "\n",
        "display(master_df.head())"
      ],
      "metadata": {
        "id": "create_master"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Inter-Rater Reliability Analysis"
      ],
      "metadata": {
        "id": "reliability_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_icc_pair(df, metric, rater1, rater2):\n",
        "    \"\"\"Calculate ICC for a pair of raters on one metric.\"\"\"\n",
        "    col1 = f\"{metric}_score_{rater1}\"\n",
        "    col2 = f\"{metric}_score_{rater2}\"\n",
        "    \n",
        "    # Prepare data\n",
        "    data = []\n",
        "    for idx, row in df.iterrows():\n",
        "        if pd.notna(row[col1]) and pd.notna(row[col2]):\n",
        "            data.append({'interview': idx, 'rater': 'rater1', 'score': row[col1]})\n",
        "            data.append({'interview': idx, 'rater': 'rater2', 'score': row[col2]})\n",
        "    \n",
        "    if len(data) < 4:\n",
        "        return np.nan, np.nan, np.nan, 0\n",
        "    \n",
        "    df_long = pd.DataFrame(data)\n",
        "    n = len(data) // 2\n",
        "    \n",
        "    try:\n",
        "        icc_result = pg.intraclass_corr(data=df_long, targets='interview', \n",
        "                                        raters='rater', ratings='score')\n",
        "        icc_row = icc_result[icc_result['Type'] == 'ICC2']\n",
        "        if len(icc_row) > 0:\n",
        "            return (icc_row['ICC'].values[0], \n",
        "                   icc_row['CI95%'].values[0][0], \n",
        "                   icc_row['CI95%'].values[0][1],\n",
        "                   n)\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    return np.nan, np.nan, np.nan, n\n",
        "\n",
        "def calculate_agreement_rate(df, metric, rater1, rater2, threshold=1):\n",
        "    \"\"\"Calculate % agreement within threshold.\"\"\"\n",
        "    col1 = f\"{metric}_score_{rater1}\"\n",
        "    col2 = f\"{metric}_score_{rater2}\"\n",
        "    \n",
        "    mask = df[col1].notna() & df[col2].notna()\n",
        "    if mask.sum() == 0:\n",
        "        return np.nan\n",
        "    \n",
        "    diffs = (df.loc[mask, col1] - df.loc[mask, col2]).abs()\n",
        "    return (diffs <= threshold).sum() / len(diffs) * 100\n",
        "\n",
        "# Calculate all pairwise comparisons\n",
        "print(\"=\" * 80)\n",
        "print(\"COMPREHENSIVE INTER-RATER RELIABILITY MATRIX\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "reliability_results = []\n",
        "\n",
        "raters = list(dataframes.keys())\n",
        "for i, rater1 in enumerate(raters):\n",
        "    for j, rater2 in enumerate(raters):\n",
        "        if j > i:  # Only upper triangle\n",
        "            print(f\"\\n{rater1.upper()} vs {rater2.upper()}:\")\n",
        "            print(\"-\" * 80)\n",
        "            \n",
        "            for metric in METRICS:\n",
        "                icc, icc_low, icc_high, n = calculate_icc_pair(master_df, metric, rater1, rater2)\n",
        "                agree_exact = calculate_agreement_rate(master_df, metric, rater1, rater2, 0)\n",
        "                agree_1 = calculate_agreement_rate(master_df, metric, rater1, rater2, 1)\n",
        "                \n",
        "                if not np.isnan(icc):\n",
        "                    print(f\"  {metric.replace('_', ' ').title():<25} \"\n",
        "                          f\"ICC={icc:.3f} [{icc_low:.3f}-{icc_high:.3f}]  \"\n",
        "                          f\"Agree(¬±1)={agree_1:.1f}%  n={n}\")\n",
        "                \n",
        "                reliability_results.append({\n",
        "                    'rater1': rater1,\n",
        "                    'rater2': rater2,\n",
        "                    'metric': metric,\n",
        "                    'icc': icc,\n",
        "                    'icc_low': icc_low,\n",
        "                    'icc_high': icc_high,\n",
        "                    'agree_exact': agree_exact,\n",
        "                    'agree_within1': agree_1,\n",
        "                    'n': n\n",
        "                })\n",
        "\n",
        "reliability_df = pd.DataFrame(reliability_results)\n",
        "print(\"\\n\" + \"=\" * 80)"
      ],
      "metadata": {
        "id": "reliability_analysis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model-to-Model Comparison (AI vs AI)"
      ],
      "metadata": {
        "id": "model_comparison_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract AI-to-AI comparisons\n",
        "ai_comparisons = reliability_df[\n",
        "    reliability_df['rater1'].isin(AI_MODELS) & \n",
        "    reliability_df['rater2'].isin(AI_MODELS)\n",
        "]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"MODEL-TO-MODEL AGREEMENT SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Calculate average ICC for each model pair\n",
        "model_pairs = []\n",
        "for i, m1 in enumerate(AI_MODELS):\n",
        "    for j, m2 in enumerate(AI_MODELS):\n",
        "        if j > i:\n",
        "            subset = ai_comparisons[\n",
        "                ((ai_comparisons['rater1'] == m1) & (ai_comparisons['rater2'] == m2)) |\n",
        "                ((ai_comparisons['rater1'] == m2) & (ai_comparisons['rater2'] == m1))\n",
        "            ]\n",
        "            if len(subset) > 0:\n",
        "                avg_icc = subset['icc'].mean()\n",
        "                avg_agree = subset['agree_within1'].mean()\n",
        "                model_pairs.append({\n",
        "                    'pair': f\"{m1.upper()} vs {m2.upper()}\",\n",
        "                    'avg_icc': avg_icc,\n",
        "                    'avg_agreement': avg_agree\n",
        "                })\n",
        "                print(f\"\\n{m1.upper()} vs {m2.upper()}:\")\n",
        "                print(f\"  Average ICC: {avg_icc:.3f}\")\n",
        "                print(f\"  Average Agreement (¬±1): {avg_agree:.1f}%\")\n",
        "                \n",
        "                # Show per-metric breakdown\n",
        "                print(\"\\n  Per-metric ICC:\")\n",
        "                for _, row in subset.iterrows():\n",
        "                    print(f\"    {row['metric'].replace('_', ' ').title():<25} {row['icc']:.3f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "# Visualize model-to-model ICC\n",
        "if len(model_pairs) > 0:\n",
        "    model_pairs_df = pd.DataFrame(model_pairs)\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # ICC comparison\n",
        "    colors = ['#e74c3c' if icc < 0.6 else '#f39c12' if icc < 0.75 else '#2ecc71' \n",
        "              for icc in model_pairs_df['avg_icc']]\n",
        "    ax1.barh(model_pairs_df['pair'], model_pairs_df['avg_icc'], color=colors)\n",
        "    ax1.axvline(0.75, color='green', linestyle='--', alpha=0.5, label='Good (0.75)')\n",
        "    ax1.axvline(0.60, color='orange', linestyle='--', alpha=0.5, label='Moderate (0.60)')\n",
        "    ax1.set_xlabel('Average ICC', fontsize=11)\n",
        "    ax1.set_title('Model-to-Model Agreement (ICC)', fontsize=12, fontweight='bold')\n",
        "    ax1.legend()\n",
        "    ax1.set_xlim(0, 1)\n",
        "    ax1.grid(axis='x', alpha=0.3)\n",
        "    \n",
        "    # Agreement rate comparison\n",
        "    colors2 = ['#e74c3c' if a < 70 else '#f39c12' if a < 80 else '#2ecc71' \n",
        "               for a in model_pairs_df['avg_agreement']]\n",
        "    ax2.barh(model_pairs_df['pair'], model_pairs_df['avg_agreement'], color=colors2)\n",
        "    ax2.axvline(80, color='green', linestyle='--', alpha=0.5, label='Good (80%)')\n",
        "    ax2.axvline(70, color='orange', linestyle='--', alpha=0.5, label='Moderate (70%)')\n",
        "    ax2.set_xlabel('Agreement within ¬±1 (%)', fontsize=11)\n",
        "    ax2.set_title('Model-to-Model Agreement Rate', fontsize=12, fontweight='bold')\n",
        "    ax2.legend()\n",
        "    ax2.set_xlim(0, 100)\n",
        "    ax2.grid(axis='x', alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "model_to_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model-to-Human Comparison (AI vs Human)"
      ],
      "metadata": {
        "id": "model_human_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract AI-to-Human comparisons\n",
        "ai_human_comparisons = reliability_df[\n",
        "    ((reliability_df['rater1'].isin(AI_MODELS) & reliability_df['rater2'].isin(HUMAN_RATERS)) |\n",
        "     (reliability_df['rater1'].isin(HUMAN_RATERS) & reliability_df['rater2'].isin(AI_MODELS)))\n",
        "]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"MODEL-TO-HUMAN AGREEMENT SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Calculate average ICC for each AI model vs humans\n",
        "model_human_results = []\n",
        "\n",
        "for ai_model in AI_MODELS:\n",
        "    if ai_model in dataframes:\n",
        "        print(f\"\\n{ai_model.upper()} vs HUMAN RATERS:\")\n",
        "        print(\"-\" * 80)\n",
        "        \n",
        "        for human in HUMAN_RATERS:\n",
        "            if human in dataframes:\n",
        "                subset = ai_human_comparisons[\n",
        "                    ((ai_human_comparisons['rater1'] == ai_model) & (ai_human_comparisons['rater2'] == human)) |\n",
        "                    ((ai_human_comparisons['rater1'] == human) & (ai_human_comparisons['rater2'] == ai_model))\n",
        "                ]\n",
        "                \n",
        "                if len(subset) > 0:\n",
        "                    avg_icc = subset['icc'].mean()\n",
        "                    avg_agree = subset['agree_within1'].mean()\n",
        "                    \n",
        "                    print(f\"\\n  vs {human.upper()}:\")\n",
        "                    print(f\"    Average ICC: {avg_icc:.3f}\")\n",
        "                    print(f\"    Average Agreement (¬±1): {avg_agree:.1f}%\")\n",
        "                    \n",
        "                    model_human_results.append({\n",
        "                        'ai_model': ai_model,\n",
        "                        'human': human,\n",
        "                        'avg_icc': avg_icc,\n",
        "                        'avg_agreement': avg_agree\n",
        "                    })\n",
        "                    \n",
        "                    # Best and worst metrics\n",
        "                    best_metric = subset.loc[subset['icc'].idxmax()]\n",
        "                    worst_metric = subset.loc[subset['icc'].idxmin()]\n",
        "                    print(f\"    Best metric: {best_metric['metric'].replace('_', ' ').title()} (ICC={best_metric['icc']:.3f})\")\n",
        "                    print(f\"    Worst metric: {worst_metric['metric'].replace('_', ' ').title()} (ICC={worst_metric['icc']:.3f})\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "# Visualize model-to-human comparison\n",
        "if len(model_human_results) > 0:\n",
        "    mh_df = pd.DataFrame(model_human_results)\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "    \n",
        "    # Prepare data for grouped bar chart\n",
        "    ai_models_present = mh_df['ai_model'].unique()\n",
        "    human_raters_present = mh_df['human'].unique()\n",
        "    \n",
        "    x = np.arange(len(ai_models_present))\n",
        "    width = 0.35\n",
        "    \n",
        "    # ICC plot\n",
        "    for i, human in enumerate(human_raters_present):\n",
        "        data = [mh_df[(mh_df['ai_model'] == ai) & (mh_df['human'] == human)]['avg_icc'].values[0]\n",
        "                if len(mh_df[(mh_df['ai_model'] == ai) & (mh_df['human'] == human)]) > 0 else 0\n",
        "                for ai in ai_models_present]\n",
        "        axes[0].bar(x + i*width - width/2, data, width, \n",
        "                   label=human.title(), alpha=0.8)\n",
        "    \n",
        "    axes[0].axhline(0.75, color='green', linestyle='--', alpha=0.5, linewidth=1)\n",
        "    axes[0].axhline(0.60, color='orange', linestyle='--', alpha=0.5, linewidth=1)\n",
        "    axes[0].set_ylabel('Average ICC', fontsize=11)\n",
        "    axes[0].set_title('AI Models vs Human Raters (ICC)', fontsize=12, fontweight='bold')\n",
        "    axes[0].set_xticks(x)\n",
        "    axes[0].set_xticklabels([ai.upper() for ai in ai_models_present])\n",
        "    axes[0].legend()\n",
        "    axes[0].set_ylim(0, 1)\n",
        "    axes[0].grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Agreement rate plot\n",
        "    for i, human in enumerate(human_raters_present):\n",
        "        data = [mh_df[(mh_df['ai_model'] == ai) & (mh_df['human'] == human)]['avg_agreement'].values[0]\n",
        "                if len(mh_df[(mh_df['ai_model'] == ai) & (mh_df['human'] == human)]) > 0 else 0\n",
        "                for ai in ai_models_present]\n",
        "        axes[1].bar(x + i*width - width/2, data, width, \n",
        "                   label=human.title(), alpha=0.8)\n",
        "    \n",
        "    axes[1].axhline(80, color='green', linestyle='--', alpha=0.5, linewidth=1)\n",
        "    axes[1].axhline(70, color='orange', linestyle='--', alpha=0.5, linewidth=1)\n",
        "    axes[1].set_ylabel('Agreement within ¬±1 (%)', fontsize=11)\n",
        "    axes[1].set_title('AI Models vs Human Raters (Agreement)', fontsize=12, fontweight='bold')\n",
        "    axes[1].set_xticks(x)\n",
        "    axes[1].set_xticklabels([ai.upper() for ai in ai_models_present])\n",
        "    axes[1].legend()\n",
        "    axes[1].set_ylim(0, 100)\n",
        "    axes[1].grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "model_to_human"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Comprehensive ICC Heatmap (All Comparisons)"
      ],
      "metadata": {
        "id": "heatmap_all_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ICC matrix for all rater pairs\n",
        "raters_list = list(dataframes.keys())\n",
        "n_raters = len(raters_list)\n",
        "\n",
        "icc_matrix = np.zeros((n_raters, n_raters))\n",
        "icc_matrix[:] = np.nan\n",
        "\n",
        "for i, rater1 in enumerate(raters_list):\n",
        "    for j, rater2 in enumerate(raters_list):\n",
        "        if i == j:\n",
        "            icc_matrix[i, j] = 1.0  # Perfect agreement with self\n",
        "        else:\n",
        "            subset = reliability_df[\n",
        "                ((reliability_df['rater1'] == rater1) & (reliability_df['rater2'] == rater2)) |\n",
        "                ((reliability_df['rater1'] == rater2) & (reliability_df['rater2'] == rater1))\n",
        "            ]\n",
        "            if len(subset) > 0:\n",
        "                icc_matrix[i, j] = subset['icc'].mean()\n",
        "\n",
        "# Plot comprehensive heatmap\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "# Create labels with categories\n",
        "labels = []\n",
        "for r in raters_list:\n",
        "    if r in AI_MODELS:\n",
        "        labels.append(f\"{r.upper()}\\n(AI)\")\n",
        "    else:\n",
        "        labels.append(f\"{r.title()}\\n(Human)\")\n",
        "\n",
        "im = ax.imshow(icc_matrix, cmap='RdYlGn', vmin=0, vmax=1, aspect='auto')\n",
        "\n",
        "# Add text annotations\n",
        "for i in range(n_raters):\n",
        "    for j in range(n_raters):\n",
        "        if not np.isnan(icc_matrix[i, j]):\n",
        "            text = ax.text(j, i, f'{icc_matrix[i, j]:.2f}',\n",
        "                         ha=\"center\", va=\"center\",\n",
        "                         color=\"white\" if icc_matrix[i, j] < 0.5 else \"black\",\n",
        "                         fontsize=10, fontweight='bold')\n",
        "\n",
        "# Labels and colorbar\n",
        "ax.set_xticks(np.arange(n_raters))\n",
        "ax.set_yticks(np.arange(n_raters))\n",
        "ax.set_xticklabels(labels, fontsize=10)\n",
        "ax.set_yticklabels(labels, fontsize=10)\n",
        "\n",
        "plt.setp(ax.get_xticklabels(), rotation=0, ha=\"center\")\n",
        "\n",
        "cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "cbar.set_label('Average ICC', fontsize=11)\n",
        "\n",
        "ax.set_title('Comprehensive Inter-Rater Reliability Matrix\\n(Average ICC across all metrics)', \n",
        "            fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "# Add grid\n",
        "ax.set_xticks(np.arange(n_raters)-.5, minor=True)\n",
        "ax.set_yticks(np.arange(n_raters)-.5, minor=True)\n",
        "ax.grid(which=\"minor\", color=\"gray\", linestyle='-', linewidth=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nICC Interpretation:\")\n",
        "print(\"  üü¢ Green (>0.75): Good to excellent agreement\")\n",
        "print(\"  üü° Yellow (0.60-0.75): Moderate agreement\")\n",
        "print(\"  üü† Orange (0.50-0.60): Fair agreement\")\n",
        "print(\"  üî¥ Red (<0.50): Poor agreement\")"
      ],
      "metadata": {
        "id": "comprehensive_heatmap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Model Performance Dashboard"
      ],
      "metadata": {
        "id": "performance_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create performance comparison for AI models\n",
        "print(\"=\" * 80)\n",
        "print(\"MODEL PERFORMANCE DASHBOARD\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "performance_data = []\n",
        "\n",
        "# Typical costs (update as needed)\n",
        "MODEL_COSTS = {\n",
        "    'gpt4o': {'cost_per_1k_input': 2.50/1000, 'cost_per_1k_output': 10.00/1000, 'label': 'GPT-4o'},\n",
        "    'gpt35': {'cost_per_1k_input': 0.50/1000, 'cost_per_1k_output': 1.50/1000, 'label': 'GPT-3.5-turbo'},\n",
        "    'llama': {'cost_per_1k_input': 0.10/1000, 'cost_per_1k_output': 0.20/1000, 'label': 'Llama-3.1-8B'}\n",
        "}\n",
        "\n",
        "for ai_model in AI_MODELS:\n",
        "    if ai_model in dataframes:\n",
        "        df = dataframes[ai_model]\n",
        "        \n",
        "        # Score statistics\n",
        "        score_cols_model = [f\"{m}_score\" for m in METRICS if f\"{m}_score\" in df.columns]\n",
        "        if score_cols_model:\n",
        "            scores = df[score_cols_model].values.flatten()\n",
        "            scores = scores[~np.isnan(scores)]\n",
        "            \n",
        "            mean_score = scores.mean()\n",
        "            std_score = scores.std()\n",
        "        else:\n",
        "            mean_score = np.nan\n",
        "            std_score = np.nan\n",
        "        \n",
        "        # Agreement with humans\n",
        "        human_agreement_icc = []\n",
        "        human_agreement_rate = []\n",
        "        for human in HUMAN_RATERS:\n",
        "            if human in dataframes:\n",
        "                subset = reliability_df[\n",
        "                    ((reliability_df['rater1'] == ai_model) & (reliability_df['rater2'] == human)) |\n",
        "                    ((reliability_df['rater1'] == human) & (reliability_df['rater2'] == ai_model))\n",
        "                ]\n",
        "                if len(subset) > 0:\n",
        "                    human_agreement_icc.append(subset['icc'].mean())\n",
        "                    human_agreement_rate.append(subset['agree_within1'].mean())\n",
        "        \n",
        "        avg_human_icc = np.mean(human_agreement_icc) if human_agreement_icc else np.nan\n",
        "        avg_human_agree = np.mean(human_agreement_rate) if human_agreement_rate else np.nan\n",
        "        \n",
        "        # Latency (if available)\n",
        "        latency_cols = [c for c in df.columns if 'latency' in c.lower()]\n",
        "        if latency_cols:\n",
        "            median_latency = df[latency_cols[0]].median()\n",
        "        else:\n",
        "            median_latency = np.nan\n",
        "        \n",
        "        # Estimated cost (rough estimate for 50 candidates, K=3)\n",
        "        if ai_model in MODEL_COSTS:\n",
        "            # Rough estimate: 1000 tokens input + 300 tokens output per scoring run\n",
        "            # 50 candidates * 3 runs * (1000 input + 300 output)\n",
        "            est_cost = (50 * 3 * 1000 * MODEL_COSTS[ai_model]['cost_per_1k_input'] +\n",
        "                       50 * 3 * 300 * MODEL_COSTS[ai_model]['cost_per_1k_output'])\n",
        "        else:\n",
        "            est_cost = np.nan\n",
        "        \n",
        "        performance_data.append({\n",
        "            'model': MODEL_COSTS.get(ai_model, {}).get('label', ai_model.upper()),\n",
        "            'mean_score': mean_score,\n",
        "            'std_score': std_score,\n",
        "            'avg_icc_human': avg_human_icc,\n",
        "            'avg_agree_human': avg_human_agree,\n",
        "            'median_latency_ms': median_latency,\n",
        "            'est_cost_50_candidates': est_cost\n",
        "        })\n",
        "\n",
        "perf_df = pd.DataFrame(performance_data)\n",
        "\n",
        "# Display table\n",
        "print(\"\\nModel Performance Comparison:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"{'Model':<20} {'Avg Score':<12} {'ICC (Human)':<15} {'Agreement':<12} {'Latency':<12} {'Cost'}\")\n",
        "print(\"-\" * 80)\n",
        "for _, row in perf_df.iterrows():\n",
        "    print(f\"{row['model']:<20} \"\n",
        "          f\"{row['mean_score']:.2f}¬±{row['std_score']:.2f}    \"\n",
        "          f\"{row['avg_icc_human']:.3f}           \"\n",
        "          f\"{row['avg_agree_human']:.1f}%        \"\n",
        "          f\"{row['median_latency_ms']:.0f}ms       \"\n",
        "          f\"${row['est_cost_50_candidates']:.2f}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Visualize performance tradeoffs\n",
        "if len(perf_df) > 1:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    \n",
        "    # 1. ICC vs Cost\n",
        "    axes[0, 0].scatter(perf_df['est_cost_50_candidates'], perf_df['avg_icc_human'], \n",
        "                      s=200, alpha=0.6, c=['#e74c3c', '#f39c12', '#2ecc71'])\n",
        "    for idx, row in perf_df.iterrows():\n",
        "        axes[0, 0].annotate(row['model'], \n",
        "                          (row['est_cost_50_candidates'], row['avg_icc_human']),\n",
        "                          fontsize=10, ha='center')\n",
        "    axes[0, 0].axhline(0.75, color='green', linestyle='--', alpha=0.3)\n",
        "    axes[0, 0].set_xlabel('Cost ($) for 50 candidates', fontsize=11)\n",
        "    axes[0, 0].set_ylabel('Avg ICC with Humans', fontsize=11)\n",
        "    axes[0, 0].set_title('Quality vs Cost', fontsize=12, fontweight='bold')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Agreement Rate comparison\n",
        "    axes[0, 1].barh(perf_df['model'], perf_df['avg_agree_human'],\n",
        "                    color=['#e74c3c', '#f39c12', '#2ecc71'])\n",
        "    axes[0, 1].axvline(75, color='green', linestyle='--', alpha=0.5)\n",
        "    axes[0, 1].set_xlabel('Agreement with Humans (¬±1 point)', fontsize=11)\n",
        "    axes[0, 1].set_title('Agreement Rate Comparison', fontsize=12, fontweight='bold')\n",
        "    axes[0, 1].set_xlim(0, 100)\n",
        "    axes[0, 1].grid(axis='x', alpha=0.3)\n",
        "    \n",
        "    # 3. Latency comparison\n",
        "    axes[1, 0].bar(perf_df['model'], perf_df['median_latency_ms'],\n",
        "                   color=['#e74c3c', '#f39c12', '#2ecc71'])\n",
        "    axes[1, 0].set_ylabel('Median Latency (ms)', fontsize=11)\n",
        "    axes[1, 0].set_title('Speed Comparison', fontsize=12, fontweight='bold')\n",
        "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # 4. Score distribution\n",
        "    axes[1, 1].bar(perf_df['model'], perf_df['mean_score'], \n",
        "                   yerr=perf_df['std_score'],\n",
        "                   color=['#e74c3c', '#f39c12', '#2ecc71'],\n",
        "                   capsize=5)\n",
        "    axes[1, 1].set_ylabel('Mean Score (¬±1 SD)', fontsize=11)\n",
        "    axes[1, 1].set_title('Score Distribution', fontsize=12, fontweight='bold')\n",
        "    axes[1, 1].set_ylim(0, 10)\n",
        "    axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)"
      ],
      "metadata": {
        "id": "performance_dashboard"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Systematic Bias Analysis (All Models)"
      ],
      "metadata": {
        "id": "bias_all_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate bias for all model pairs\n",
        "print(\"=\" * 80)\n",
        "print(\"SYSTEMATIC BIAS ANALYSIS (RATER 2 - RATER 1)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "bias_results = []\n",
        "\n",
        "for i, rater1 in enumerate(raters_list):\n",
        "    for j, rater2 in enumerate(raters_list):\n",
        "        if j > i:\n",
        "            print(f\"\\n{rater2.upper()} - {rater1.upper()}:\")\n",
        "            print(\"-\" * 80)\n",
        "            \n",
        "            for metric in METRICS:\n",
        "                col1 = f\"{metric}_score_{rater1}\"\n",
        "                col2 = f\"{metric}_score_{rater2}\"\n",
        "                \n",
        "                mask = master_df[col1].notna() & master_df[col2].notna()\n",
        "                if mask.sum() > 1:\n",
        "                    bias = (master_df.loc[mask, col2] - master_df.loc[mask, col1]).mean()\n",
        "                    mae = (master_df.loc[mask, col2] - master_df.loc[mask, col1]).abs().mean()\n",
        "                    \n",
        "                    print(f\"  {metric.replace('_', ' ').title():<25} Bias: {bias:+.2f}  MAE: {mae:.2f}\")\n",
        "                    \n",
        "                    bias_results.append({\n",
        "                        'rater1': rater1,\n",
        "                        'rater2': rater2,\n",
        "                        'metric': metric,\n",
        "                        'bias': bias,\n",
        "                        'mae': mae\n",
        "                    })\n",
        "\n",
        "bias_df = pd.DataFrame(bias_results)\n",
        "\n",
        "# Visualize bias patterns for AI models vs humans\n",
        "ai_human_bias = bias_df[\n",
        "    ((bias_df['rater1'].isin(AI_MODELS) & bias_df['rater2'].isin(HUMAN_RATERS)) |\n",
        "     (bias_df['rater1'].isin(HUMAN_RATERS) & bias_df['rater2'].isin(AI_MODELS)))\n",
        "]\n",
        "\n",
        "if len(ai_human_bias) > 0:\n",
        "    # Create a heatmap of bias\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    \n",
        "    # Pivot for heatmap\n",
        "    pivot_data = []\n",
        "    for ai in AI_MODELS:\n",
        "        if ai in dataframes:\n",
        "            row_data = []\n",
        "            for metric in METRICS:\n",
        "                # Get average bias across both human raters\n",
        "                biases = []\n",
        "                for human in HUMAN_RATERS:\n",
        "                    if human in dataframes:\n",
        "                        b1 = bias_df[(bias_df['rater1'] == ai) & (bias_df['rater2'] == human) & (bias_df['metric'] == metric)]\n",
        "                        b2 = bias_df[(bias_df['rater1'] == human) & (bias_df['rater2'] == ai) & (bias_df['metric'] == metric)]\n",
        "                        if len(b1) > 0:\n",
        "                            biases.append(b1['bias'].values[0])\n",
        "                        elif len(b2) > 0:\n",
        "                            biases.append(-b2['bias'].values[0])  # Flip sign\n",
        "                if biases:\n",
        "                    row_data.append(np.mean(biases))\n",
        "                else:\n",
        "                    row_data.append(0)\n",
        "            pivot_data.append(row_data)\n",
        "    \n",
        "    pivot_array = np.array(pivot_data)\n",
        "    \n",
        "    im = ax.imshow(pivot_array, cmap='RdBu_r', vmin=-2, vmax=2, aspect='auto')\n",
        "    \n",
        "    # Annotations\n",
        "    for i in range(len(pivot_array)):\n",
        "        for j in range(len(METRICS)):\n",
        "            text = ax.text(j, i, f'{pivot_array[i, j]:.2f}',\n",
        "                         ha=\"center\", va=\"center\",\n",
        "                         color=\"white\" if abs(pivot_array[i, j]) > 1 else \"black\",\n",
        "                         fontsize=10, fontweight='bold')\n",
        "    \n",
        "    ax.set_xticks(np.arange(len(METRICS)))\n",
        "    ax.set_yticks(np.arange(len([ai for ai in AI_MODELS if ai in dataframes])))\n",
        "    ax.set_xticklabels([m.replace('_', '\\n').title() for m in METRICS], fontsize=10)\n",
        "    ax.set_yticklabels([MODEL_COSTS.get(ai, {}).get('label', ai.upper()) \n",
        "                       for ai in AI_MODELS if ai in dataframes], fontsize=10)\n",
        "    \n",
        "    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "    cbar.set_label('Bias (Human - AI)', fontsize=11)\n",
        "    \n",
        "    ax.set_title('Systematic Bias: AI Models vs Human Average\\n(Positive = Humans score higher)', \n",
        "                fontsize=13, fontweight='bold', pad=15)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nBias Interpretation:\")\n",
        "    print(\"  üî¥ Red (positive): Humans score higher than AI\")\n",
        "    print(\"  üîµ Blue (negative): AI scores higher than humans\")\n",
        "    print(\"  ‚ö™ White (near zero): Well-calibrated\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)"
      ],
      "metadata": {
        "id": "bias_all"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Final Recommendations"
      ],
      "metadata": {
        "id": "recommendations_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"COMPREHENSIVE ANALYSIS SUMMARY & RECOMMENDATIONS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 1. Model-to-Model Consistency\n",
        "print(\"\\n1. AI MODEL CONSISTENCY\")\n",
        "print(\"-\" * 80)\n",
        "if len(model_pairs_df) > 0:\n",
        "    best_pair = model_pairs_df.loc[model_pairs_df['avg_icc'].idxmax()]\n",
        "    print(f\"Best model agreement: {best_pair['pair']} (ICC={best_pair['avg_icc']:.3f})\")\n",
        "    \n",
        "    if best_pair['avg_icc'] > 0.85:\n",
        "        print(\"  ‚úì Models are highly consistent with each other\")\n",
        "    elif best_pair['avg_icc'] > 0.75:\n",
        "        print(\"  ‚úì Models show good consistency\")\n",
        "    else:\n",
        "        print(\"  ‚ö†Ô∏è Models show significant variation - consider why\")\n",
        "\n",
        "# 2. Model-to-Human Agreement\n",
        "print(\"\\n2. AI vs HUMAN AGREEMENT\")\n",
        "print(\"-\" * 80)\n",
        "if len(model_human_results) > 0:\n",
        "    mh_df = pd.DataFrame(model_human_results)\n",
        "    for ai in AI_MODELS:\n",
        "        if ai in dataframes:\n",
        "            ai_results = mh_df[mh_df['ai_model'] == ai]\n",
        "            if len(ai_results) > 0:\n",
        "                avg_icc = ai_results['avg_icc'].mean()\n",
        "                avg_agree = ai_results['avg_agreement'].mean()\n",
        "                \n",
        "                label = MODEL_COSTS.get(ai, {}).get('label', ai.upper())\n",
        "                print(f\"\\n{label}:\")\n",
        "                print(f\"  Average ICC: {avg_icc:.3f}\")\n",
        "                print(f\"  Average Agreement: {avg_agree:.1f}%\")\n",
        "                \n",
        "                if avg_icc > 0.75 and avg_agree > 75:\n",
        "                    print(f\"  ‚úì {label} ready for production\")\n",
        "                elif avg_icc > 0.60 and avg_agree > 70:\n",
        "                    print(f\"  ‚ö†Ô∏è {label} suitable with human oversight\")\n",
        "                else:\n",
        "                    print(f\"  ‚ùå {label} needs calibration before deployment\")\n",
        "\n",
        "# 3. Model Selection Recommendation\n",
        "print(\"\\n3. MODEL SELECTION RECOMMENDATION\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "if len(perf_df) > 1:\n",
        "    # Score models on quality, cost, and speed\n",
        "    perf_df['quality_score'] = perf_df['avg_icc_human'] * 100\n",
        "    perf_df['cost_score'] = (1 / (perf_df['est_cost_50_candidates'] + 0.01)) * 10\n",
        "    perf_df['speed_score'] = (1 / (perf_df['median_latency_ms'] / 1000 + 0.01)) * 10\n",
        "    \n",
        "    # Overall score (weighted)\n",
        "    perf_df['overall_score'] = (\n",
        "        perf_df['quality_score'] * 0.50 +  # 50% weight on quality\n",
        "        perf_df['cost_score'] * 0.30 +     # 30% weight on cost\n",
        "        perf_df['speed_score'] * 0.20      # 20% weight on speed\n",
        "    )\n",
        "    \n",
        "    best_model = perf_df.loc[perf_df['overall_score'].idxmax()]\n",
        "    \n",
        "    print(f\"\\nüèÜ Recommended Model: {best_model['model']}\")\n",
        "    print(f\"   Quality: {best_model['avg_icc_human']:.3f} ICC\")\n",
        "    print(f\"   Cost: ${best_model['est_cost_50_candidates']:.2f} per 50 candidates\")\n",
        "    print(f\"   Speed: {best_model['median_latency_ms']:.0f}ms median\")\n",
        "    \n",
        "    print(\"\\nüìä Model Rankings:\")\n",
        "    ranked = perf_df.sort_values('overall_score', ascending=False)\n",
        "    for idx, (_, row) in enumerate(ranked.iterrows(), 1):\n",
        "        print(f\"   {idx}. {row['model']} (Overall Score: {row['overall_score']:.1f})\")\n",
        "    \n",
        "    print(\"\\nüí° Use Case Recommendations:\")\n",
        "    print(\"   ‚Ä¢ Production (Best Balance): \", end=\"\")\n",
        "    print(ranked.iloc[0]['model'])\n",
        "    \n",
        "    print(\"   ‚Ä¢ Cost-Sensitive: \", end=\"\")\n",
        "    print(perf_df.loc[perf_df['est_cost_50_candidates'].idxmin()]['model'])\n",
        "    \n",
        "    print(\"   ‚Ä¢ Quality-First: \", end=\"\")\n",
        "    print(perf_df.loc[perf_df['avg_icc_human'].idxmax()]['model'])\n",
        "    \n",
        "    print(\"   ‚Ä¢ Speed-Critical: \", end=\"\")\n",
        "    print(perf_df.loc[perf_df['median_latency_ms'].idxmin()]['model'])\n",
        "\n",
        "# 4. Action Items\n",
        "print(\"\\n4. NEXT STEPS\")\n",
        "print(\"-\" * 80)\n",
        "print(\"1. Review high-disagreement cases for selected model\")\n",
        "print(\"2. Validate on fresh test set (recommend 100+ interviews)\")\n",
        "print(\"3. Set up monitoring for agreement rates over time\")\n",
        "print(\"4. Define human review triggers (e.g., confidence < threshold)\")\n",
        "print(\"5. Plan quarterly recalibration sessions\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"END OF COMPREHENSIVE ANALYSIS\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "final_recommendations"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Export Results"
      ],
      "metadata": {
        "id": "export_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# Create output directory\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "output_dir = \"/content/drive/MyDrive/mvp/multi_model_analysis\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Save all results\n",
        "reliability_df.to_csv(f\"{output_dir}/all_pairwise_reliability_{timestamp}.csv\", index=False)\n",
        "bias_df.to_csv(f\"{output_dir}/all_pairwise_bias_{timestamp}.csv\", index=False)\n",
        "master_df.to_csv(f\"{output_dir}/master_comparison_{timestamp}.csv\", index=False)\n",
        "\n",
        "if len(perf_df) > 0:\n",
        "    perf_df.to_csv(f\"{output_dir}/model_performance_{timestamp}.csv\", index=False)\n",
        "\n",
        "if len(model_pairs_df) > 0:\n",
        "    model_pairs_df.to_csv(f\"{output_dir}/model_to_model_{timestamp}.csv\", index=False)\n",
        "\n",
        "if len(model_human_results) > 0:\n",
        "    pd.DataFrame(model_human_results).to_csv(f\"{output_dir}/model_to_human_{timestamp}.csv\", index=False)\n",
        "\n",
        "print(f\"‚úì All results saved to: {output_dir}\")\n",
        "print(f\"\\nFiles created:\")\n",
        "print(f\"  - all_pairwise_reliability_{timestamp}.csv\")\n",
        "print(f\"  - all_pairwise_bias_{timestamp}.csv\")\n",
        "print(f\"  - master_comparison_{timestamp}.csv\")\n",
        "print(f\"  - model_performance_{timestamp}.csv\")\n",
        "print(f\"  - model_to_model_{timestamp}.csv\")\n",
        "print(f\"  - model_to_human_{timestamp}.csv\")"
      ],
      "metadata": {
        "id": "export"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
