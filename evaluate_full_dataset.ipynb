{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vULq_VZ3VUu4"
      },
      "source": [
        "# Evaluate Fine-tuned Model on Full Dataset\n",
        "\n",
        "This notebook evaluates your fine-tuned GPT-4o model on the entire dataset (all 250 samples).\n",
        "\n",
        "**Steps:**\n",
        "1. Retrieve your fine-tuned model name from OpenAI\n",
        "2. Load the full dataset\n",
        "3. Run inference on all samples (with latency tracking)\n",
        "4. Calculate and save metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2F5LFwpVUu6"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_ofBqNfUVUu6"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install openai pandas numpy matplotlib -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yso-HAxAVUu7",
        "outputId": "65c0568d-9812-4a0c-9291-b11eaef274ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hrc4ep7cVUu8",
        "outputId": "1cff6886-bf9a-4428-d947-120e6e543412"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paste your OpenAI API key: ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Set OpenAI API key\n",
        "if \"OPENAI_API_KEY\" not in os.environ or not os.environ[\"OPENAI_API_KEY\"]:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI API key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mk5tloUVUu8",
        "outputId": "88d15149-6a8b-4df9-fc03-df8eb28a52aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Libraries loaded\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import Dict, List\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "print(\"✓ Libraries loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB1RrpSCVUu9"
      },
      "source": [
        "## Step 1: Retrieve Your Fine-tuned Model\n",
        "\n",
        "This will list all your fine-tuning jobs and show the model names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6teCtmBiVUu9",
        "outputId": "042baf89-893a-4bce-cb15-3e1a301c3160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving your fine-tuning jobs...\n",
            "======================================================================\n",
            "\n",
            "Recent Fine-tuning Jobs:\n",
            "\n",
            "1. Job ID: ftjob-t9Yi16FKGtHzeZsY7zAPq7KU\n",
            "   Status: succeeded\n",
            "   Created: 2025-11-30 19:04:04\n",
            "   ✓ Fine-tuned Model: ft:gpt-4o-2024-08-06:cleanagent:interview-scorer:ChhOUzkh\n",
            "   Finished: 2025-11-30 19:29:03\n",
            "\n",
            "2. Job ID: ftjob-5o6R4f1LQDKyeDsJ0tICpclW\n",
            "   Status: failed\n",
            "   Created: 2025-11-29 22:31:39\n",
            "   Model: Not yet available (status: failed)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# List all fine-tuning jobs\n",
        "print(\"Retrieving your fine-tuning jobs...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "jobs = client.fine_tuning.jobs.list(limit=10)\n",
        "\n",
        "print(\"\\nRecent Fine-tuning Jobs:\\n\")\n",
        "\n",
        "for i, job in enumerate(jobs.data, 1):\n",
        "    print(f\"{i}. Job ID: {job.id}\")\n",
        "    print(f\"   Status: {job.status}\")\n",
        "    print(f\"   Created: {datetime.fromtimestamp(job.created_at).strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    if job.fine_tuned_model:\n",
        "        print(f\"   ✓ Fine-tuned Model: {job.fine_tuned_model}\")\n",
        "    else:\n",
        "        print(f\"   Model: Not yet available (status: {job.status})\")\n",
        "\n",
        "    if job.finished_at:\n",
        "        print(f\"   Finished: {datetime.fromtimestamp(job.finished_at).strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkhz_nWWVUu-",
        "outputId": "3df26a23-a82d-431d-f6ad-c464206c9c19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using model: ft:gpt-4o-2024-08-06:cleanagent:interview-scorer:ChhOUzkh\n"
          ]
        }
      ],
      "source": [
        "# Set your fine-tuned model name here\n",
        "# Copy it from the output above\n",
        "\n",
        "FINE_TUNED_MODEL = \"ft:gpt-4o-2024-08-06:cleanagent:interview-scorer:ChhOUzkh\"  # ← UPDATE THIS!\n",
        "\n",
        "# Example: FINE_TUNED_MODEL = \"ft:gpt-4o-2024-08-06:personal::AbCdEfGh\"\n",
        "\n",
        "print(f\"Using model: {FINE_TUNED_MODEL}\")\n",
        "\n",
        "if FINE_TUNED_MODEL == \"YOUR_MODEL_NAME_HERE\":\n",
        "    print(\"\\n⚠️  WARNING: You need to update FINE_TUNED_MODEL with your actual model name!\")\n",
        "    print(\"Copy it from the output above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxpBKmY-VUu-"
      },
      "source": [
        "## Step 2: Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvABdt-_VUu_",
        "outputId": "008e5cbc-1042-45f4-9262-2de8e4201aa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration:\n",
            "  Model: ft:gpt-4o-2024-08-06:cleanagent:interview-scorer:ChhOUzkh\n",
            "  Data path: /content/drive/MyDrive/hiring_evaluations.csv\n",
            "  Output directory: /content/drive/MyDrive/finetuning_output\n"
          ]
        }
      ],
      "source": [
        "# File paths - UPDATE THESE\n",
        "DATA_PATH = \"/content/drive/MyDrive/hiring_evaluations.csv\"  # ← Update this path\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/finetuning_output\"  # Where to save results\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Metrics\n",
        "METRICS = [\n",
        "    \"cognitive_ability\",\n",
        "    \"experience\",\n",
        "    \"problem_solving\",\n",
        "    \"reliability\",\n",
        "    \"professionalism\",\n",
        "    \"communication\"\n",
        "]\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Model: {FINE_TUNED_MODEL}\")\n",
        "print(f\"  Data path: {DATA_PATH}\")\n",
        "print(f\"  Output directory: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8aFoCIMVUu_"
      },
      "source": [
        "## Step 3: Load Full Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mE3eS8akVUu_",
        "outputId": "5b6d095d-2e1a-4a4c-c83c-5ed169f0c9f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "✓ Loaded 250 samples\n",
            "\n",
            "Dataset overview:\n",
            "                          interview_id                             role  \\\n",
            "0  customer_service_representative_001  Customer Service Representative   \n",
            "1  customer_service_representative_002  Customer Service Representative   \n",
            "2  customer_service_representative_003  Customer Service Representative   \n",
            "3  customer_service_representative_004  Customer Service Representative   \n",
            "4  customer_service_representative_005  Customer Service Representative   \n",
            "\n",
            "   cognitive_ability  experience  problem_solving  reliability  \\\n",
            "0                  6           1                6            5   \n",
            "1                  6           5                9            5   \n",
            "2                  6           5                9            5   \n",
            "3                  6           5                6            5   \n",
            "4                  6           4                9            5   \n",
            "\n",
            "   professionalism  communication  \n",
            "0                4              1  \n",
            "1                4              1  \n",
            "2                4              1  \n",
            "3                4              2  \n",
            "4                4              3  \n",
            "\n",
            "Role distribution:\n",
            "role\n",
            "Customer Service Representative    50\n",
            "Sales Representative               50\n",
            "Field Technician                   50\n",
            "Home Service Technician            50\n",
            "General Manager (Franchise)        50\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Load the full dataset\n",
        "print(\"Loading dataset...\")\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "print(f\"✓ Loaded {len(df)} samples\")\n",
        "print(f\"\\nDataset overview:\")\n",
        "print(df[['interview_id', 'role'] + METRICS].head())\n",
        "\n",
        "print(f\"\\nRole distribution:\")\n",
        "print(df['role'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13yzoqgzVUu_"
      },
      "source": [
        "## Step 4: Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rk2SifQEVUu_",
        "outputId": "e00cfbc7-f787-4e38-f3ca-0ed10e304aad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Helper functions loaded\n"
          ]
        }
      ],
      "source": [
        "def build_scoring_prompt(qa_text: str, role: str) -> str:\n",
        "    \"\"\"Build the scoring prompt matching the training format\"\"\"\n",
        "    prompt = f\"\"\"You are evaluating a candidate interview for the role: {role}\n",
        "\n",
        "Analyze the candidate's responses using these six metrics (each scored 1-10):\n",
        "\n",
        "1. **Cognitive Ability (35%)**: Structured thinking, planning, logic, analytical reasoning\n",
        "2. **Experience (35%)**: Relevant work history (last 10 years), demonstrated skills, accomplishments\n",
        "3. **Problem Solving (15%)**: Resourcefulness, creative solutions, handling constraints\n",
        "4. **Reliability (5%)**: Punctuality, follow-through, dependability signals\n",
        "5. **Professionalism (5%)**: Respect for clients/rules, composure under stress\n",
        "6. **Communication (5%)**: Clarity and tone (ignore filler words like um, uh, like)\n",
        "\n",
        "CRITICAL INSTRUCTIONS:\n",
        "- Return ONLY a valid JSON object\n",
        "- Use these exact keys: cognitive_ability, experience, problem_solving, reliability, professionalism, communication\n",
        "- Each value must be an integer from 1 to 10\n",
        "- Do not include any explanations, just the JSON\n",
        "\n",
        "Interview Transcript:\n",
        "--- START TRANSCRIPT ---\n",
        "{qa_text}\n",
        "--- END TRANSCRIPT ---\n",
        "\n",
        "Return your scores in this format:\n",
        "{{\"cognitive_ability\":7,\"experience\":6,\"problem_solving\":7,\"reliability\":6,\"professionalism\":7,\"communication\":8}}\"\"\"\n",
        "    return prompt\n",
        "\n",
        "def run_single_inference(qa_text: str, role: str, model_name: str) -> tuple:\n",
        "    \"\"\"Run inference on a single sample and return (scores, latency_ms)\"\"\"\n",
        "    system_message = \"You are an expert interviewer evaluating candidates. You provide accurate, consistent scoring based on interview transcripts.\"\n",
        "    user_message = build_scoring_prompt(qa_text, role)\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                {\"role\": \"user\", \"content\": user_message}\n",
        "            ],\n",
        "            temperature=0.0,\n",
        "            max_tokens=512\n",
        "        )\n",
        "\n",
        "        latency_ms = (time.time() - start_time) * 1000\n",
        "\n",
        "        response_text = response.choices[0].message.content.strip()\n",
        "\n",
        "        # Clean response (remove markdown code blocks if present)\n",
        "        if response_text.startswith(\"```\"):\n",
        "            response_text = response_text.split(\"```\")[1]\n",
        "            if response_text.startswith(\"json\"):\n",
        "                response_text = response_text[4:]\n",
        "            response_text = response_text.strip()\n",
        "\n",
        "        scores = json.loads(response_text)\n",
        "\n",
        "        # Validate and clamp scores\n",
        "        validated_scores = {}\n",
        "        for metric in METRICS:\n",
        "            if metric in scores:\n",
        "                validated_scores[metric] = max(1, min(10, int(scores[metric])))\n",
        "            else:\n",
        "                print(f\"⚠️  Missing metric '{metric}', defaulting to 5\")\n",
        "                validated_scores[metric] = 5\n",
        "\n",
        "        return validated_scores, latency_ms\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        latency_ms = (time.time() - start_time) * 1000 if 'start_time' in locals() else 0\n",
        "        print(f\"❌ JSON decode error: {e}\")\n",
        "        return {metric: 5 for metric in METRICS}, latency_ms\n",
        "    except Exception as e:\n",
        "        latency_ms = (time.time() - start_time) * 1000 if 'start_time' in locals() else 0\n",
        "        print(f\"❌ Error: {e}\")\n",
        "        return {metric: 5 for metric in METRICS}, latency_ms\n",
        "\n",
        "print(\"✓ Helper functions loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wSHKydHVUvA"
      },
      "source": [
        "## Step 5: Run Inference on All Samples\n",
        "\n",
        "This will take approximately 3-5 minutes for 250 samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZC_-k8s6VUvA",
        "outputId": "4be47795-52be-471d-d843-1f34963505b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Running inference on 250 samples...\n",
            "======================================================================\n",
            "This will take approximately 3-5 minutes\n",
            "\n",
            "\n",
            "\n",
            "✓ Completed in 428.7 seconds (1.71s per sample)\n",
            "\n",
            "Latency Statistics:\n",
            "----------------------------------------------------------------------\n",
            "Mean:   1614 ms\n",
            "Median: 1519 ms\n",
            "P95:    3570 ms\n",
            "P99:    4415 ms\n",
            "Min:    769 ms\n",
            "Max:    5192 ms\n"
          ]
        }
      ],
      "source": [
        "# Run inference on all samples\n",
        "print(\"=\" * 70)\n",
        "print(f\"Running inference on {len(df)} samples...\")\n",
        "print(\"=\" * 70)\n",
        "print(\"This will take approximately 3-5 minutes\\n\")\n",
        "\n",
        "predictions = []\n",
        "latencies = []\n",
        "start_time = time.time()\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    print(f\"Processing {idx + 1}/{len(df)}: {row['interview_id']}\", end=\"\\r\")\n",
        "\n",
        "    pred_scores, latency = run_single_inference(row['qa_text'], row['role'], FINE_TUNED_MODEL)\n",
        "    predictions.append(pred_scores)\n",
        "    latencies.append(latency)\n",
        "\n",
        "    # Small delay to avoid rate limits\n",
        "    time.sleep(0.1)\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"\\n\\n✓ Completed in {elapsed_time:.1f} seconds ({elapsed_time/len(df):.2f}s per sample)\")\n",
        "\n",
        "# Latency statistics\n",
        "avg_latency = np.mean(latencies)\n",
        "p50_latency = np.percentile(latencies, 50)\n",
        "p95_latency = np.percentile(latencies, 95)\n",
        "p99_latency = np.percentile(latencies, 99)\n",
        "\n",
        "print(\"\\nLatency Statistics:\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"Mean:   {avg_latency:.0f} ms\")\n",
        "print(f\"Median: {p50_latency:.0f} ms\")\n",
        "print(f\"P95:    {p95_latency:.0f} ms\")\n",
        "print(f\"P99:    {p99_latency:.0f} ms\")\n",
        "print(f\"Min:    {min(latencies):.0f} ms\")\n",
        "print(f\"Max:    {max(latencies):.0f} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGhMD8aYVUvA"
      },
      "source": [
        "## Step 6: Calculate Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKeQNGQnVUvA",
        "outputId": "01af1629-06dd-4a41-a12d-d1d98fd4d32c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "EVALUATION METRICS\n",
            "======================================================================\n",
            "\n",
            "Per-Metric Performance:\n",
            "----------------------------------------------------------------------\n",
            "Metric                    MAE     RMSE    Exact       ±1       ±2\n",
            "----------------------------------------------------------------------\n",
            "cognitive_ability        0.14     0.54    92.8%    92.8%   100.0%\n",
            "experience               0.20     0.81    93.6%    94.0%    94.0%\n",
            "problem_solving          0.67     1.41    76.0%    76.8%    82.0%\n",
            "reliability              0.54     1.12    74.8%    74.8%    98.0%\n",
            "professionalism          0.40     0.78    69.6%    90.0%   100.0%\n",
            "communication            0.62     1.08    61.2%    79.6%    96.8%\n",
            "\n",
            "======================================================================\n",
            "Overall Performance (all metrics combined):\n",
            "----------------------------------------------------------------------\n",
            "Mean Absolute Error (MAE):  0.431\n",
            "Root Mean Squared Error:     0.996\n",
            "Exact Match Accuracy:        78.0%\n",
            "Within ±1 Accuracy:          84.7%\n",
            "Within ±2 Accuracy:          95.1%\n"
          ]
        }
      ],
      "source": [
        "# Calculate evaluation metrics\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"EVALUATION METRICS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Per-metric metrics\n",
        "results = {}\n",
        "for metric in METRICS:\n",
        "    true_values = df[metric].values\n",
        "    pred_values = np.array([p[metric] for p in predictions])\n",
        "\n",
        "    mae = np.mean(np.abs(true_values - pred_values))\n",
        "    rmse = np.sqrt(np.mean((true_values - pred_values) ** 2))\n",
        "    exact_match = np.mean(true_values == pred_values)\n",
        "    within_1 = np.mean(np.abs(true_values - pred_values) <= 1)\n",
        "    within_2 = np.mean(np.abs(true_values - pred_values) <= 2)\n",
        "\n",
        "    results[metric] = {\n",
        "        'mae': mae,\n",
        "        'rmse': rmse,\n",
        "        'exact_match': exact_match,\n",
        "        'within_1': within_1,\n",
        "        'within_2': within_2\n",
        "    }\n",
        "\n",
        "# Display per-metric results\n",
        "print(\"\\nPer-Metric Performance:\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Metric':<20} {'MAE':>8} {'RMSE':>8} {'Exact':>8} {'±1':>8} {'±2':>8}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for metric in METRICS:\n",
        "    m = results[metric]\n",
        "    print(f\"{metric:<20} {m['mae']:>8.2f} {m['rmse']:>8.2f} \"\n",
        "          f\"{m['exact_match']*100:>7.1f}% {m['within_1']*100:>7.1f}% {m['within_2']*100:>7.1f}%\")\n",
        "\n",
        "# Overall metrics\n",
        "all_true = df[METRICS].values.flatten()\n",
        "all_pred = np.array([[p[m] for m in METRICS] for p in predictions]).flatten()\n",
        "\n",
        "overall_mae = np.mean(np.abs(all_true - all_pred))\n",
        "overall_rmse = np.sqrt(np.mean((all_true - all_pred) ** 2))\n",
        "overall_exact = np.mean(all_true == all_pred)\n",
        "overall_within_1 = np.mean(np.abs(all_true - all_pred) <= 1)\n",
        "overall_within_2 = np.mean(np.abs(all_true - all_pred) <= 2)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Overall Performance (all metrics combined):\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"Mean Absolute Error (MAE):  {overall_mae:.3f}\")\n",
        "print(f\"Root Mean Squared Error:     {overall_rmse:.3f}\")\n",
        "print(f\"Exact Match Accuracy:        {overall_exact*100:.1f}%\")\n",
        "print(f\"Within ±1 Accuracy:          {overall_within_1*100:.1f}%\")\n",
        "print(f\"Within ±2 Accuracy:          {overall_within_2*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zDZ4LcdVUvA"
      },
      "source": [
        "## Step 7: Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cQAVY42VUvB",
        "outputId": "6a198c80-ecc6-4c3d-9e00-c0bd773408dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Detailed predictions saved: /content/drive/MyDrive/finetuning_output/full_dataset_predictions.csv\n",
            "✓ Metrics saved: /content/drive/MyDrive/finetuning_output/full_dataset_metrics.json\n"
          ]
        }
      ],
      "source": [
        "# Create detailed results dataframe\n",
        "results_df = df.copy()\n",
        "\n",
        "# Add predictions and errors\n",
        "for metric in METRICS:\n",
        "    results_df[f'predicted_{metric}'] = [p[metric] for p in predictions]\n",
        "    results_df[f'error_{metric}'] = results_df[f'predicted_{metric}'] - results_df[metric]\n",
        "\n",
        "results_df['total_abs_error'] = sum(abs(results_df[f'error_{m}']) for m in METRICS)\n",
        "results_df['latency_ms'] = latencies\n",
        "\n",
        "# Save predictions\n",
        "predictions_path = os.path.join(OUTPUT_DIR, \"full_dataset_predictions.csv\")\n",
        "results_df.to_csv(predictions_path, index=False)\n",
        "print(f\"✓ Detailed predictions saved: {predictions_path}\")\n",
        "\n",
        "# Save metrics as JSON\n",
        "metrics_data = {\n",
        "    'model': FINE_TUNED_MODEL,\n",
        "    'total_samples': len(df),\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'per_metric': results,\n",
        "    'overall': {\n",
        "        'mae': float(overall_mae),\n",
        "        'rmse': float(overall_rmse),\n",
        "        'exact_match': float(overall_exact),\n",
        "        'within_1': float(overall_within_1),\n",
        "        'within_2': float(overall_within_2)\n",
        "    },\n",
        "    'latency': {\n",
        "        'mean_ms': float(np.mean(latencies)),\n",
        "        'median_ms': float(np.median(latencies)),\n",
        "        'p95_ms': float(np.percentile(latencies, 95)),\n",
        "        'p99_ms': float(np.percentile(latencies, 99)),\n",
        "        'min_ms': float(min(latencies)),\n",
        "        'max_ms': float(max(latencies))\n",
        "    }\n",
        "}\n",
        "\n",
        "metrics_path = os.path.join(OUTPUT_DIR, \"full_dataset_metrics.json\")\n",
        "with open(metrics_path, 'w') as f:\n",
        "    json.dump(metrics_data, f, indent=2)\n",
        "print(f\"✓ Metrics saved: {metrics_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H22sFc_aVUvB"
      },
      "source": [
        "## Step 8: Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-zh--3FVUvB",
        "outputId": "4c57fe71-0fb5-4950-ae54-c81dd1899a83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ERROR ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "Top 10 Worst Predictions (by total absolute error):\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "#1. Interview ID: sales_representative_004\n",
            "   Role: Sales Representative\n",
            "   Total Absolute Error: 11\n",
            "   Latency: 1732 ms\n",
            "   Scores (Truth → Predicted):\n",
            "     ✓ cognitive_ability   :  8 →  8 (error:  +0)\n",
            "     ✗ experience          :  5 →  8 (error:  +3)\n",
            "     ✗ problem_solving     :  7 →  9 (error:  +2)\n",
            "     ✗ reliability         :  7 →  5 (error:  -2)\n",
            "     ✗ professionalism     :  7 →  5 (error:  -2)\n",
            "     ✗ communication       :  7 →  5 (error:  -2)\n",
            "\n",
            "#2. Interview ID: general_manager_(franchise)_019\n",
            "   Role: General Manager (Franchise)\n",
            "   Total Absolute Error: 9\n",
            "   Latency: 3835 ms\n",
            "   Scores (Truth → Predicted):\n",
            "     ✗ cognitive_ability   :  4 →  6 (error:  +2)\n",
            "     ✓ experience          :  8 →  8 (error:  +0)\n",
            "     ✗ problem_solving     :  9 →  6 (error:  -3)\n",
            "     ✗ reliability         :  1 →  5 (error:  +4)\n",
            "     ✓ professionalism     :  2 →  2 (error:  +0)\n",
            "     ✓ communication       :  1 →  1 (error:  +0)\n",
            "\n",
            "#3. Interview ID: general_manager_(franchise)_030\n",
            "   Role: General Manager (Franchise)\n",
            "   Total Absolute Error: 9\n",
            "   Latency: 1563 ms\n",
            "   Scores (Truth → Predicted):\n",
            "     ✓ cognitive_ability   :  8 →  8 (error:  +0)\n",
            "     ✓ experience          :  8 →  8 (error:  +0)\n",
            "     ✓ problem_solving     :  9 →  9 (error:  +0)\n",
            "     ✗ reliability         :  3 →  7 (error:  +4)\n",
            "     ✗ professionalism     :  5 →  7 (error:  +2)\n",
            "     ✗ communication       :  7 →  4 (error:  -3)\n",
            "\n",
            "#4. Interview ID: customer_service_representative_004\n",
            "   Role: Customer Service Representative\n",
            "   Total Absolute Error: 8\n",
            "   Latency: 1321 ms\n",
            "   Scores (Truth → Predicted):\n",
            "     ✓ cognitive_ability   :  6 →  6 (error:  +0)\n",
            "     ✓ experience          :  5 →  5 (error:  +0)\n",
            "     ✗ problem_solving     :  6 →  9 (error:  +3)\n",
            "     ✗ reliability         :  5 →  3 (error:  -2)\n",
            "     ✗ professionalism     :  4 →  2 (error:  -2)\n",
            "     ✓ communication       :  2 →  1 (error:  -1)\n",
            "\n",
            "#5. Interview ID: customer_service_representative_005\n",
            "   Role: Customer Service Representative\n",
            "   Total Absolute Error: 8\n",
            "   Latency: 1924 ms\n",
            "   Scores (Truth → Predicted):\n",
            "     ✓ cognitive_ability   :  6 →  6 (error:  +0)\n",
            "     ✓ experience          :  4 →  5 (error:  +1)\n",
            "     ✗ problem_solving     :  9 →  6 (error:  -3)\n",
            "     ✓ reliability         :  5 →  5 (error:  +0)\n",
            "     ✗ professionalism     :  4 →  2 (error:  -2)\n",
            "     ✗ communication       :  3 →  1 (error:  -2)\n",
            "\n",
            "#6. Interview ID: general_manager_(franchise)_028\n",
            "   Role: General Manager (Franchise)\n",
            "   Total Absolute Error: 8\n",
            "   Latency: 1158 ms\n",
            "   Scores (Truth → Predicted):\n",
            "     ✗ cognitive_ability   :  4 →  6 (error:  +2)\n",
            "     ✓ experience          :  8 →  8 (error:  +0)\n",
            "     ✗ problem_solving     :  9 →  6 (error:  -3)\n",
            "     ✓ reliability         :  5 →  5 (error:  +0)\n",
            "     ✗ professionalism     :  4 →  2 (error:  -2)\n",
            "     ✓ communication       :  2 →  1 (error:  -1)\n",
            "\n",
            "#7. Interview ID: customer_service_representative_001\n",
            "   Role: Customer Service Representative\n",
            "   Total Absolute Error: 7\n",
            "   Latency: 4329 ms\n",
            "   Scores (Truth → Predicted):\n",
            "     ✓ cognitive_ability   :  6 →  6 (error:  +0)\n",
            "     ✗ experience          :  1 →  5 (error:  +4)\n",
            "     ✗ problem_solving     :  6 →  9 (error:  +3)\n",
            "     ✓ reliability         :  5 →  5 (error:  +0)\n",
            "     ✓ professionalism     :  4 →  4 (error:  +0)\n",
            "     ✓ communication       :  1 →  1 (error:  +0)\n",
            "\n",
            "#8. Interview ID: customer_service_representative_019\n",
            "   Role: Customer Service Representative\n",
            "   Total Absolute Error: 7\n",
            "   Latency: 4139 ms\n",
            "   Scores (Truth → Predicted):\n",
            "     ✓ cognitive_ability   :  6 →  6 (error:  +0)\n",
            "     ✓ experience          :  5 →  5 (error:  +0)\n",
            "     ✗ problem_solving     :  9 →  6 (error:  -3)\n",
            "     ✗ reliability         :  5 →  3 (error:  -2)\n",
            "     ✗ professionalism     :  4 →  2 (error:  -2)\n",
            "     ✓ communication       :  1 →  1 (error:  +0)\n",
            "\n",
            "#9. Interview ID: customer_service_representative_025\n",
            "   Role: Customer Service Representative\n",
            "   Total Absolute Error: 7\n",
            "   Latency: 1046 ms\n",
            "   Scores (Truth → Predicted):\n",
            "     ✓ cognitive_ability   :  8 →  8 (error:  +0)\n",
            "     ✗ experience          :  5 →  8 (error:  +3)\n",
            "     ✓ problem_solving     :  9 →  9 (error:  +0)\n",
            "     ✗ reliability         :  7 →  5 (error:  -2)\n",
            "     ✓ professionalism     :  7 →  6 (error:  -1)\n",
            "     ✓ communication       :  5 →  4 (error:  -1)\n",
            "\n",
            "#10. Interview ID: customer_service_representative_041\n",
            "   Role: Customer Service Representative\n",
            "   Total Absolute Error: 7\n",
            "   Latency: 1665 ms\n",
            "   Scores (Truth → Predicted):\n",
            "     ✓ cognitive_ability   :  8 →  8 (error:  +0)\n",
            "     ✗ experience          :  5 →  8 (error:  +3)\n",
            "     ✓ problem_solving     :  9 →  9 (error:  +0)\n",
            "     ✗ reliability         :  7 →  5 (error:  -2)\n",
            "     ✓ professionalism     :  7 →  6 (error:  -1)\n",
            "     ✓ communication       :  5 →  4 (error:  -1)\n"
          ]
        }
      ],
      "source": [
        "# Error Analysis\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ERROR ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Find worst predictions\n",
        "worst_10 = results_df.nlargest(10, 'total_abs_error')\n",
        "\n",
        "print(\"\\nTop 10 Worst Predictions (by total absolute error):\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for idx, row in worst_10.iterrows():\n",
        "    print(f\"\\n#{list(worst_10.index).index(idx) + 1}. Interview ID: {row['interview_id']}\")\n",
        "    print(f\"   Role: {row['role']}\")\n",
        "    print(f\"   Total Absolute Error: {row['total_abs_error']:.0f}\")\n",
        "    print(f\"   Latency: {row['latency_ms']:.0f} ms\")\n",
        "    print(\"   Scores (Truth → Predicted):\")\n",
        "    for metric in METRICS:\n",
        "        error_indicator = \"✓\" if abs(row[f'error_{metric}']) <= 1 else \"✗\"\n",
        "        print(f\"     {error_indicator} {metric:<20}: {row[metric]:>2} → {row[f'predicted_{metric}']:>2} (error: {row[f'error_{metric}']:+3.0f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJH0yQR6VUvB",
        "outputId": "b16fbe2c-4530-43fc-f9d9-32c19ec23ce1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Top 10 Best Predictions (by total absolute error):\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "#1. Interview ID: customer_service_representative_002\n",
            "   Role: Customer Service Representative\n",
            "   Total Absolute Error: 0\n",
            "   Latency: 2167 ms\n",
            "   Scores (Truth → Predicted):\n",
            "     ✓ cognitive_ability   :  6 →  6 (error:  +0)\n",
            "     ✓ experience          :  5 →  5 (error:  +0)\n",
            "     ✓ problem_solving     :  9 →  9 (error:  +0)\n",
            "     ✓ reliability         :  5 →  5 (error:  +0)\n",
            "     ✓ professionalism     :  4 →  4 (error:  +0)\n",
            "     ✓ communication       :  1 →  1 (error:  +0)\n",
            "\n",
            "#2. Interview ID: customer_service_representative_016\n",
            "   Role: Customer Service Representative\n",
            "   Total Absolute Error: 0\n",
            "   Latency: 1659 ms\n",
            "   Scores (Truth → Predicted):\n",
            "     ✓ cognitive_ability   :  6 →  6 (error:  +0)\n",
            "     ✓ experience          :  5 →  5 (error:  +0)\n",
            "     ✓ problem_solving     :  6 →  6 (error:  +0)\n",
            "     ✓ reliability         :  5 →  5 (error:  +0)\n",
            "     ✓ professionalism     :  4 →  4 (error:  +0)\n",
            "     ✓ communication       :  1 →  1 (error:  +0)\n",
            "\n",
            "#3. Interview ID: customer_service_representative_024\n",
            "   Role: Customer Service Representative\n",
            "   Total Absolute Error: 0\n",
            "   Latency: 1077 ms\n",
            "   Scores (Truth → Predicted):\n",
            "     ✓ cognitive_ability   :  6 →  6 (error:  +0)\n",
            "     ✓ experience          :  5 →  5 (error:  +0)\n",
            "     ✓ problem_solving     :  9 →  9 (error:  +0)\n",
            "     ✓ reliability         :  5 →  5 (error:  +0)\n",
            "     ✓ professionalism     :  4 →  4 (error:  +0)\n",
            "     ✓ communication       :  1 →  1 (error:  +0)\n",
            "\n",
            "#4. Interview ID: customer_service_representative_031\n",
            "   Role: Customer Service Representative\n",
            "   Total Absolute Error: 0\n",
            "   Latency: 1500 ms\n",
            "   Scores (Truth → Predicted):\n",
            "     ✓ cognitive_ability   :  8 →  8 (error:  +0)\n",
            "     ✓ experience          :  8 →  8 (error:  +0)\n",
            "     ✓ problem_solving     :  9 →  9 (error:  +0)\n",
            "     ✓ reliability         :  7 →  7 (error:  +0)\n",
            "     ✓ professionalism     :  6 →  6 (error:  +0)\n",
            "     ✓ communication       :  6 →  6 (error:  +0)\n",
            "\n",
            "#5. Interview ID: customer_service_representative_046\n",
            "   Role: Customer Service Representative\n",
            "   Total Absolute Error: 0\n",
            "   Latency: 1579 ms\n",
            "   Scores (Truth → Predicted):\n",
            "     ✓ cognitive_ability   :  8 →  8 (error:  +0)\n",
            "     ✓ experience          :  8 →  8 (error:  +0)\n",
            "     ✓ problem_solving     :  9 →  9 (error:  +0)\n",
            "     ✓ reliability         :  5 →  5 (error:  +0)\n",
            "     ✓ professionalism     :  7 →  7 (error:  +0)\n",
            "     ✓ communication       :  6 →  6 (error:  +0)\n",
            "\n",
            "#6. Interview ID: customer_service_representative_049\n",
            "   Role: Customer Service Representative\n",
            "   Total Absolute Error: 0\n",
            "   Latency: 1557 ms\n",
            "   Scores (Truth → Predicted):\n",
            "     ✓ cognitive_ability   :  6 →  6 (error:  +0)\n",
            "     ✓ experience          :  5 →  5 (error:  +0)\n",
            "     ✓ problem_solving     :  6 →  6 (error:  +0)\n",
            "     ✓ reliability         :  5 →  5 (error:  +0)\n",
            "     ✓ professionalism     :  4 →  4 (error:  +0)\n",
            "     ✓ communication       :  1 →  1 (error:  +0)\n",
            "\n",
            "#7. Interview ID: sales_representative_001\n",
            "   Role: Sales Representative\n",
            "   Total Absolute Error: 0\n",
            "   Latency: 949 ms\n",
            "   Scores (Truth → Predicted):\n",
            "     ✓ cognitive_ability   :  8 →  8 (error:  +0)\n",
            "     ✓ experience          :  8 →  8 (error:  +0)\n",
            "     ✓ problem_solving     :  7 →  7 (error:  +0)\n",
            "     ✓ reliability         :  5 →  5 (error:  +0)\n",
            "     ✓ professionalism     :  5 →  5 (error:  +0)\n",
            "     ✓ communication       :  5 →  5 (error:  +0)\n",
            "\n",
            "#8. Interview ID: sales_representative_011\n",
            "   Role: Sales Representative\n",
            "   Total Absolute Error: 0\n",
            "   Latency: 1787 ms\n",
            "   Scores (Truth → Predicted):\n",
            "     ✓ cognitive_ability   :  6 →  6 (error:  +0)\n",
            "     ✓ experience          :  5 →  5 (error:  +0)\n",
            "     ✓ problem_solving     :  5 →  5 (error:  +0)\n",
            "     ✓ reliability         :  3 →  3 (error:  +0)\n",
            "     ✓ professionalism     :  2 →  2 (error:  +0)\n",
            "     ✓ communication       :  1 →  1 (error:  +0)\n",
            "\n",
            "#9. Interview ID: sales_representative_016\n",
            "   Role: Sales Representative\n",
            "   Total Absolute Error: 0\n",
            "   Latency: 1545 ms\n",
            "   Scores (Truth → Predicted):\n",
            "     ✓ cognitive_ability   :  6 →  6 (error:  +0)\n",
            "     ✓ experience          :  5 →  5 (error:  +0)\n",
            "     ✓ problem_solving     :  5 →  5 (error:  +0)\n",
            "     ✓ reliability         :  3 →  3 (error:  +0)\n",
            "     ✓ professionalism     :  2 →  2 (error:  +0)\n",
            "     ✓ communication       :  1 →  1 (error:  +0)\n",
            "\n",
            "#10. Interview ID: sales_representative_024\n",
            "   Role: Sales Representative\n",
            "   Total Absolute Error: 0\n",
            "   Latency: 954 ms\n",
            "   Scores (Truth → Predicted):\n",
            "     ✓ cognitive_ability   :  8 →  8 (error:  +0)\n",
            "     ✓ experience          :  8 →  8 (error:  +0)\n",
            "     ✓ problem_solving     :  9 →  9 (error:  +0)\n",
            "     ✓ reliability         :  5 →  5 (error:  +0)\n",
            "     ✓ professionalism     :  5 →  5 (error:  +0)\n",
            "     ✓ communication       :  5 →  5 (error:  +0)\n"
          ]
        }
      ],
      "source": [
        "# Show some examples of best predictions\n",
        "best_10 = results_df.nsmallest(10, 'total_abs_error')\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Top 10 Best Predictions (by total absolute error):\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for idx, row in best_10.iterrows():\n",
        "    print(f\"\\n#{list(best_10.index).index(idx) + 1}. Interview ID: {row['interview_id']}\")\n",
        "    print(f\"   Role: {row['role']}\")\n",
        "    print(f\"   Total Absolute Error: {row['total_abs_error']:.0f}\")\n",
        "    print(f\"   Latency: {row['latency_ms']:.0f} ms\")\n",
        "    print(\"   Scores (Truth → Predicted):\")\n",
        "    for metric in METRICS:\n",
        "        error_indicator = \"✓\" if abs(row[f'error_{metric}']) == 0 else \"~\"\n",
        "        print(f\"     {error_indicator} {metric:<20}: {row[metric]:>2} → {row[f'predicted_{metric}']:>2} (error: {row[f'error_{metric}']:+3.0f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PszPe6s6VUvB"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXIXxn0-VUvB",
        "outputId": "6080a735-ba8c-4317-f1a8-fb88a1df6766"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "EVALUATION COMPLETE\n",
            "======================================================================\n",
            "\n",
            "Summary:\n",
            "  Model: ft:gpt-4o-2024-08-06:cleanagent:interview-scorer:ChhOUzkh\n",
            "  Total samples evaluated: 250\n",
            "  Overall MAE: 0.431\n",
            "  Overall RMSE: 0.996\n",
            "  Exact Match: 78.0%\n",
            "  Within ±1: 84.7%\n",
            "  Within ±2: 95.1%\n",
            "\n",
            "Latency:\n",
            "  Mean: 1614 ms\n",
            "  P95: 3570 ms\n",
            "\n",
            "Files saved:\n",
            "  - /content/drive/MyDrive/finetuning_output/full_dataset_predictions.csv\n",
            "  - /content/drive/MyDrive/finetuning_output/full_dataset_metrics.json\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"EVALUATION COMPLETE\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nSummary:\")\n",
        "print(f\"  Model: {FINE_TUNED_MODEL}\")\n",
        "print(f\"  Total samples evaluated: {len(df)}\")\n",
        "print(f\"  Overall MAE: {overall_mae:.3f}\")\n",
        "print(f\"  Overall RMSE: {overall_rmse:.3f}\")\n",
        "print(f\"  Exact Match: {overall_exact*100:.1f}%\")\n",
        "print(f\"  Within ±1: {overall_within_1*100:.1f}%\")\n",
        "print(f\"  Within ±2: {overall_within_2*100:.1f}%\")\n",
        "print(f\"\\nLatency:\")\n",
        "print(f\"  Mean: {np.mean(latencies):.0f} ms\")\n",
        "print(f\"  P95: {np.percentile(latencies, 95):.0f} ms\")\n",
        "print(f\"\\nFiles saved:\")\n",
        "print(f\"  - {predictions_path}\")\n",
        "print(f\"  - {metrics_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eYF8GEl1ZXNo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}