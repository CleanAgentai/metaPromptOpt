{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "friB-KJe8q-5"
      },
      "source": [
        "# Synthetic Interview Dataset Generator\n",
        "\n",
        "This notebook generates synthetic interview data for 5 service roles:\n",
        "- Customer Service Representative\n",
        "- Sales Representative\n",
        "- Field Technician\n",
        "- Home Service Technician\n",
        "- General Manager (Franchise)\n",
        "\n",
        "Each role will have 50 candidates with varying quality levels based on these metrics:\n",
        "- Cognitive Ability (35%)\n",
        "- Experience (35%)\n",
        "- Problem Solving (15%)\n",
        "- Reliability (5%)\n",
        "- Professionalism (5%)\n",
        "- Communication (5%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9KzDv0EU8q-7"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install huggingface_hub pandas numpy tqdm openai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SrDiHjGe8q-8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from typing import Dict, List, Tuple\n",
        "import random\n",
        "\n",
        "# Import based on provider choice\n",
        "try:\n",
        "    from huggingface_hub import InferenceClient\n",
        "except ImportError:\n",
        "    print(\"Warning: huggingface_hub not found. Install with: pip install huggingface_hub\")\n",
        "\n",
        "try:\n",
        "    from openai import OpenAI\n",
        "except ImportError:\n",
        "    print(\"Warning: openai not found. Install with: pip install openai\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pzTY5fdl8q-8"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "# Choose your inference provider:\n",
        "# Option 1: Hugging Face Inference API (recommended)\n",
        "# Option 2: OpenAI-compatible API (Novita AI, Together AI, etc.)\n",
        "\n",
        "INFERENCE_PROVIDER = \"huggingface\"  # \"huggingface\" or \"openai_compatible\"\n",
        "\n",
        "# For Hugging Face\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "HF_MODEL = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "# For OpenAI-compatible APIs (Novita AI, Together AI, etc.)\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")  # or NOVITA_API_KEY\n",
        "OPENAI_BASE_URL = \"https://api.novita.ai/v3/openai\"  # Change based on your provider\n",
        "OPENAI_MODEL = \"meta-llama/llama-3.1-8b-instruct\"  # Novita format\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = InferenceClient(token=HF_TOKEN)\n",
        "MODEL_NAME = HF_MODEL"
      ],
      "metadata": {
        "id": "DqBYeuGP9Ffr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "CJ6a3zr38q-8"
      },
      "outputs": [],
      "source": [
        "# # Initialize the appropriate client\n",
        "# if INFERENCE_PROVIDER == \"huggingface\":\n",
        "#     print(\"Using Hugging Face Inference API\")\n",
        "#     if not HF_TOKEN:\n",
        "#         raise ValueError(\"HF_TOKEN environment variable not set\")\n",
        "#     client = InferenceClient(token=HF_TOKEN)\n",
        "#     MODEL_NAME = HF_MODEL\n",
        "\n",
        "# elif INFERENCE_PROVIDER == \"openai_compatible\":\n",
        "#     print(\"Using OpenAI-compatible API\")\n",
        "#     if not OPENAI_API_KEY:\n",
        "#         raise ValueError(\"OPENAI_API_KEY environment variable not set\")\n",
        "#     client = OpenAI(\n",
        "#         api_key=OPENAI_API_KEY,\n",
        "#         base_url=OPENAI_BASE_URL\n",
        "#     )\n",
        "#     MODEL_NAME = OPENAI_MODEL\n",
        "\n",
        "# else:\n",
        "#     raise ValueError(f\"Invalid INFERENCE_PROVIDER: {INFERENCE_PROVIDER}\")\n",
        "\n",
        "# print(f\"Model: {MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "CsYhQUzJ8q-8"
      },
      "outputs": [],
      "source": [
        "# Define roles and metrics\n",
        "ROLES = [\n",
        "    \"Customer Service Representative\",\n",
        "    \"Sales Representative\",\n",
        "    \"Field Technician\",\n",
        "    \"Home Service Technician\",\n",
        "    \"General Manager (Franchise)\"\n",
        "]\n",
        "\n",
        "METRICS_DEF = \"\\n\".join([\n",
        "    \"- Cognitive Ability (35%): Structured thinking, planning, logic.\",\n",
        "    \"- Experience (35%): Relevant work (last 10 years), skills, accomplishments in similar service jobs.\",\n",
        "    \"- Problem Solving (15%): Resourcefulness, safe tradeoffs under constraints.\",\n",
        "    \"- Reliability (5%): Punctuality, follow-through, transport reliability.\",\n",
        "    \"- Professionalism (5%): Respect for clients/rules, composure under stress.\",\n",
        "    \"- Communication (5%): Clarity and tone; IGNORE filler words.\"\n",
        "])\n",
        "\n",
        "NUM_QUESTIONS = 10\n",
        "NUM_YES_NO_QUESTIONS = 3\n",
        "NUM_CANDIDATES_PER_ROLE = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "tGlkJafY8q-8"
      },
      "outputs": [],
      "source": [
        "def call_llm(messages: List[Dict], max_tokens: int = 1000, temperature: float = 0.7) -> str:\n",
        "    \"\"\"Call the LLM with given messages and return response.\"\"\"\n",
        "    try:\n",
        "        if INFERENCE_PROVIDER == \"huggingface\":\n",
        "            response = client.chat.completions.create(\n",
        "                model=MODEL_NAME,\n",
        "                messages=messages,\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=temperature\n",
        "            )\n",
        "            return response.choices[0].message.content.strip()\n",
        "\n",
        "        elif INFERENCE_PROVIDER == \"openai_compatible\":\n",
        "            response = client.chat.completions.create(\n",
        "                model=MODEL_NAME,\n",
        "                messages=messages,\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=temperature\n",
        "            )\n",
        "            return response.choices[0].message.content.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling LLM: {e}\")\n",
        "        time.sleep(2)  # Wait before retry\n",
        "        return call_llm(messages, max_tokens, temperature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "BQR50Y7_8q-9"
      },
      "outputs": [],
      "source": [
        "def generate_questions_for_role(role: str) -> List[Dict[str, str]]:\n",
        "    \"\"\"Generate interview questions for a specific role.\"\"\"\n",
        "    prompt = f\"\"\"You are an expert interviewer creating interview questions for a {role} position.\n",
        "\n",
        "Based on these evaluation metrics:\n",
        "{METRICS_DEF}\n",
        "\n",
        "Generate exactly {NUM_QUESTIONS} interview questions:\n",
        "- {NUM_YES_NO_QUESTIONS} should be yes/no questions\n",
        "- {NUM_QUESTIONS - NUM_YES_NO_QUESTIONS} should be open-ended questions\n",
        "\n",
        "The questions should assess the candidate across all metrics.\n",
        "\n",
        "Format your response as a JSON array with this structure:\n",
        "[{{\"question\": \"...\", \"type\": \"yes_no\"}}, {{\"question\": \"...\", \"type\": \"open_ended\"}}, ...]\n",
        "\n",
        "Return ONLY the JSON array, no additional text.\"\"\"\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = call_llm(messages, max_tokens=1500, temperature=0.8)\n",
        "\n",
        "    # Parse JSON from response\n",
        "    try:\n",
        "        # Try to extract JSON if wrapped in markdown\n",
        "        if \"```json\" in response:\n",
        "            response = response.split(\"```json\")[1].split(\"```\")[0].strip()\n",
        "        elif \"```\" in response:\n",
        "            response = response.split(\"```\")[1].split(\"```\")[0].strip()\n",
        "\n",
        "        questions = json.loads(response)\n",
        "        return questions\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error parsing questions JSON: {e}\")\n",
        "        print(f\"Response: {response}\")\n",
        "        # Fallback: generate simple questions\n",
        "        return [\n",
        "            {\"question\": f\"Question {i+1} for {role}\", \"type\": \"yes_no\" if i < NUM_YES_NO_QUESTIONS else \"open_ended\"}\n",
        "            for i in range(NUM_QUESTIONS)\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "WpqumK9M8q-9"
      },
      "outputs": [],
      "source": [
        "def assign_candidate_score() -> Tuple[float, str]:\n",
        "    \"\"\"Randomly assign a candidate quality score and label.\"\"\"\n",
        "    score = np.random.uniform(1.0, 10.0)\n",
        "\n",
        "    if score >= 8.0:\n",
        "        quality = \"good\"\n",
        "    elif score >= 5.0:\n",
        "        quality = \"moderate\"\n",
        "    else:\n",
        "        quality = \"poor\"\n",
        "\n",
        "    return round(score, 2), quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "2VO--0ws8q-9"
      },
      "outputs": [],
      "source": [
        "def generate_metric_scores(overall_score: float) -> Dict[str, float]:\n",
        "    \"\"\"Generate individual metric scores based on overall score.\"\"\"\n",
        "    # Add some variance but keep it centered around overall score\n",
        "    variance = 1.5\n",
        "\n",
        "    scores = {\n",
        "        \"cognitive_ability\": np.clip(np.random.normal(overall_score, variance), 1, 10),\n",
        "        \"experience\": np.clip(np.random.normal(overall_score, variance), 1, 10),\n",
        "        \"problem_solving\": np.clip(np.random.normal(overall_score, variance), 1, 10),\n",
        "        \"reliability\": np.clip(np.random.normal(overall_score, variance), 1, 10),\n",
        "        \"professionalism\": np.clip(np.random.normal(overall_score, variance), 1, 10),\n",
        "        \"communication\": np.clip(np.random.normal(overall_score, variance), 1, 10)\n",
        "    }\n",
        "\n",
        "    return {k: round(v, 2) for k, v in scores.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "q7IqK5BW8q-9"
      },
      "outputs": [],
      "source": [
        "def generate_answer(question: str, question_type: str, role: str, overall_score: float, quality: str) -> str:\n",
        "    \"\"\"Generate a candidate's answer based on their quality level.\"\"\"\n",
        "\n",
        "    quality_description = {\n",
        "        \"good\": \"excellent candidate with strong experience, clear thinking, and great problem-solving skills\",\n",
        "        \"moderate\": \"average candidate with some relevant experience but room for improvement\",\n",
        "        \"poor\": \"weak candidate with limited experience, unclear thinking, or poor communication\"\n",
        "    }\n",
        "\n",
        "    prompt = f\"\"\"You are roleplaying as a {quality} candidate (score: {overall_score}/10) interviewing for a {role} position.\n",
        "\n",
        "A {quality} candidate is: {quality_description[quality]}\n",
        "\n",
        "Question: {question}\n",
        "Question Type: {question_type}\n",
        "\n",
        "Respond naturally as this candidate would. Your answer should reflect your quality level:\n",
        "- Good candidates (8-10): Detailed, structured, shows experience and insight\n",
        "- Moderate candidates (5-8): Adequate but may lack depth or specific examples\n",
        "- Poor candidates (1-5): Vague, inexperienced, or shows poor judgment\n",
        "\n",
        "For yes/no questions, start with yes or no, then briefly explain.\n",
        "Keep answers realistic and conversational (2-5 sentences for open-ended, 1-2 for yes/no).\n",
        "\n",
        "Return ONLY the answer, no additional formatting or labels.\"\"\"\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    answer = call_llm(messages, max_tokens=300, temperature=0.9)\n",
        "\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "42MFtKGQ8q-9"
      },
      "outputs": [],
      "source": [
        "def generate_interview(role: str, questions: List[Dict], interview_id: str) -> Dict:\n",
        "    \"\"\"Generate a complete interview for one candidate.\"\"\"\n",
        "\n",
        "    # Assign candidate quality\n",
        "    overall_score, quality = assign_candidate_score()\n",
        "    metric_scores = generate_metric_scores(overall_score)\n",
        "\n",
        "    # Generate Q&A pairs\n",
        "    qa_pairs = []\n",
        "    for q in questions:\n",
        "        answer = generate_answer(\n",
        "            question=q[\"question\"],\n",
        "            question_type=q[\"type\"],\n",
        "            role=role,\n",
        "            overall_score=overall_score,\n",
        "            quality=quality\n",
        "        )\n",
        "\n",
        "        qa_pairs.append({\n",
        "            \"question\": q[\"question\"],\n",
        "            \"question_type\": q[\"type\"],\n",
        "            \"answer\": answer\n",
        "        })\n",
        "\n",
        "        time.sleep(0.5)  # Rate limiting\n",
        "\n",
        "    # Construct interview record\n",
        "    interview = {\n",
        "        \"interview_id\": interview_id,\n",
        "        \"role\": role,\n",
        "        \"overall_score\": overall_score,\n",
        "        \"quality\": quality,\n",
        "        \"metric_scores\": metric_scores,\n",
        "        \"qa_pairs\": qa_pairs,\n",
        "        \"full_transcript\": \"\\n\\n\".join([\n",
        "            f\"Q: {qa['question']}\\nA: {qa['answer']}\"\n",
        "            for qa in qa_pairs\n",
        "        ])\n",
        "    }\n",
        "\n",
        "    return interview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq8CtY6u8q--"
      },
      "source": [
        "## Generate Questions for Each Role\n",
        "\n",
        "First, we'll generate the interview questions for each role."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WavmB-qx8q--",
        "outputId": "4234c45d-de71-40f7-d9a6-5ded236481c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating interview questions for each role...\n",
            "\n",
            "Generating questions for: Customer Service Representative\n",
            "  Generated 10 questions\n",
            "  Yes/No questions: 4\n",
            "  Open-ended questions: 6\n",
            "\n",
            "Generating questions for: Sales Representative\n",
            "Error parsing questions JSON: Expecting ',' delimiter: line 11 column 165 (char 1248)\n",
            "Response: [\n",
            "  {\"question\": \"Have you ever worked with CRM software?\", \"type\": \"yes_no\"},\n",
            "  {\"question\": \"Can you describe a time when you had to meet an ambitious sales target within a tight deadline?\", \"type\": \"open_ended\"},\n",
            "  {\"question\": \"Do you have a valid driver's license?\", \"type\": \"yes_no\"},\n",
            "  {\"question\": \"How would you handle a situation where a client is unsatisfied with the product or service?\", \"type\": \"open_ended\"},\n",
            "  {\"question\": \"Have you ever had to adjust your sales strategy on the fly to meet a client's changing needs?\", \"type\": \"open_ended\"},\n",
            "  {\"question\": \"How do you prioritize your tasks and manage your time effectively?\", \"type\": \"open_ended\"},\n",
            "  {\"question\": \"Can you tell me about a particularly challenging sales pitch you've made and how you overcame any obstacles?\", \"type\": \"open_ended\"},\n",
            "  {\"question\": \"Have you ever had to travel or be away from home for an extended period of time for work?\", \"type\": \"yes_no\"},\n",
            "  {\"question\": \"How do you maintain a professional demeanor under pressure or when dealing with difficult clients?\", \"type\": \"open_ended\"},\n",
            "  {\"question\": \"Can you describe your experience with sales reporting and analysis, and how you use that data to inform your sales strategy?\", \"type\": \"open_ended\"}\n",
            "  Generated 10 questions\n",
            "  Yes/No questions: 3\n",
            "  Open-ended questions: 7\n",
            "\n",
            "Generating questions for: Field Technician\n",
            "  Generated 10 questions\n",
            "  Yes/No questions: 3\n",
            "  Open-ended questions: 7\n",
            "\n",
            "Generating questions for: Home Service Technician\n",
            "  Generated 10 questions\n",
            "  Yes/No questions: 3\n",
            "  Open-ended questions: 7\n",
            "\n",
            "Generating questions for: General Manager (Franchise)\n",
            "  Generated 10 questions\n",
            "  Yes/No questions: 4\n",
            "  Open-ended questions: 6\n",
            "\n",
            "✓ Questions generated for all roles\n"
          ]
        }
      ],
      "source": [
        "# Generate questions for each role\n",
        "role_questions = {}\n",
        "\n",
        "print(\"Generating interview questions for each role...\\n\")\n",
        "for role in ROLES:\n",
        "    print(f\"Generating questions for: {role}\")\n",
        "    questions = generate_questions_for_role(role)\n",
        "    role_questions[role] = questions\n",
        "\n",
        "    print(f\"  Generated {len(questions)} questions\")\n",
        "    print(f\"  Yes/No questions: {sum(1 for q in questions if q['type'] == 'yes_no')}\")\n",
        "    print(f\"  Open-ended questions: {sum(1 for q in questions if q['type'] == 'open_ended')}\\n\")\n",
        "\n",
        "    time.sleep(1)  # Rate limiting between roles\n",
        "\n",
        "print(\"✓ Questions generated for all roles\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZP9_YuT8q--",
        "outputId": "4d1f9f19-875c-4948-dfc7-9c5a453df00b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample questions for Customer Service Representative:\n",
            "\n",
            "1. [yes_no] Have you worked in a customer-facing role for at least 6 months?\n",
            "2. [open_ended] Tell me about a time when you had to handle a high volume of customer calls or queries within a tight timeframe, and how you managed your workload.\n",
            "3. [yes_no] Are you available to work a standard 8-hour shift, 5 days a week?\n"
          ]
        }
      ],
      "source": [
        "# Display sample questions\n",
        "sample_role = ROLES[0]\n",
        "print(f\"Sample questions for {sample_role}:\\n\")\n",
        "for i, q in enumerate(role_questions[sample_role][:3], 1):\n",
        "    print(f\"{i}. [{q['type']}] {q['question']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LyNNjhK8q--"
      },
      "source": [
        "## Generate Synthetic Interview Dataset\n",
        "\n",
        "Now we'll generate 50 candidates for each role with varying quality levels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2cDSh4H8q--",
        "outputId": "b05a6be8-194b-4b0e-ebb1-ef04cb348906"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating 50 interviews per role...\n",
            "\n",
            "Total interviews to generate: 250\n",
            "\n",
            "\n",
            "============================================================\n",
            "Generating interviews for: Customer Service Representative\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Customer Service Representative: 100%|██████████| 50/50 [22:04<00:00, 26.49s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Generating interviews for: Sales Representative\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sales Representative: 100%|██████████| 50/50 [19:11<00:00, 23.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Generating interviews for: Field Technician\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Field Technician: 100%|██████████| 50/50 [23:00<00:00, 27.61s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Generating interviews for: Home Service Technician\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Home Service Technician: 100%|██████████| 50/50 [24:28<00:00, 29.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Generating interviews for: General Manager (Franchise)\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "General Manager (Franchise): 100%|██████████| 50/50 [23:40<00:00, 28.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Generated 250 total interviews\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate complete dataset\n",
        "all_interviews = []\n",
        "\n",
        "print(f\"Generating {NUM_CANDIDATES_PER_ROLE} interviews per role...\\n\")\n",
        "print(f\"Total interviews to generate: {len(ROLES) * NUM_CANDIDATES_PER_ROLE}\\n\")\n",
        "\n",
        "for role in ROLES:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Generating interviews for: {role}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    questions = role_questions[role]\n",
        "\n",
        "    for candidate_num in tqdm(range(NUM_CANDIDATES_PER_ROLE), desc=f\"{role}\"):\n",
        "        interview_id = f\"{role.lower().replace(' ', '_')}_{candidate_num+1:03d}\"\n",
        "\n",
        "        interview = generate_interview(role, questions, interview_id)\n",
        "        all_interviews.append(interview)\n",
        "\n",
        "        # Save checkpoint every 10 interviews\n",
        "        if (candidate_num + 1) % 10 == 0:\n",
        "            temp_df = pd.DataFrame(all_interviews)\n",
        "            temp_df.to_json(\"interview_dataset_checkpoint.json\", orient=\"records\", indent=2)\n",
        "\n",
        "print(f\"\\n✓ Generated {len(all_interviews)} total interviews\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeclt9Wp8q--"
      },
      "source": [
        "## Analyze Dataset Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-Pg78Il8q--",
        "outputId": "91e114a4-25d9-42ad-84b9-53224e3776d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Overview:\n",
            "Total interviews: 250\n",
            "\n",
            "Interviews per role:\n",
            "role\n",
            "Customer Service Representative    50\n",
            "Sales Representative               50\n",
            "Field Technician                   50\n",
            "Home Service Technician            50\n",
            "General Manager (Franchise)        50\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Candidate quality distribution:\n",
            "quality\n",
            "poor        110\n",
            "moderate     82\n",
            "good         58\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Quality distribution (%)\n",
            "quality\n",
            "poor        44.0\n",
            "moderate    32.8\n",
            "good        23.2\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Overall score statistics:\n",
            "count    250.000000\n",
            "mean       5.575800\n",
            "std        2.598592\n",
            "min        1.060000\n",
            "25%        3.387500\n",
            "50%        5.675000\n",
            "75%        7.940000\n",
            "max        9.930000\n",
            "Name: overall_score, dtype: float64\n",
            "\n",
            "Score distribution by quality:\n",
            "          count      mean       std   min     25%    50%     75%   max\n",
            "quality                                                               \n",
            "good       58.0  8.927069  0.614232  8.02  8.3325  8.970  9.5300  9.93\n",
            "moderate   82.0  6.610366  0.898375  5.04  5.7975  6.665  7.4250  7.99\n",
            "poor      110.0  3.037545  1.160009  1.06  2.0775  3.190  4.0275  4.97\n"
          ]
        }
      ],
      "source": [
        "# Create DataFrame\n",
        "df = pd.DataFrame(all_interviews)\n",
        "\n",
        "print(\"Dataset Overview:\")\n",
        "print(f\"Total interviews: {len(df)}\")\n",
        "print(f\"\\nInterviews per role:\")\n",
        "print(df['role'].value_counts())\n",
        "\n",
        "print(f\"\\nCandidate quality distribution:\")\n",
        "print(df['quality'].value_counts())\n",
        "print(f\"\\nQuality distribution (%)\")\n",
        "print(df['quality'].value_counts(normalize=True) * 100)\n",
        "\n",
        "print(f\"\\nOverall score statistics:\")\n",
        "print(df['overall_score'].describe())\n",
        "\n",
        "print(f\"\\nScore distribution by quality:\")\n",
        "print(df.groupby('quality')['overall_score'].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UBPxOBl8q--",
        "outputId": "a7f9faf0-1101-4169-87d6-81a5173ae1b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Quality distribution by role:\n",
            "quality                          good  moderate  poor\n",
            "role                                                 \n",
            "Customer Service Representative  26.0      38.0  36.0\n",
            "Field Technician                 20.0      28.0  52.0\n",
            "General Manager (Franchise)      20.0      34.0  46.0\n",
            "Home Service Technician          34.0      32.0  34.0\n",
            "Sales Representative             16.0      32.0  52.0\n"
          ]
        }
      ],
      "source": [
        "# Quality distribution by role\n",
        "print(\"\\nQuality distribution by role:\")\n",
        "quality_by_role = pd.crosstab(df['role'], df['quality'], normalize='index') * 100\n",
        "print(quality_by_role.round(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Erx3NMPW8q--"
      },
      "source": [
        "## Save Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSV9Ta2cXVhY",
        "outputId": "9de97e94-74c0-49de-a22a-1e9838b94958"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emu0Irob8q--",
        "outputId": "2fb0879a-207a-4c02-d168-f711232a8ebe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Saved: synthetic_interview_dataset.json\n",
            "✓ Saved: synthetic_interview_dataset.csv\n",
            "✓ Saved: synthetic_interview_qa_detailed.csv\n",
            "\n",
            "✓ Dataset generation complete!\n",
            "Total interviews: 250\n",
            "Total Q&A pairs: 2500\n"
          ]
        }
      ],
      "source": [
        "# Save complete dataset in multiple formats\n",
        "\n",
        "# 1. JSON format (preserves nested structure)\n",
        "df.to_json(\"synthetic_interview_dataset.json\", orient=\"records\", indent=2)\n",
        "print(\"✓ Saved: synthetic_interview_dataset.json\")\n",
        "\n",
        "# 2. CSV format (flattened for easy analysis)\n",
        "df_flat = df.copy()\n",
        "df_flat['qa_text'] = df_flat['full_transcript']\n",
        "df_flat = df_flat.drop(columns=['qa_pairs', 'full_transcript'])\n",
        "\n",
        "# Flatten metric scores\n",
        "metric_scores_df = pd.json_normalize(df_flat['metric_scores'])\n",
        "df_flat = pd.concat([df_flat.drop(columns=['metric_scores']), metric_scores_df], axis=1)\n",
        "\n",
        "df_flat.to_csv(\"synthetic_interview_dataset.csv\", index=False)\n",
        "print(\"✓ Saved: synthetic_interview_dataset.csv\")\n",
        "\n",
        "# 3. Detailed Q&A format\n",
        "qa_records = []\n",
        "for _, interview in df.iterrows():\n",
        "    for i, qa in enumerate(interview['qa_pairs']):\n",
        "        qa_records.append({\n",
        "            'interview_id': interview['interview_id'],\n",
        "            'role': interview['role'],\n",
        "            'overall_score': interview['overall_score'],\n",
        "            'quality': interview['quality'],\n",
        "            'question_num': i + 1,\n",
        "            'question': qa['question'],\n",
        "            'question_type': qa['question_type'],\n",
        "            'answer': qa['answer']\n",
        "        })\n",
        "\n",
        "qa_df = pd.DataFrame(qa_records)\n",
        "qa_df.to_csv(\"synthetic_interview_qa_detailed.csv\", index=False)\n",
        "print(\"✓ Saved: synthetic_interview_qa_detailed.csv\")\n",
        "\n",
        "print(f\"\\n✓ Dataset generation complete!\")\n",
        "print(f\"Total interviews: {len(df)}\")\n",
        "print(f\"Total Q&A pairs: {len(qa_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoylgNry8q--"
      },
      "source": [
        "## Sample Interview Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_EGcce08q--"
      },
      "outputs": [],
      "source": [
        "# Display sample interviews\n",
        "print(\"Sample Interviews:\\n\")\n",
        "\n",
        "for quality in ['good', 'moderate', 'poor']:\n",
        "    sample = df[df['quality'] == quality].iloc[0]\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"EXAMPLE {quality.upper()} CANDIDATE\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Interview ID: {sample['interview_id']}\")\n",
        "    print(f\"Role: {sample['role']}\")\n",
        "    print(f\"Overall Score: {sample['overall_score']}/10\")\n",
        "    print(f\"Quality: {sample['quality']}\")\n",
        "    print(f\"\\nMetric Scores:\")\n",
        "    for metric, score in sample['metric_scores'].items():\n",
        "        print(f\"  - {metric.replace('_', ' ').title()}: {score}\")\n",
        "    print(f\"\\nInterview Transcript (first 3 Q&A):\")\n",
        "    print(\"-\" * 80)\n",
        "    for i, qa in enumerate(sample['qa_pairs'][:3], 1):\n",
        "        print(f\"\\nQ{i}: {qa['question']}\")\n",
        "        print(f\"A{i}: {qa['answer']}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWv9_2fT8q--"
      },
      "source": [
        "## Dataset Summary\n",
        "\n",
        "The generated dataset includes:\n",
        "\n",
        "1. **synthetic_interview_dataset.json** - Complete dataset with nested structure\n",
        "2. **synthetic_interview_dataset.csv** - Flattened dataset for analysis\n",
        "3. **synthetic_interview_qa_detailed.csv** - Detailed Q&A pairs\n",
        "\n",
        "Each interview contains:\n",
        "- Unique interview ID\n",
        "- Role information\n",
        "- Overall quality score (1-10)\n",
        "- Quality label (good/moderate/poor)\n",
        "- Individual metric scores\n",
        "- 10 Q&A pairs (3 yes/no, 7 open-ended)\n",
        "- Full transcript"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}