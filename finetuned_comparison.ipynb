{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tuned GPT-4o vs Self-Consistency Models\n",
        "\n",
        "Comparing fine-tuned (K=1) against self-consistency (K=5, K=7) on accuracy, latency, and cost.\n",
        "\n",
        "**Models**: GPT-4o K=5/K=7, GPT-3.5 K=5/K=7, Llama K=5/K=7, Fine-tuned K=1\n",
        "\n",
        "**Dataset**: 250 interviews with human ground truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pandas numpy scipy matplotlib seaborn pingouin -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import mannwhitneyu\n",
        "import pingouin as pg\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "print(\"\u2713 Setup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Update paths to match your local file structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# UPDATE THESE PATHS!\n",
        "BASE_PATH = '/Users/shreya_sudan/Desktop/ServiceAgent/CleanAgent/metaPromptOpt/data'\n",
        "\n",
        "FILE_PATHS = {\n",
        "    'gpt4o_k5': f'{BASE_PATH}/k5RunsResults/final_gpt-4o_20251113-222901.csv',\n",
        "    'gpt4o_k7': f'{BASE_PATH}/k7RunsResults/final_gpt-4o_20251120-163127.csv',\n",
        "    'gpt35_k5': f'{BASE_PATH}/k5RunsResults/final_gpt-3.5-turbo_20251113-211454.csv',\n",
        "    'gpt35_k7': f'{BASE_PATH}/k7RunsResults/final_gpt-3.5-turbo_20251120-151235.csv',\n",
        "    'llama_k5': f'{BASE_PATH}/k5RunsResults/final_llama_k5.csv',  # Update with actual filename\n",
        "    'llama_k7': f'{BASE_PATH}/k7RunsResults/final_llama_k7.csv',  # Update with actual filename\n",
        "    'finetuned': f'{BASE_PATH}/finetuning_output/full_dataset_predictions.csv',\n",
        "    'human': f'{BASE_PATH}/humanScores/hiring_evaluations.csv'\n",
        "}\n",
        "\n",
        "MODELS_CONFIG = {\n",
        "    'gpt4o_k5': {'label': 'GPT-4o (K=5)', 'k': 5, 'cost_in': 2.50/1000, 'cost_out': 10.00/1000, 'color': '#3498db'},\n",
        "    'gpt4o_k7': {'label': 'GPT-4o (K=7)', 'k': 7, 'cost_in': 2.50/1000, 'cost_out': 10.00/1000, 'color': '#2980b9'},\n",
        "    'finetuned': {'label': 'GPT-4o FT (K=1)', 'k': 1, 'cost_in': 2.50/1000, 'cost_out': 10.00/1000, 'training_cost': 26.12, 'color': '#e74c3c'},\n",
        "    'gpt35_k5': {'label': 'GPT-3.5 (K=5)', 'k': 5, 'cost_in': 0.50/1000, 'cost_out': 1.50/1000, 'color': '#95a5a6'},\n",
        "    'gpt35_k7': {'label': 'GPT-3.5 (K=7)', 'k': 7, 'cost_in': 0.50/1000, 'cost_out': 1.50/1000, 'color': '#7f8c8d'},\n",
        "    'llama_k5': {'label': 'Llama (K=5)', 'k': 5, 'cost_in': 0.10/1000, 'cost_out': 0.20/1000, 'color': '#34495e'},\n",
        "    'llama_k7': {'label': 'Llama (K=7)', 'k': 7, 'cost_in': 0.10/1000, 'cost_out': 0.20/1000, 'color': '#2c3e50'},\n",
        "}\n",
        "\n",
        "METRICS = ['cognitive_ability', 'experience', 'problem_solving', 'reliability', 'professionalism', 'communication']\n",
        "ABBREV = {'cognitive_ability': 'ca', 'experience': 'exp', 'problem_solving': 'ps', \n",
        "          'reliability': 'rel', 'professionalism': 'prof', 'communication': 'comm'}\n",
        "\n",
        "print(\"\u2713 Config loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load & Standardize Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_standardize(path, model_key):\n",
        "    \"\"\"Load and standardize column names to {metric}_score format\"\"\"\n",
        "    df = pd.read_csv(path)\n",
        "    df_out = df.copy()\n",
        "    \n",
        "    # K-runs: ca_score -> cognitive_ability_score\n",
        "    if 'ca_score' in df.columns:\n",
        "        for metric, abbrev in ABBREV.items():\n",
        "            if f'{abbrev}_score' in df.columns:\n",
        "                df_out[f'{metric}_score'] = df[f'{abbrev}_score']\n",
        "    \n",
        "    # Fine-tuned: predicted_cognitive_ability -> cognitive_ability_score\n",
        "    elif model_key == 'finetuned':\n",
        "        for metric in METRICS:\n",
        "            if f'predicted_{metric}' in df.columns:\n",
        "                df_out[f'{metric}_score'] = df[f'predicted_{metric}']\n",
        "    \n",
        "    # Human: cognitive_ability -> cognitive_ability_score\n",
        "    elif model_key == 'human':\n",
        "        for metric in METRICS:\n",
        "            if metric in df.columns:\n",
        "                df_out[f'{metric}_score'] = df[metric]\n",
        "    \n",
        "    return df_out\n",
        "\n",
        "# Load all\n",
        "print(\"Loading datasets...\")\n",
        "data = {}\n",
        "for key, path in FILE_PATHS.items():\n",
        "    try:\n",
        "        data[key] = load_and_standardize(path, key)\n",
        "        print(f\"  \u2713 {key}: {len(data[key])} samples\")\n",
        "    except Exception as e:\n",
        "        print(f\"  \u26a0\ufe0f  {key}: {str(e)[:50]}\")\n",
        "\n",
        "print(f\"\\n\u2713 Loaded {len(data)} datasets\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate Metrics vs Human"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_metrics(df_model, df_human):\n",
        "    \"\"\"Calculate MAE, ICC, agreement vs human\"\"\"\n",
        "    merged = pd.merge(df_model[['interview_id'] + [f'{m}_score' for m in METRICS]],\n",
        "                     df_human[['interview_id'] + [f'{m}_score' for m in METRICS]],\n",
        "                     on='interview_id', suffixes=('_m', '_h'))\n",
        "    \n",
        "    all_m = np.concatenate([merged[f'{m}_score_m'].values for m in METRICS])\n",
        "    all_h = np.concatenate([merged[f'{m}_score_h'].values for m in METRICS])\n",
        "    \n",
        "    # Calculate ICC\n",
        "    try:\n",
        "        icc_vals = []\n",
        "        for metric in METRICS:\n",
        "            icc_df = pd.DataFrame({\n",
        "                'target': list(merged['interview_id'].astype(str)) * 2,\n",
        "                'rater': ['model']*len(merged) + ['human']*len(merged),\n",
        "                'score': list(merged[f'{metric}_score_m']) + list(merged[f'{metric}_score_h'])\n",
        "            })\n",
        "            icc_result = pg.intraclass_corr(data=icc_df, targets='target', raters='rater', ratings='score')\n",
        "            icc_vals.append(icc_result[icc_result['Type'] == 'ICC2']['ICC'].values[0])\n",
        "        avg_icc = np.mean(icc_vals)\n",
        "    except:\n",
        "        avg_icc = np.nan\n",
        "    \n",
        "    return {\n",
        "        'mae': np.mean(np.abs(all_m - all_h)),\n",
        "        'rmse': np.sqrt(np.mean((all_m - all_h)**2)),\n",
        "        'icc': avg_icc,\n",
        "        'within_1': np.mean(np.abs(all_m - all_h) <= 1) * 100,\n",
        "        'within_2': np.mean(np.abs(all_m - all_h) <= 2) * 100,\n",
        "        'n': len(merged)\n",
        "    }\n",
        "\n",
        "# Calculate for all models\n",
        "print(\"\\nCalculating metrics vs human...\")\n",
        "results = {}\n",
        "for key in MODELS_CONFIG.keys():\n",
        "    if key in data and 'human' in data:\n",
        "        results[key] = calc_metrics(data[key], data['human'])\n",
        "        print(f\"  {MODELS_CONFIG[key]['label']}: MAE={results[key]['mae']:.3f}, ICC={results[key]['icc']:.3f}\")\n",
        "\n",
        "print(\"\\n\u2713 Metrics calculated\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract Latency & Calculate Costs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract latency and costs from actual data\n",
        "print(\"Extracting latency and costs from data...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for key in results.keys():\n",
        "    df = data[key]\n",
        "    cfg = MODELS_CONFIG[key]\n",
        "    \n",
        "    # LATENCY\n",
        "    lat_cols = [c for c in df.columns if 'latency' in c.lower() and 'ms' in c.lower()]\n",
        "    if lat_cols:\n",
        "        # K-runs might have rewrite_latency_ms (this is per sample, already includes all K runs)\n",
        "        results[key]['latency_ms'] = df[lat_cols[0]].median()\n",
        "        results[key]['total_latency_ms'] = results[key]['latency_ms']  # Already total for K runs\n",
        "    else:\n",
        "        results[key]['latency_ms'] = np.nan\n",
        "        results[key]['total_latency_ms'] = np.nan\n",
        "    \n",
        "    # COST\n",
        "    # For K-runs, use actual total_cost from data (already includes K runs)\n",
        "    if 'total_cost' in df.columns:\n",
        "        # This is the actual cost per candidate (already averaged over K runs)\n",
        "        cost_per_cand = df['total_cost'].mean()\n",
        "        results[key]['cost_per_cand'] = cost_per_cand\n",
        "        results[key]['cost_250'] = cost_per_cand * 250\n",
        "        results[key]['cost_source'] = 'actual'\n",
        "    \n",
        "    # For fine-tuned, calculate from config\n",
        "    elif key == 'finetuned':\n",
        "        # Fine-tuned uses same token costs but only K=1\n",
        "        AVG_IN_TOKENS, AVG_OUT_TOKENS = 1000, 100\n",
        "        inf_cost = AVG_IN_TOKENS * cfg['cost_in'] + AVG_OUT_TOKENS * cfg['cost_out']\n",
        "        train_cost = cfg.get('training_cost', 0)\n",
        "        \n",
        "        results[key]['cost_per_cand'] = inf_cost + (train_cost / 250)\n",
        "        results[key]['cost_250'] = inf_cost * 250 + train_cost\n",
        "        results[key]['training_cost'] = train_cost\n",
        "        results[key]['cost_source'] = 'estimated+training'\n",
        "    \n",
        "    else:\n",
        "        # Fallback estimation\n",
        "        AVG_IN_TOKENS, AVG_OUT_TOKENS = 1000, 100\n",
        "        inf_cost = (AVG_IN_TOKENS * cfg['cost_in'] + AVG_OUT_TOKENS * cfg['cost_out']) * cfg['k']\n",
        "        results[key]['cost_per_cand'] = inf_cost\n",
        "        results[key]['cost_250'] = inf_cost * 250\n",
        "        results[key]['cost_source'] = 'estimated'\n",
        "    \n",
        "    print(f\"  {cfg['label']}:\")\n",
        "    print(f\"    Latency: {results[key].get('latency_ms', 0):.0f}ms (total for K={cfg['k']})\")\n",
        "    print(f\"    Cost: ${results[key]['cost_per_cand']:.4f}/cand (source: {results[key].get('cost_source', 'unknown')})\")\n",
        "    if key == 'finetuned':\n",
        "        print(f\"    Training cost: ${cfg.get('training_cost', 0):.2f} (one-time)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\u2713 Latency and cost data extracted\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build table\n",
        "rows = []\n",
        "for key, res in results.items():\n",
        "    rows.append({\n",
        "        'Model': MODELS_CONFIG[key]['label'],\n",
        "        'K': MODELS_CONFIG[key]['k'],\n",
        "        'MAE': res['mae'],\n",
        "        'ICC': res['icc'],\n",
        "        'Within \u00b11%': res['within_1'],\n",
        "        'Total Latency (ms)': res['total_latency_ms'],\n",
        "        'Cost/Cand ($)': res['cost_per_cand'],\n",
        "        'Cost 250 ($)': res['cost_250'],\n",
        "        '_key': key\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(rows).sort_values('MAE')\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPREHENSIVE RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(df[['Model', 'K', 'MAE', 'ICC', 'Within \u00b11%', 'Total Latency (ms)', 'Cost/Cand ($)']].to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BEST PERFORMERS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\ud83c\udfaf Best Accuracy: {df.iloc[0]['Model']} (MAE={df.iloc[0]['MAE']:.3f})\")\n",
        "print(f\"\ud83d\udcb0 Lowest Cost: {df.sort_values('Cost/Cand ($)').iloc[0]['Model']} (${df.sort_values('Cost/Cand ($)').iloc[0]['Cost/Cand ($)']:.4f}/cand)\")\n",
        "print(f\"\u26a1 Fastest: {df.sort_values('Total Latency (ms)').iloc[0]['Model']} ({df.sort_values('Total Latency (ms)').iloc[0]['Total Latency (ms)']:.0f}ms total)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "fig.suptitle('Fine-tuned vs Self-Consistency: Comprehensive Comparison', fontsize=16, fontweight='bold')\n",
        "\n",
        "colors = [MODELS_CONFIG[row['_key']]['color'] for _, row in df.iterrows()]\n",
        "\n",
        "# 1. Cost vs Accuracy\n",
        "ax = axes[0, 0]\n",
        "for _, row in df.iterrows():\n",
        "    ax.scatter(row['Cost 250 ($)'], row['ICC'], s=400, alpha=0.7, \n",
        "              color=MODELS_CONFIG[row['_key']]['color'], edgecolors='black', linewidths=2)\n",
        "    ax.annotate(row['Model'].split('(')[0], (row['Cost 250 ($)'], row['ICC']), \n",
        "               fontsize=8, xytext=(3, 3), textcoords='offset points')\n",
        "ax.set_xlabel('Total Cost 250 ($)', fontweight='bold')\n",
        "ax.set_ylabel('ICC', fontweight='bold')\n",
        "ax.set_title('Cost vs Accuracy')\n",
        "ax.grid(alpha=0.3)\n",
        "ax.axhline(0.75, color='green', linestyle='--', alpha=0.5)\n",
        "\n",
        "# 2. MAE\n",
        "ax = axes[0, 1]\n",
        "ax.barh(range(len(df)), df['MAE'], color=colors, edgecolor='black')\n",
        "ax.set_yticks(range(len(df)))\n",
        "ax.set_yticklabels(df['Model'], fontsize=9)\n",
        "ax.set_xlabel('MAE (Lower Better)', fontweight='bold')\n",
        "ax.set_title('Accuracy')\n",
        "ax.invert_xaxis()\n",
        "ax.grid(axis='x', alpha=0.3)\n",
        "\n",
        "# 3. Latency\n",
        "ax = axes[0, 2]\n",
        "ax.barh(range(len(df)), df['Total Latency (ms)'], color=colors, edgecolor='black')\n",
        "ax.set_yticks(range(len(df)))\n",
        "ax.set_yticklabels(df['Model'], fontsize=9)\n",
        "ax.set_xlabel('Total Latency (ms)', fontweight='bold')\n",
        "ax.set_title('Speed')\n",
        "ax.invert_xaxis()\n",
        "ax.grid(axis='x', alpha=0.3)\n",
        "\n",
        "# 4. Agreement\n",
        "ax = axes[1, 0]\n",
        "x = np.arange(len(df))\n",
        "ax.bar(x, df['Within \u00b11%'], color=colors, edgecolor='black', alpha=0.8)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels([m.split('(')[0] for m in df['Model']], rotation=45, ha='right', fontsize=8)\n",
        "ax.set_ylabel('Within \u00b11 (%)', fontweight='bold')\n",
        "ax.set_title('Agreement Rate')\n",
        "ax.axhline(80, color='green', linestyle='--', alpha=0.5)\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 5. K vs Accuracy\n",
        "ax = axes[1, 1]\n",
        "for _, row in df.iterrows():\n",
        "    ax.scatter(row['K'], row['MAE'], s=300, color=MODELS_CONFIG[row['_key']]['color'],\n",
        "              edgecolors='black', linewidths=2, alpha=0.7)\n",
        "ax.set_xlabel('K Runs', fontweight='bold')\n",
        "ax.set_ylabel('MAE', fontweight='bold')\n",
        "ax.set_title('K vs Accuracy')\n",
        "ax.set_xticks([1, 3, 5, 7])\n",
        "ax.grid(alpha=0.3)\n",
        "ax.invert_yaxis()\n",
        "\n",
        "# 6. Efficiency Score\n",
        "ax = axes[1, 2]\n",
        "norm_icc = df['ICC'] / df['ICC'].max()\n",
        "norm_cost = 1 - (df['Cost/Cand ($)'] / df['Cost/Cand ($)'].max())\n",
        "norm_speed = 1 - (df['Total Latency (ms)'] / df['Total Latency (ms)'].max())\n",
        "eff = (norm_icc * 0.5 + norm_cost * 0.25 + norm_speed * 0.25) * 100\n",
        "ax.barh(range(len(df)), eff, color=colors, edgecolor='black')\n",
        "ax.set_yticks(range(len(df)))\n",
        "ax.set_yticklabels(df['Model'], fontsize=9)\n",
        "ax.set_xlabel('Efficiency Score', fontweight='bold')\n",
        "ax.set_title('Overall Efficiency\\n(50% Acc, 25% Cost, 25% Speed)')\n",
        "ax.grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ROI Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'finetuned' in results and 'gpt4o_k5' in results:\n",
        "    ft = results['finetuned']\n",
        "    k5 = results['gpt4o_k5']\n",
        "    \n",
        "    training_cost = 26.12\n",
        "    savings_per_cand = k5['cost_per_cand'] - ft['cost_per_cand']\n",
        "    breakeven = training_cost / savings_per_cand if savings_per_cand > 0 else np.inf\n",
        "    \n",
        "    latency_improvement = ((k5['total_latency_ms'] - ft['total_latency_ms']) / k5['total_latency_ms']) * 100\n",
        "    mae_change = ((ft['mae'] - k5['mae']) / k5['mae']) * 100\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ROI ANALYSIS: Fine-tuned (K=1) vs GPT-4o (K=5)\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\n\ud83d\udcb0 COST:\")\n",
        "    print(f\"   Training (one-time): ${training_cost:.2f}\")\n",
        "    print(f\"   Savings per candidate: ${savings_per_cand:.4f}\")\n",
        "    print(f\"   Break-even: {breakeven:.0f} candidates\")\n",
        "    print(f\"   Savings @ 1000 candidates: ${savings_per_cand * 1000:.2f}\")\n",
        "    \n",
        "    print(f\"\\n\u26a1 SPEED:\")\n",
        "    print(f\"   K=5: {k5['total_latency_ms']:.0f}ms total\")\n",
        "    print(f\"   K=1: {ft['total_latency_ms']:.0f}ms total\")\n",
        "    print(f\"   Improvement: {latency_improvement:.1f}% faster\")\n",
        "    \n",
        "    print(f\"\\n\ud83c\udfaf ACCURACY:\")\n",
        "    print(f\"   K=5 MAE: {k5['mae']:.3f}\")\n",
        "    print(f\"   K=1 MAE: {ft['mae']:.3f}\")\n",
        "    print(f\"   Change: {mae_change:+.1f}% {'(worse)' if mae_change > 0 else '(better)'}\")\n",
        "    \n",
        "    print(f\"\\n\ud83d\udcca RECOMMENDATION:\")\n",
        "    if breakeven <= 500 and abs(mae_change) < 5:\n",
        "        print(f\"   \u2705 STRONG ROI: Break-even at {breakeven:.0f} candidates with maintained accuracy\")\n",
        "    elif breakeven <= 1000:\n",
        "        print(f\"   \u2705 Good ROI: Break-even at {breakeven:.0f} candidates\")\n",
        "    else:\n",
        "        print(f\"   \u26a0\ufe0f  High volume needed: Break-even at {breakeven:.0f} candidates\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}