{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Comprehensive AI vs Human Rater Analysis\n",
        "\n",
        "Advanced comparison of Llama AI scoring vs Human Recruiter and Service Owner ratings.\n",
        "\n",
        "## Analysis Sections\n",
        "\n",
        "1. **Setup & Data Loading**\n",
        "2. **Overall Agreement Metrics**\n",
        "   - Inter-rater reliability (ICC, Cohen's Kappa)\n",
        "   - Agreement rates at different thresholds\n",
        "   - Correlation analysis\n",
        "3. **Score Distribution Analysis**\n",
        "   - Distribution comparison plots\n",
        "   - Statistical tests for differences\n",
        "   - Bias detection\n",
        "4. **Metric-by-Metric Deep Dive**\n",
        "   - Per-metric agreement\n",
        "   - Bland-Altman plots\n",
        "   - Confusion matrices\n",
        "5. **Disagreement Analysis**\n",
        "   - High-disagreement cases\n",
        "   - Systematic bias patterns\n",
        "   - Agreement heatmaps\n",
        "6. **Summary & Recommendations**"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup & Data Loading"
      ],
      "metadata": {
        "id": "setup_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install pandas numpy scipy matplotlib seaborn pingouin -q"
      ],
      "metadata": {
        "id": "install"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import pearsonr, spearmanr, wilcoxon, mannwhitneyu\n",
        "import pingouin as pg  # For ICC and advanced statistics\n",
        "from typing import Dict, List, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['savefig.dpi'] = 300"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mount"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "llamaPath = '/content/drive/MyDrive/mvp/finalScores20251024-000950.csv'\n",
        "recruiterPath = '/content/drive/MyDrive/mvp/candidates_v2_recruiter_graded.csv'\n",
        "ownerPath = '/content/drive/MyDrive/mvp/candidates_v2_service_owner_graded.csv'\n",
        "\n",
        "llama_df = pd.read_csv(llamaPath)\n",
        "recruiter_raw = pd.read_csv(recruiterPath)\n",
        "owner_raw = pd.read_csv(ownerPath)\n",
        "\n",
        "print(f\"Loaded data:\")\n",
        "print(f\"  Llama: {llama_df.shape[0]} interviews\")\n",
        "print(f\"  Recruiter: {recruiter_raw.shape[0]} interviews\")\n",
        "print(f\"  Service Owner: {owner_raw.shape[0]} interviews\")"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse human scores (extract numeric scores from \"N – justification\" format)\n",
        "METRICS_HUMAN = [\"Cognitive ability\", \"Experience\", \"Problem Solving\", \n",
        "                 \"Reliability\", \"Professionalism\", \"Communication\"]\n",
        "\n",
        "def extract_score_and_justification(df, metric_cols):\n",
        "    \"\"\"Extract numeric scores from human rating format.\"\"\"\n",
        "    out = df.copy()\n",
        "    for metric in metric_cols:\n",
        "        base = metric.lower().replace(\" \", \"_\")\n",
        "        score_col = f\"{base}_score\"\n",
        "        just_col = f\"{base}_justification\"\n",
        "        # Extract numeric score\n",
        "        out[score_col] = out[metric].str.extract(r'(\\d+)').astype(float)\n",
        "        # Extract justification\n",
        "        out[just_col] = out[metric].str.replace(r'^\\s*\\d+\\s*[–-]\\s*', '', regex=True).str.strip()\n",
        "    return out\n",
        "\n",
        "recruiter_df = extract_score_and_justification(recruiter_raw, METRICS_HUMAN)\n",
        "owner_df = extract_score_and_justification(owner_raw, METRICS_HUMAN)\n",
        "\n",
        "print(\"\\n✓ Parsed human scores\")"
      ],
      "metadata": {
        "id": "parse_scores"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define metric columns\n",
        "METRICS = [\"cognitive_ability\", \"experience\", \"problem_solving\", \n",
        "           \"reliability\", \"professionalism\", \"communication\"]\n",
        "SCORE_COLS = [f\"{m}_score\" for m in METRICS]\n",
        "\n",
        "# Create unified dataframe for analysis\n",
        "def prepare_comparison_df(llama, recruiter, owner):\n",
        "    \"\"\"Merge all three scoring sources by interview_id.\"\"\"\n",
        "    # Select relevant columns\n",
        "    llama_scores = llama[['interview_id'] + SCORE_COLS].copy()\n",
        "    recruiter_scores = recruiter[['interview_id'] + SCORE_COLS].copy()\n",
        "    owner_scores = owner[['interview_id'] + SCORE_COLS].copy()\n",
        "    \n",
        "    # Rename columns with suffix\n",
        "    llama_scores.columns = ['interview_id'] + [f\"{c}_llama\" for c in SCORE_COLS]\n",
        "    recruiter_scores.columns = ['interview_id'] + [f\"{c}_recruiter\" for c in SCORE_COLS]\n",
        "    owner_scores.columns = ['interview_id'] + [f\"{c}_owner\" for c in SCORE_COLS]\n",
        "    \n",
        "    # Merge\n",
        "    df = llama_scores.merge(recruiter_scores, on='interview_id', how='outer')\n",
        "    df = df.merge(owner_scores, on='interview_id', how='outer')\n",
        "    \n",
        "    return df\n",
        "\n",
        "comparison_df = prepare_comparison_df(llama_df, recruiter_df, owner_df)\n",
        "\n",
        "print(f\"\\n✓ Created comparison dataframe: {comparison_df.shape[0]} interviews\")\n",
        "print(f\"  Available comparisons:\")\n",
        "print(f\"    Llama vs Recruiter: {comparison_df[[c for c in comparison_df.columns if 'llama' in c][0]].notna().sum()}\")\n",
        "print(f\"    Llama vs Owner: {comparison_df[[c for c in comparison_df.columns if 'owner' in c][0]].notna().sum()}\")\n",
        "\n",
        "display(comparison_df.head())"
      ],
      "metadata": {
        "id": "prepare_comparison"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Overall Agreement Metrics\n",
        "\n",
        "### 2.1 Inter-Rater Reliability"
      ],
      "metadata": {
        "id": "agreement_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_icc(df, metric, rater1_col, rater2_col):\n",
        "    \"\"\"Calculate Intraclass Correlation Coefficient.\"\"\"\n",
        "    # Prepare data in long format for ICC\n",
        "    data = []\n",
        "    for idx, row in df.iterrows():\n",
        "        if pd.notna(row[rater1_col]) and pd.notna(row[rater2_col]):\n",
        "            data.append({'interview': idx, 'rater': 'rater1', 'score': row[rater1_col]})\n",
        "            data.append({'interview': idx, 'rater': 'rater2', 'score': row[rater2_col]})\n",
        "    \n",
        "    if len(data) < 4:\n",
        "        return np.nan, np.nan, np.nan\n",
        "    \n",
        "    df_long = pd.DataFrame(data)\n",
        "    \n",
        "    try:\n",
        "        icc_result = pg.intraclass_corr(data=df_long, targets='interview', \n",
        "                                        raters='rater', ratings='score')\n",
        "        # Get ICC(2,1) - Two-way random effects, absolute agreement, single rater\n",
        "        icc_row = icc_result[icc_result['Type'] == 'ICC2']\n",
        "        if len(icc_row) > 0:\n",
        "            return icc_row['ICC'].values[0], icc_row['CI95%'].values[0][0], icc_row['CI95%'].values[0][1]\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    return np.nan, np.nan, np.nan\n",
        "\n",
        "def calculate_cohens_kappa_ordinal(df, rater1_col, rater2_col):\n",
        "    \"\"\"Calculate weighted Cohen's Kappa for ordinal data.\"\"\"\n",
        "    mask = df[rater1_col].notna() & df[rater2_col].notna()\n",
        "    if mask.sum() < 2:\n",
        "        return np.nan\n",
        "    \n",
        "    r1 = df.loc[mask, rater1_col].astype(int).values\n",
        "    r2 = df.loc[mask, rater2_col].astype(int).values\n",
        "    \n",
        "    try:\n",
        "        from sklearn.metrics import cohen_kappa_score\n",
        "        return cohen_kappa_score(r1, r2, weights='linear')\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "# Calculate ICC and Kappa for each metric and rater pair\n",
        "print(\"=\" * 80)\n",
        "print(\"INTER-RATER RELIABILITY ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "reliability_results = []\n",
        "\n",
        "for metric in METRICS:\n",
        "    metric_col = f\"{metric}_score\"\n",
        "    \n",
        "    print(f\"\\n{metric.replace('_', ' ').title()}:\")\n",
        "    \n",
        "    # Llama vs Recruiter\n",
        "    llama_col = f\"{metric_col}_llama\"\n",
        "    rec_col = f\"{metric_col}_recruiter\"\n",
        "    owner_col = f\"{metric_col}_owner\"\n",
        "    \n",
        "    # ICC\n",
        "    icc_lr, icc_lr_low, icc_lr_high = calculate_icc(comparison_df, metric, llama_col, rec_col)\n",
        "    icc_lo, icc_lo_low, icc_lo_high = calculate_icc(comparison_df, metric, llama_col, owner_col)\n",
        "    icc_ro, icc_ro_low, icc_ro_high = calculate_icc(comparison_df, metric, rec_col, owner_col)\n",
        "    \n",
        "    # Correlations\n",
        "    mask_lr = comparison_df[llama_col].notna() & comparison_df[rec_col].notna()\n",
        "    mask_lo = comparison_df[llama_col].notna() & comparison_df[owner_col].notna()\n",
        "    mask_ro = comparison_df[rec_col].notna() & comparison_df[owner_col].notna()\n",
        "    \n",
        "    pearson_lr = pearsonr(comparison_df.loc[mask_lr, llama_col], \n",
        "                         comparison_df.loc[mask_lr, rec_col])[0] if mask_lr.sum() > 1 else np.nan\n",
        "    pearson_lo = pearsonr(comparison_df.loc[mask_lo, llama_col], \n",
        "                         comparison_df.loc[mask_lo, owner_col])[0] if mask_lo.sum() > 1 else np.nan\n",
        "    pearson_ro = pearsonr(comparison_df.loc[mask_ro, rec_col], \n",
        "                         comparison_df.loc[mask_ro, owner_col])[0] if mask_ro.sum() > 1 else np.nan\n",
        "    \n",
        "    print(f\"  Llama vs Recruiter:\")\n",
        "    print(f\"    ICC: {icc_lr:.3f} (95% CI: [{icc_lr_low:.3f}, {icc_lr_high:.3f}])\")\n",
        "    print(f\"    Pearson r: {pearson_lr:.3f}\")\n",
        "    \n",
        "    print(f\"  Llama vs Owner:\")\n",
        "    print(f\"    ICC: {icc_lo:.3f} (95% CI: [{icc_lo_low:.3f}, {icc_lo_high:.3f}])\")\n",
        "    print(f\"    Pearson r: {pearson_lo:.3f}\")\n",
        "    \n",
        "    print(f\"  Recruiter vs Owner:\")\n",
        "    print(f\"    ICC: {icc_ro:.3f} (95% CI: [{icc_ro_low:.3f}, {icc_ro_high:.3f}])\")\n",
        "    print(f\"    Pearson r: {pearson_ro:.3f}\")\n",
        "    \n",
        "    reliability_results.append({\n",
        "        'metric': metric,\n",
        "        'icc_llama_recruiter': icc_lr,\n",
        "        'icc_llama_owner': icc_lo,\n",
        "        'icc_recruiter_owner': icc_ro,\n",
        "        'corr_llama_recruiter': pearson_lr,\n",
        "        'corr_llama_owner': pearson_lo,\n",
        "        'corr_recruiter_owner': pearson_ro\n",
        "    })\n",
        "\n",
        "reliability_df = pd.DataFrame(reliability_results)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\\nICC Interpretation:\")\n",
        "print(\"  < 0.50: Poor reliability\")\n",
        "print(\"  0.50-0.75: Moderate reliability\")\n",
        "print(\"  0.75-0.90: Good reliability\")\n",
        "print(\"  > 0.90: Excellent reliability\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "icc_analysis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Agreement Rates"
      ],
      "metadata": {
        "id": "agreement_rates_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_agreement_rates(df, llama_col, human_col, thresholds=[0, 1, 2]):\n",
        "    \"\"\"Calculate % agreement within different score thresholds.\"\"\"\n",
        "    mask = df[llama_col].notna() & df[human_col].notna()\n",
        "    if mask.sum() == 0:\n",
        "        return {t: np.nan for t in thresholds}\n",
        "    \n",
        "    diffs = (df.loc[mask, llama_col] - df.loc[mask, human_col]).abs()\n",
        "    total = len(diffs)\n",
        "    \n",
        "    return {t: (diffs <= t).sum() / total * 100 for t in thresholds}\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"AGREEMENT RATES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "agreement_results = []\n",
        "\n",
        "for metric in METRICS:\n",
        "    metric_col = f\"{metric}_score\"\n",
        "    llama_col = f\"{metric_col}_llama\"\n",
        "    rec_col = f\"{metric_col}_recruiter\"\n",
        "    owner_col = f\"{metric_col}_owner\"\n",
        "    \n",
        "    # Llama vs Recruiter\n",
        "    agree_lr = calculate_agreement_rates(comparison_df, llama_col, rec_col)\n",
        "    # Llama vs Owner\n",
        "    agree_lo = calculate_agreement_rates(comparison_df, llama_col, owner_col)\n",
        "    \n",
        "    print(f\"\\n{metric.replace('_', ' ').title()}:\")\n",
        "    print(f\"  Llama vs Recruiter:\")\n",
        "    print(f\"    Exact match: {agree_lr[0]:.1f}%\")\n",
        "    print(f\"    Within ±1 point: {agree_lr[1]:.1f}%\")\n",
        "    print(f\"    Within ±2 points: {agree_lr[2]:.1f}%\")\n",
        "    \n",
        "    print(f\"  Llama vs Owner:\")\n",
        "    print(f\"    Exact match: {agree_lo[0]:.1f}%\")\n",
        "    print(f\"    Within ±1 point: {agree_lo[1]:.1f}%\")\n",
        "    print(f\"    Within ±2 points: {agree_lo[2]:.1f}%\")\n",
        "    \n",
        "    agreement_results.append({\n",
        "        'metric': metric,\n",
        "        'exact_llama_recruiter': agree_lr[0],\n",
        "        'within1_llama_recruiter': agree_lr[1],\n",
        "        'within2_llama_recruiter': agree_lr[2],\n",
        "        'exact_llama_owner': agree_lo[0],\n",
        "        'within1_llama_owner': agree_lo[1],\n",
        "        'within2_llama_owner': agree_lo[2]\n",
        "    })\n",
        "\n",
        "agreement_df = pd.DataFrame(agreement_results)\n",
        "print(\"\\n\" + \"=\" * 80)"
      ],
      "metadata": {
        "id": "agreement_rates"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Visualization: ICC Heatmap"
      ],
      "metadata": {
        "id": "icc_viz_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ICC heatmap\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Reshape data for heatmap\n",
        "icc_matrix_lr = reliability_df.pivot_table(index='metric', values='icc_llama_recruiter').values.reshape(-1, 1)\n",
        "icc_matrix_lo = reliability_df.pivot_table(index='metric', values='icc_llama_owner').values.reshape(-1, 1)\n",
        "icc_matrix_ro = reliability_df.pivot_table(index='metric', values='icc_recruiter_owner').values.reshape(-1, 1)\n",
        "\n",
        "metric_labels = [m.replace('_', ' ').title() for m in METRICS]\n",
        "\n",
        "# Plot 1: Llama vs Recruiter\n",
        "sns.heatmap(icc_matrix_lr, annot=True, fmt='.3f', cmap='RdYlGn', vmin=0, vmax=1,\n",
        "            yticklabels=metric_labels, xticklabels=['ICC'], ax=axes[0], cbar_kws={'label': 'ICC'})\n",
        "axes[0].set_title('Llama vs Recruiter', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Plot 2: Llama vs Owner\n",
        "sns.heatmap(icc_matrix_lo, annot=True, fmt='.3f', cmap='RdYlGn', vmin=0, vmax=1,\n",
        "            yticklabels=metric_labels, xticklabels=['ICC'], ax=axes[1], cbar_kws={'label': 'ICC'})\n",
        "axes[1].set_title('Llama vs Service Owner', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Plot 3: Recruiter vs Owner\n",
        "sns.heatmap(icc_matrix_ro, annot=True, fmt='.3f', cmap='RdYlGn', vmin=0, vmax=1,\n",
        "            yticklabels=metric_labels, xticklabels=['ICC'], ax=axes[2], cbar_kws={'label': 'ICC'})\n",
        "axes[2].set_title('Recruiter vs Service Owner', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ ICC values:\")\n",
        "print(\"  Green (>0.75): Good to excellent agreement\")\n",
        "print(\"  Yellow (0.50-0.75): Moderate agreement\")\n",
        "print(\"  Red (<0.50): Poor agreement\")"
      ],
      "metadata": {
        "id": "icc_heatmap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Visualization: Agreement Rates"
      ],
      "metadata": {
        "id": "agreement_viz_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot agreement rates\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Llama vs Recruiter\n",
        "x = np.arange(len(METRICS))\n",
        "width = 0.25\n",
        "\n",
        "axes[0].bar(x - width, agreement_df['exact_llama_recruiter'], width, label='Exact Match', color='#2ecc71')\n",
        "axes[0].bar(x, agreement_df['within1_llama_recruiter'], width, label='Within ±1', color='#3498db')\n",
        "axes[0].bar(x + width, agreement_df['within2_llama_recruiter'], width, label='Within ±2', color='#9b59b6')\n",
        "\n",
        "axes[0].set_ylabel('Agreement Rate (%)', fontsize=11)\n",
        "axes[0].set_title('Llama vs Recruiter Agreement', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xticks(x)\n",
        "axes[0].set_xticklabels([m.replace('_', '\\n').title() for m in METRICS], rotation=0, ha='center')\n",
        "axes[0].legend()\n",
        "axes[0].set_ylim(0, 100)\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Llama vs Owner\n",
        "axes[1].bar(x - width, agreement_df['exact_llama_owner'], width, label='Exact Match', color='#2ecc71')\n",
        "axes[1].bar(x, agreement_df['within1_llama_owner'], width, label='Within ±1', color='#3498db')\n",
        "axes[1].bar(x + width, agreement_df['within2_llama_owner'], width, label='Within ±2', color='#9b59b6')\n",
        "\n",
        "axes[1].set_ylabel('Agreement Rate (%)', fontsize=11)\n",
        "axes[1].set_title('Llama vs Service Owner Agreement', fontsize=12, fontweight='bold')\n",
        "axes[1].set_xticks(x)\n",
        "axes[1].set_xticklabels([m.replace('_', '\\n').title() for m in METRICS], rotation=0, ha='center')\n",
        "axes[1].legend()\n",
        "axes[1].set_ylim(0, 100)\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "agreement_plot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Score Distribution Analysis\n",
        "\n",
        "### 3.1 Distribution Comparison"
      ],
      "metadata": {
        "id": "distribution_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate overall statistics for each rater\n",
        "print(\"=\" * 80)\n",
        "print(\"SCORE DISTRIBUTION STATISTICS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for metric in METRICS:\n",
        "    metric_col = f\"{metric}_score\"\n",
        "    llama_col = f\"{metric_col}_llama\"\n",
        "    rec_col = f\"{metric_col}_recruiter\"\n",
        "    owner_col = f\"{metric_col}_owner\"\n",
        "    \n",
        "    print(f\"\\n{metric.replace('_', ' ').title()}:\")\n",
        "    \n",
        "    # Llama\n",
        "    llama_scores = comparison_df[llama_col].dropna()\n",
        "    print(f\"  Llama: Mean={llama_scores.mean():.2f}, Std={llama_scores.std():.2f}, \"\n",
        "          f\"Median={llama_scores.median():.0f}, Range=[{llama_scores.min():.0f}, {llama_scores.max():.0f}]\")\n",
        "    \n",
        "    # Recruiter\n",
        "    rec_scores = comparison_df[rec_col].dropna()\n",
        "    if len(rec_scores) > 0:\n",
        "        print(f\"  Recruiter: Mean={rec_scores.mean():.2f}, Std={rec_scores.std():.2f}, \"\n",
        "              f\"Median={rec_scores.median():.0f}, Range=[{rec_scores.min():.0f}, {rec_scores.max():.0f}]\")\n",
        "    \n",
        "    # Owner\n",
        "    owner_scores = comparison_df[owner_col].dropna()\n",
        "    if len(owner_scores) > 0:\n",
        "        print(f\"  Owner: Mean={owner_scores.mean():.2f}, Std={owner_scores.std():.2f}, \"\n",
        "              f\"Median={owner_scores.median():.0f}, Range=[{owner_scores.min():.0f}, {owner_scores.max():.0f}]\")\n",
        "    \n",
        "    # Statistical tests\n",
        "    if len(rec_scores) > 0 and len(llama_scores) > 0:\n",
        "        # Wilcoxon signed-rank test (paired)\n",
        "        mask = comparison_df[llama_col].notna() & comparison_df[rec_col].notna()\n",
        "        if mask.sum() > 5:\n",
        "            stat, p = wilcoxon(comparison_df.loc[mask, llama_col], \n",
        "                              comparison_df.loc[mask, rec_col])\n",
        "            sig = \"***\" if p < 0.001 else \"**\" if p < 0.01 else \"*\" if p < 0.05 else \"ns\"\n",
        "            print(f\"  Wilcoxon test (Llama vs Recruiter): p={p:.4f} {sig}\")\n",
        "    \n",
        "    if len(owner_scores) > 0 and len(llama_scores) > 0:\n",
        "        mask = comparison_df[llama_col].notna() & comparison_df[owner_col].notna()\n",
        "        if mask.sum() > 5:\n",
        "            stat, p = wilcoxon(comparison_df.loc[mask, llama_col], \n",
        "                              comparison_df.loc[mask, owner_col])\n",
        "            sig = \"***\" if p < 0.001 else \"**\" if p < 0.01 else \"*\" if p < 0.05 else \"ns\"\n",
        "            print(f\"  Wilcoxon test (Llama vs Owner): p={p:.4f} {sig}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Significance codes: *** p<0.001, ** p<0.01, * p<0.05, ns = not significant\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "distribution_stats"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize distributions\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, metric in enumerate(METRICS):\n",
        "    metric_col = f\"{metric}_score\"\n",
        "    llama_col = f\"{metric_col}_llama\"\n",
        "    rec_col = f\"{metric_col}_recruiter\"\n",
        "    owner_col = f\"{metric_col}_owner\"\n",
        "    \n",
        "    # Get data\n",
        "    llama_data = comparison_df[llama_col].dropna()\n",
        "    rec_data = comparison_df[rec_col].dropna()\n",
        "    owner_data = comparison_df[owner_col].dropna()\n",
        "    \n",
        "    # Plot\n",
        "    axes[idx].hist(llama_data, bins=np.arange(0.5, 11.5, 1), alpha=0.5, label='Llama', color='#e74c3c', edgecolor='black')\n",
        "    if len(rec_data) > 0:\n",
        "        axes[idx].hist(rec_data, bins=np.arange(0.5, 11.5, 1), alpha=0.5, label='Recruiter', color='#3498db', edgecolor='black')\n",
        "    if len(owner_data) > 0:\n",
        "        axes[idx].hist(owner_data, bins=np.arange(0.5, 11.5, 1), alpha=0.5, label='Owner', color='#2ecc71', edgecolor='black')\n",
        "    \n",
        "    axes[idx].set_xlabel('Score', fontsize=10)\n",
        "    axes[idx].set_ylabel('Frequency', fontsize=10)\n",
        "    axes[idx].set_title(metric.replace('_', ' ').title(), fontsize=11, fontweight='bold')\n",
        "    axes[idx].legend()\n",
        "    axes[idx].set_xlim(0, 11)\n",
        "    axes[idx].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Score Distribution by Rater and Metric', fontsize=14, fontweight='bold', y=1.00)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "distribution_plot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Bias Detection (Bland-Altman Plots)"
      ],
      "metadata": {
        "id": "bias_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bland-Altman plots to detect systematic bias\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, metric in enumerate(METRICS):\n",
        "    metric_col = f\"{metric}_score\"\n",
        "    llama_col = f\"{metric_col}_llama\"\n",
        "    rec_col = f\"{metric_col}_recruiter\"\n",
        "    \n",
        "    # Get paired data\n",
        "    mask = comparison_df[llama_col].notna() & comparison_df[rec_col].notna()\n",
        "    if mask.sum() < 2:\n",
        "        axes[idx].text(0.5, 0.5, 'Insufficient data', ha='center', va='center')\n",
        "        axes[idx].set_title(metric.replace('_', ' ').title(), fontsize=11, fontweight='bold')\n",
        "        continue\n",
        "    \n",
        "    llama_vals = comparison_df.loc[mask, llama_col].values\n",
        "    rec_vals = comparison_df.loc[mask, rec_col].values\n",
        "    \n",
        "    # Calculate mean and difference\n",
        "    mean_vals = (llama_vals + rec_vals) / 2\n",
        "    diff_vals = llama_vals - rec_vals\n",
        "    \n",
        "    # Calculate bias and limits of agreement\n",
        "    bias = diff_vals.mean()\n",
        "    std_diff = diff_vals.std()\n",
        "    upper_loa = bias + 1.96 * std_diff\n",
        "    lower_loa = bias - 1.96 * std_diff\n",
        "    \n",
        "    # Plot\n",
        "    axes[idx].scatter(mean_vals, diff_vals, alpha=0.6, color='#34495e')\n",
        "    axes[idx].axhline(bias, color='#e74c3c', linestyle='--', linewidth=2, label=f'Bias: {bias:.2f}')\n",
        "    axes[idx].axhline(upper_loa, color='#95a5a6', linestyle=':', linewidth=1.5, label=f'+1.96 SD: {upper_loa:.2f}')\n",
        "    axes[idx].axhline(lower_loa, color='#95a5a6', linestyle=':', linewidth=1.5, label=f'-1.96 SD: {lower_loa:.2f}')\n",
        "    axes[idx].axhline(0, color='black', linestyle='-', linewidth=0.5, alpha=0.3)\n",
        "    \n",
        "    axes[idx].set_xlabel('Mean Score', fontsize=10)\n",
        "    axes[idx].set_ylabel('Llama - Recruiter', fontsize=10)\n",
        "    axes[idx].set_title(metric.replace('_', ' ').title(), fontsize=11, fontweight='bold')\n",
        "    axes[idx].legend(fontsize=8, loc='best')\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Bland-Altman Plots: Llama vs Recruiter', fontsize=14, fontweight='bold', y=1.00)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nBland-Altman Interpretation:\")\n",
        "print(\"  • Points scattered randomly around bias line = no systematic trend\")\n",
        "print(\"  • Bias significantly different from 0 = systematic over/under-scoring\")\n",
        "print(\"  • Most points within ±1.96 SD = good agreement\")\n",
        "print(\"  • Funnel shape = heteroscedasticity (variance increases with score)\")"
      ],
      "metadata": {
        "id": "bland_altman"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Metric-by-Metric Deep Dive\n",
        "\n",
        "### 4.1 Confusion Matrices"
      ],
      "metadata": {
        "id": "deepdive_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Create confusion matrices for each metric\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, metric in enumerate(METRICS):\n",
        "    metric_col = f\"{metric}_score\"\n",
        "    llama_col = f\"{metric_col}_llama\"\n",
        "    rec_col = f\"{metric_col}_recruiter\"\n",
        "    \n",
        "    # Get paired data\n",
        "    mask = comparison_df[llama_col].notna() & comparison_df[rec_col].notna()\n",
        "    if mask.sum() < 2:\n",
        "        axes[idx].text(0.5, 0.5, 'Insufficient data', ha='center', va='center')\n",
        "        axes[idx].set_title(metric.replace('_', ' ').title(), fontsize=12, fontweight='bold')\n",
        "        continue\n",
        "    \n",
        "    llama_vals = comparison_df.loc[mask, llama_col].astype(int).values\n",
        "    rec_vals = comparison_df.loc[mask, rec_col].astype(int).values\n",
        "    \n",
        "    # Create confusion matrix\n",
        "    cm = confusion_matrix(rec_vals, llama_vals, labels=range(1, 11))\n",
        "    \n",
        "    # Normalize by row (true label)\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    cm_normalized = np.nan_to_num(cm_normalized)\n",
        "    \n",
        "    # Plot\n",
        "    im = axes[idx].imshow(cm_normalized, cmap='Blues', aspect='auto', vmin=0, vmax=1)\n",
        "    \n",
        "    # Add text annotations\n",
        "    for i in range(len(cm_normalized)):\n",
        "        for j in range(len(cm_normalized[0])):\n",
        "            if cm[i, j] > 0:\n",
        "                text = axes[idx].text(j, i, f'{cm[i, j]}\\n({cm_normalized[i, j]:.0%})',\n",
        "                                    ha=\"center\", va=\"center\", color=\"white\" if cm_normalized[i, j] > 0.5 else \"black\",\n",
        "                                    fontsize=7)\n",
        "    \n",
        "    axes[idx].set_xlabel('Llama Score', fontsize=10)\n",
        "    axes[idx].set_ylabel('Recruiter Score', fontsize=10)\n",
        "    axes[idx].set_title(metric.replace('_', ' ').title(), fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_xticks(range(10))\n",
        "    axes[idx].set_yticks(range(10))\n",
        "    axes[idx].set_xticklabels(range(1, 11))\n",
        "    axes[idx].set_yticklabels(range(1, 11))\n",
        "    \n",
        "    # Add colorbar\n",
        "    plt.colorbar(im, ax=axes[idx], fraction=0.046, pad=0.04)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Confusion Matrices: Recruiter (rows) vs Llama (columns)', fontsize=14, fontweight='bold', y=1.00)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nConfusion Matrix Interpretation:\")\n",
        "print(\"  • Diagonal = exact matches\")\n",
        "print(\"  • Cells near diagonal = close agreement\")\n",
        "print(\"  • Dark blue in cell (i,j) = high frequency of Recruiter giving score i when Llama gives j\")"
      ],
      "metadata": {
        "id": "confusion_matrix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Scatter Plots with Regression"
      ],
      "metadata": {
        "id": "scatter_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import linregress\n",
        "\n",
        "# Scatter plots with regression lines\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, metric in enumerate(METRICS):\n",
        "    metric_col = f\"{metric}_score\"\n",
        "    llama_col = f\"{metric_col}_llama\"\n",
        "    rec_col = f\"{metric_col}_recruiter\"\n",
        "    \n",
        "    # Get paired data\n",
        "    mask = comparison_df[llama_col].notna() & comparison_df[rec_col].notna()\n",
        "    if mask.sum() < 2:\n",
        "        axes[idx].text(0.5, 0.5, 'Insufficient data', ha='center', va='center')\n",
        "        axes[idx].set_title(metric.replace('_', ' ').title(), fontsize=11, fontweight='bold')\n",
        "        continue\n",
        "    \n",
        "    llama_vals = comparison_df.loc[mask, llama_col].values\n",
        "    rec_vals = comparison_df.loc[mask, rec_col].values\n",
        "    \n",
        "    # Regression\n",
        "    slope, intercept, r_value, p_value, std_err = linregress(llama_vals, rec_vals)\n",
        "    \n",
        "    # Plot\n",
        "    axes[idx].scatter(llama_vals, rec_vals, alpha=0.6, s=50, color='#3498db', edgecolor='black', linewidth=0.5)\n",
        "    \n",
        "    # Regression line\n",
        "    x_line = np.linspace(llama_vals.min(), llama_vals.max(), 100)\n",
        "    y_line = slope * x_line + intercept\n",
        "    axes[idx].plot(x_line, y_line, 'r-', linewidth=2, label=f'y = {slope:.2f}x + {intercept:.2f}')\n",
        "    \n",
        "    # Identity line (perfect agreement)\n",
        "    axes[idx].plot([1, 10], [1, 10], 'k--', linewidth=1, alpha=0.5, label='Perfect agreement')\n",
        "    \n",
        "    axes[idx].set_xlabel('Llama Score', fontsize=10)\n",
        "    axes[idx].set_ylabel('Recruiter Score', fontsize=10)\n",
        "    axes[idx].set_title(f\"{metric.replace('_', ' ').title()}\\nr = {r_value:.3f}, p = {p_value:.4f}\", \n",
        "                       fontsize=11, fontweight='bold')\n",
        "    axes[idx].legend(fontsize=8, loc='best')\n",
        "    axes[idx].set_xlim(0, 11)\n",
        "    axes[idx].set_ylim(0, 11)\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "    axes[idx].set_aspect('equal')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Llama vs Recruiter: Scatter Plots with Regression', fontsize=14, fontweight='bold', y=1.00)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "scatter_regression"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Disagreement Analysis\n",
        "\n",
        "### 5.1 High-Disagreement Cases"
      ],
      "metadata": {
        "id": "disagreement_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate disagreement scores for each interview\n",
        "disagreement_data = []\n",
        "\n",
        "for _, row in comparison_df.iterrows():\n",
        "    interview_id = row['interview_id']\n",
        "    \n",
        "    # Calculate MAE across all metrics\n",
        "    mae_recruiter = 0\n",
        "    mae_owner = 0\n",
        "    count_rec = 0\n",
        "    count_owner = 0\n",
        "    \n",
        "    for metric in METRICS:\n",
        "        metric_col = f\"{metric}_score\"\n",
        "        llama_col = f\"{metric_col}_llama\"\n",
        "        rec_col = f\"{metric_col}_recruiter\"\n",
        "        owner_col = f\"{metric_col}_owner\"\n",
        "        \n",
        "        if pd.notna(row[llama_col]) and pd.notna(row[rec_col]):\n",
        "            mae_recruiter += abs(row[llama_col] - row[rec_col])\n",
        "            count_rec += 1\n",
        "        \n",
        "        if pd.notna(row[llama_col]) and pd.notna(row[owner_col]):\n",
        "            mae_owner += abs(row[llama_col] - row[owner_col])\n",
        "            count_owner += 1\n",
        "    \n",
        "    disagreement_data.append({\n",
        "        'interview_id': interview_id,\n",
        "        'mae_recruiter': mae_recruiter / count_rec if count_rec > 0 else np.nan,\n",
        "        'mae_owner': mae_owner / count_owner if count_owner > 0 else np.nan\n",
        "    })\n",
        "\n",
        "disagreement_df = pd.DataFrame(disagreement_data)\n",
        "\n",
        "# Find high disagreement cases\n",
        "print(\"=\" * 80)\n",
        "print(\"HIGH DISAGREEMENT CASES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nTop 10 interviews with highest disagreement (Llama vs Recruiter):\")\n",
        "top_disagreement = disagreement_df.nlargest(10, 'mae_recruiter')\n",
        "display(top_disagreement)\n",
        "\n",
        "# Show score details for top disagreement case\n",
        "if len(top_disagreement) > 0:\n",
        "    worst_case_id = top_disagreement.iloc[0]['interview_id']\n",
        "    print(f\"\\n\\nDetailed scores for highest disagreement case: {worst_case_id}\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    case_row = comparison_df[comparison_df['interview_id'] == worst_case_id].iloc[0]\n",
        "    \n",
        "    print(f\"{'Metric':<25} {'Llama':<10} {'Recruiter':<12} {'Difference':<12}\")\n",
        "    print(\"-\" * 80)\n",
        "    for metric in METRICS:\n",
        "        metric_col = f\"{metric}_score\"\n",
        "        llama_val = case_row[f\"{metric_col}_llama\"]\n",
        "        rec_val = case_row[f\"{metric_col}_recruiter\"]\n",
        "        if pd.notna(llama_val) and pd.notna(rec_val):\n",
        "            diff = llama_val - rec_val\n",
        "            print(f\"{metric.replace('_', ' ').title():<25} {llama_val:<10.0f} {rec_val:<12.0f} {diff:+.0f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)"
      ],
      "metadata": {
        "id": "high_disagreement"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Agreement Heatmap"
      ],
      "metadata": {
        "id": "heatmap_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create agreement heatmap showing MAE for each metric and rater pair\n",
        "mae_data = []\n",
        "\n",
        "for metric in METRICS:\n",
        "    metric_col = f\"{metric}_score\"\n",
        "    llama_col = f\"{metric_col}_llama\"\n",
        "    rec_col = f\"{metric_col}_recruiter\"\n",
        "    owner_col = f\"{metric_col}_owner\"\n",
        "    \n",
        "    # Llama vs Recruiter\n",
        "    mask_lr = comparison_df[llama_col].notna() & comparison_df[rec_col].notna()\n",
        "    mae_lr = (comparison_df.loc[mask_lr, llama_col] - comparison_df.loc[mask_lr, rec_col]).abs().mean()\n",
        "    \n",
        "    # Llama vs Owner\n",
        "    mask_lo = comparison_df[llama_col].notna() & comparison_df[owner_col].notna()\n",
        "    mae_lo = (comparison_df.loc[mask_lo, llama_col] - comparison_df.loc[mask_lo, owner_col]).abs().mean()\n",
        "    \n",
        "    # Recruiter vs Owner\n",
        "    mask_ro = comparison_df[rec_col].notna() & comparison_df[owner_col].notna()\n",
        "    mae_ro = (comparison_df.loc[mask_ro, rec_col] - comparison_df.loc[mask_ro, owner_col]).abs().mean()\n",
        "    \n",
        "    mae_data.append({\n",
        "        'metric': metric,\n",
        "        'Llama vs\\nRecruiter': mae_lr,\n",
        "        'Llama vs\\nOwner': mae_lo,\n",
        "        'Recruiter vs\\nOwner': mae_ro\n",
        "    })\n",
        "\n",
        "mae_matrix_df = pd.DataFrame(mae_data).set_index('metric')\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(mae_matrix_df.T, annot=True, fmt='.2f', cmap='RdYlGn_r', \n",
        "            vmin=0, vmax=2, cbar_kws={'label': 'Mean Absolute Error'},\n",
        "            xticklabels=[m.replace('_', ' ').title() for m in METRICS])\n",
        "plt.title('Agreement Heatmap: Mean Absolute Error by Metric', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.xlabel('')\n",
        "plt.ylabel('')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"  Green (MAE < 0.5): Excellent agreement\")\n",
        "print(\"  Yellow (MAE 0.5-1.0): Good agreement\")\n",
        "print(\"  Orange (MAE 1.0-1.5): Moderate agreement\")\n",
        "print(\"  Red (MAE > 1.5): Poor agreement\")"
      ],
      "metadata": {
        "id": "agreement_heatmap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Systematic Bias Analysis"
      ],
      "metadata": {
        "id": "bias_analysis_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate mean bias (Human - Llama) for each metric\n",
        "bias_data = []\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"SYSTEMATIC BIAS ANALYSIS (Human - Llama)\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nPositive values: Humans score higher than Llama\")\n",
        "print(\"Negative values: Llama scores higher than Humans\\n\")\n",
        "\n",
        "for metric in METRICS:\n",
        "    metric_col = f\"{metric}_score\"\n",
        "    llama_col = f\"{metric_col}_llama\"\n",
        "    rec_col = f\"{metric_col}_recruiter\"\n",
        "    owner_col = f\"{metric_col}_owner\"\n",
        "    \n",
        "    # Recruiter bias\n",
        "    mask_lr = comparison_df[llama_col].notna() & comparison_df[rec_col].notna()\n",
        "    bias_rec = (comparison_df.loc[mask_lr, rec_col] - comparison_df.loc[mask_lr, llama_col]).mean()\n",
        "    \n",
        "    # Owner bias\n",
        "    mask_lo = comparison_df[llama_col].notna() & comparison_df[owner_col].notna()\n",
        "    bias_owner = (comparison_df.loc[mask_lo, owner_col] - comparison_df.loc[mask_lo, llama_col]).mean()\n",
        "    \n",
        "    print(f\"{metric.replace('_', ' ').title():<25} Recruiter: {bias_rec:+.2f}   Owner: {bias_owner:+.2f}\")\n",
        "    \n",
        "    bias_data.append({\n",
        "        'metric': metric,\n",
        "        'recruiter_bias': bias_rec,\n",
        "        'owner_bias': bias_owner\n",
        "    })\n",
        "\n",
        "bias_df = pd.DataFrame(bias_data)\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "x = np.arange(len(METRICS))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax.bar(x - width/2, bias_df['recruiter_bias'], width, label='Recruiter - Llama',\n",
        "               color=['#e74c3c' if b < 0 else '#3498db' for b in bias_df['recruiter_bias']])\n",
        "bars2 = ax.bar(x + width/2, bias_df['owner_bias'], width, label='Owner - Llama',\n",
        "               color=['#e74c3c' if b < 0 else '#2ecc71' for b in bias_df['owner_bias']])\n",
        "\n",
        "ax.axhline(0, color='black', linewidth=1, linestyle='-')\n",
        "ax.set_ylabel('Mean Bias (Human - Llama)', fontsize=12)\n",
        "ax.set_title('Systematic Scoring Bias by Metric', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels([m.replace('_', '\\n').title() for m in METRICS])\n",
        "ax.legend()\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\\nKey Findings:\")\n",
        "avg_rec_bias = bias_df['recruiter_bias'].mean()\n",
        "avg_owner_bias = bias_df['owner_bias'].mean()\n",
        "print(f\"  Average Recruiter bias: {avg_rec_bias:+.2f}\")\n",
        "print(f\"  Average Owner bias: {avg_owner_bias:+.2f}\")\n",
        "\n",
        "if avg_rec_bias > 0.5:\n",
        "    print(\"  ⚠️ Recruiter tends to score HIGHER than Llama\")\n",
        "elif avg_rec_bias < -0.5:\n",
        "    print(\"  ⚠️ Recruiter tends to score LOWER than Llama\")\n",
        "else:\n",
        "    print(\"  ✓ Recruiter scores are well-calibrated with Llama\")\n",
        "\n",
        "if avg_owner_bias > 0.5:\n",
        "    print(\"  ⚠️ Owner tends to score HIGHER than Llama\")\n",
        "elif avg_owner_bias < -0.5:\n",
        "    print(\"  ⚠️ Owner tends to score LOWER than Llama\")\n",
        "else:\n",
        "    print(\"  ✓ Owner scores are well-calibrated with Llama\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "bias_analysis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Summary & Recommendations"
      ],
      "metadata": {
        "id": "summary_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"COMPREHENSIVE ANALYSIS SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Overall statistics\n",
        "print(\"\\n1. INTER-RATER RELIABILITY\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"Average ICC (Llama vs Recruiter): {reliability_df['icc_llama_recruiter'].mean():.3f}\")\n",
        "print(f\"Average ICC (Llama vs Owner): {reliability_df['icc_llama_owner'].mean():.3f}\")\n",
        "print(f\"Average ICC (Recruiter vs Owner): {reliability_df['icc_recruiter_owner'].mean():.3f}\")\n",
        "\n",
        "# Best and worst metrics\n",
        "best_metric = reliability_df.loc[reliability_df['icc_llama_recruiter'].idxmax(), 'metric']\n",
        "worst_metric = reliability_df.loc[reliability_df['icc_llama_recruiter'].idxmin(), 'metric']\n",
        "print(f\"\\nBest agreement metric: {best_metric.replace('_', ' ').title()}\")\n",
        "print(f\"Weakest agreement metric: {worst_metric.replace('_', ' ').title()}\")\n",
        "\n",
        "print(\"\\n2. AGREEMENT RATES\")\n",
        "print(\"-\" * 80)\n",
        "avg_exact = agreement_df['exact_llama_recruiter'].mean()\n",
        "avg_within1 = agreement_df['within1_llama_recruiter'].mean()\n",
        "avg_within2 = agreement_df['within2_llama_recruiter'].mean()\n",
        "print(f\"Average exact match rate: {avg_exact:.1f}%\")\n",
        "print(f\"Average within ±1 agreement: {avg_within1:.1f}%\")\n",
        "print(f\"Average within ±2 agreement: {avg_within2:.1f}%\")\n",
        "\n",
        "print(\"\\n3. SYSTEMATIC BIAS\")\n",
        "print(\"-\" * 80)\n",
        "overall_rec_bias = bias_df['recruiter_bias'].mean()\n",
        "overall_owner_bias = bias_df['owner_bias'].mean()\n",
        "print(f\"Overall Recruiter bias: {overall_rec_bias:+.2f} points\")\n",
        "print(f\"Overall Owner bias: {overall_owner_bias:+.2f} points\")\n",
        "\n",
        "print(\"\\n4. KEY RECOMMENDATIONS\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Generate recommendations based on analysis\n",
        "recommendations = []\n",
        "\n",
        "if avg_within1 > 75:\n",
        "    recommendations.append(\"✓ Strong overall agreement - Llama can be used with confidence\")\n",
        "elif avg_within1 > 60:\n",
        "    recommendations.append(\"⚠️ Moderate agreement - Consider human review for edge cases\")\n",
        "else:\n",
        "    recommendations.append(\"⚠️ Weak agreement - Significant calibration needed\")\n",
        "\n",
        "if abs(overall_rec_bias) > 0.5:\n",
        "    if overall_rec_bias > 0:\n",
        "        recommendations.append(f\"⚠️ Recruiter systematically scores {overall_rec_bias:.2f} points higher - Consider calibration\")\n",
        "    else:\n",
        "        recommendations.append(f\"⚠️ Llama systematically scores {-overall_rec_bias:.2f} points higher - Review scoring criteria\")\n",
        "\n",
        "if worst_metric:\n",
        "    recommendations.append(f\"⚠️ Focus calibration efforts on '{worst_metric.replace('_', ' ').title()}' - weakest agreement\")\n",
        "\n",
        "# Check for high-variance metrics\n",
        "high_variance_metrics = []\n",
        "for metric in METRICS:\n",
        "    llama_col = f\"{metric}_score_llama\"\n",
        "    if comparison_df[llama_col].std() > 1.5:\n",
        "        high_variance_metrics.append(metric)\n",
        "\n",
        "if high_variance_metrics:\n",
        "    recommendations.append(f\"⚠️ High variance in: {', '.join([m.replace('_', ' ').title() for m in high_variance_metrics])}\")\n",
        "\n",
        "for i, rec in enumerate(recommendations, 1):\n",
        "    print(f\"{i}. {rec}\")\n",
        "\n",
        "print(\"\\n5. NEXT STEPS\")\n",
        "print(\"-\" * 80)\n",
        "print(\"1. Review high-disagreement cases to identify patterns\")\n",
        "print(\"2. Calibrate human raters if systematic bias is detected\")\n",
        "print(\"3. Refine Llama prompts for metrics with weak agreement\")\n",
        "print(\"4. Consider ensemble approach (Llama + human) for critical decisions\")\n",
        "print(\"5. Monitor agreement rates over time as system evolves\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"END OF ANALYSIS\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "final_summary"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export Results"
      ],
      "metadata": {
        "id": "export_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save all analysis results\n",
        "from datetime import datetime\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "output_dir = \"/content/drive/MyDrive/mvp/analysis_results\"\n",
        "\n",
        "import os\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Save dataframes\n",
        "reliability_df.to_csv(f\"{output_dir}/reliability_metrics_{timestamp}.csv\", index=False)\n",
        "agreement_df.to_csv(f\"{output_dir}/agreement_rates_{timestamp}.csv\", index=False)\n",
        "bias_df.to_csv(f\"{output_dir}/bias_analysis_{timestamp}.csv\", index=False)\n",
        "disagreement_df.to_csv(f\"{output_dir}/disagreement_scores_{timestamp}.csv\", index=False)\n",
        "comparison_df.to_csv(f\"{output_dir}/full_comparison_{timestamp}.csv\", index=False)\n",
        "\n",
        "print(f\"✓ Results saved to: {output_dir}\")\n",
        "print(f\"  - reliability_metrics_{timestamp}.csv\")\n",
        "print(f\"  - agreement_rates_{timestamp}.csv\")\n",
        "print(f\"  - bias_analysis_{timestamp}.csv\")\n",
        "print(f\"  - disagreement_scores_{timestamp}.csv\")\n",
        "print(f\"  - full_comparison_{timestamp}.csv\")"
      ],
      "metadata": {
        "id": "export"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
