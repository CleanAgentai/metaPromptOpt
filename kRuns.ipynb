{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Self-Consistency Evaluation with Cost Tracking\n",
        "This notebook performs K self-consistency runs on multiple LLMs (Llama & GPT) with comprehensive cost tracking.\n",
        "\n",
        "**Dataset**: 250 candidates (50 per role × 5 roles)\n",
        "\n",
        "**Models**: \n",
        "- Llama 3.1 8B (via Hugging Face)\n",
        "- GPT-4o\n",
        "- GPT-3.5-turbo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install openai pandas numpy pyyaml tiktoken huggingface_hub -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Set API keys\n",
        "if \"OPENAI_API_KEY\" not in os.environ or not os.environ[\"OPENAI_API_KEY\"]:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI API key: \")\n",
        "\n",
        "# For Llama via Hugging Face\n",
        "# if \"HF_TOKEN\" not in os.environ or not os.environ[\"HF_TOKEN\"]:\n",
        "#     os.environ[\"HF_TOKEN\"] = getpass(\"Paste your Hugging Face token (or press Enter to skip): \") or \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "import tiktoken\n",
        "from typing import Dict, Any, List, Tuple, Optional\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "from openai import OpenAI\n",
        "from huggingface_hub import InferenceClient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration loaded for model: gpt-4o\n",
            "Dataset path: /Users/shreya_sudan/Desktop/ServiceAgent/CleanAgent/metaPromptOpt/data/syntheticDataset/synthetic_interview_dataset.json\n",
            "K samples per interview: 7\n"
          ]
        }
      ],
      "source": [
        "# Dataset configuration\n",
        "DATASET_PATH = \"/Users/shreya_sudan/Desktop/ServiceAgent/CleanAgent/metaPromptOpt/data/syntheticDataset/synthetic_interview_dataset.json\"\n",
        "K_SAMPLES = 7  # Number of self-consistency samples per interview\n",
        "\n",
        "# Models to evaluate\n",
        "MODELS_CONFIG = {\n",
        "    # OpenAI models\n",
        "    \"gpt-4o\": {\n",
        "        \"provider\": \"openai\",\n",
        "        \"input_cost_per_1m\": 2.50,  # $2.50 per 1M input tokens\n",
        "        \"output_cost_per_1m\": 10.00,  # $10.00 per 1M output tokens\n",
        "        \"temperature\": 0.0,\n",
        "        \"max_tokens\": 512\n",
        "    },\n",
        "    \"gpt-3.5-turbo\": {\n",
        "        \"provider\": \"openai\",\n",
        "        \"input_cost_per_1m\": 0.50,  # $0.50 per 1M input tokens\n",
        "        \"output_cost_per_1m\": 1.50,  # $1.50 per 1M output tokens\n",
        "        \"temperature\": 0.0,\n",
        "        \"max_tokens\": 512\n",
        "    },\n",
        "    # Llama via Hugging Face Inference API\n",
        "    \"meta-llama/Llama-3.1-8B-Instruct:novita\": {\n",
        "        \"provider\": \"hf_openai\",\n",
        "        \"input_cost_per_1m\": 0.00,  # Free with HF Pro\n",
        "        \"output_cost_per_1m\": 0.00,\n",
        "        \"temperature\": 0.0,\n",
        "        \"max_tokens\": 512\n",
        "    },\n",
        "}\n",
        "\n",
        "# Scoring weights and metrics\n",
        "WEIGHTS = {\n",
        "    \"cognitive_ability\": 0.35,\n",
        "    \"experience\": 0.35,\n",
        "    \"problem_solving\": 0.15,\n",
        "    \"reliability\": 0.05,\n",
        "    \"professionalism\": 0.05,\n",
        "    \"communication\": 0.05\n",
        "}\n",
        "\n",
        "METRICS = list(WEIGHTS.keys())\n",
        "METRIC_ABBREV = {\n",
        "    \"cognitive_ability\": \"ca\",\n",
        "    \"experience\": \"exp\",\n",
        "    \"problem_solving\": \"ps\",\n",
        "    \"reliability\": \"rel\",\n",
        "    \"professionalism\": \"prof\",\n",
        "    \"communication\": \"comm\"\n",
        "}\n",
        "\n",
        "# Model to run (change this to run different models)\n",
        "CURRENT_MODEL = \"gpt-4o\"  # Options: gpt-4o, gpt-3.5-turbo, meta-llama/Llama-3.1-8B-Instruct\n",
        "\n",
        "print(f\"Configuration loaded for model: {CURRENT_MODEL}\")\n",
        "print(f\"Dataset path: {DATASET_PATH}\")\n",
        "print(f\"K samples per interview: {K_SAMPLES}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Helper functions loaded\n"
          ]
        }
      ],
      "source": [
        "def clamp_int(x, lo=1, hi=10):\n",
        "    \"\"\"Clamp value to integer range.\"\"\"\n",
        "    try:\n",
        "        xi = int(round(float(x)))\n",
        "    except Exception:\n",
        "        xi = 5\n",
        "    return max(lo, min(hi, xi))\n",
        "\n",
        "def compute_overall_weighted(scores: Dict[str, float]) -> float:\n",
        "    \"\"\"Compute weighted overall score.\"\"\"\n",
        "    total = sum(WEIGHTS.get(metric, 0) * scores.get(metric, 5) for metric in METRICS)\n",
        "    return round(total, 2)\n",
        "\n",
        "def iqr_confidence(vals: List[float]) -> str:\n",
        "    \"\"\"Calculate confidence based on IQR.\"\"\"\n",
        "    if len(vals) < 2:\n",
        "        return \"low\"\n",
        "    q1, q3 = np.percentile(vals, [25, 75])\n",
        "    iqr = q3 - q1\n",
        "    if iqr <= 1:\n",
        "        return \"high\"\n",
        "    elif iqr <= 2:\n",
        "        return \"medium\"\n",
        "    else:\n",
        "        return \"low\"\n",
        "\n",
        "def estimate_tokens(text: str, model: str = \"gpt-3.5-turbo\") -> int:\n",
        "    \"\"\"Estimate token count for text.\"\"\"\n",
        "    try:\n",
        "        if \"gpt\" in model:\n",
        "            encoding = tiktoken.encoding_for_model(model)\n",
        "        else:\n",
        "            # Fallback for non-OpenAI models\n",
        "            encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "        return len(encoding.encode(text))\n",
        "    except Exception:\n",
        "        # Rough estimate: ~4 chars per token\n",
        "        return len(text) // 4\n",
        "\n",
        "def calculate_cost(input_tokens: int, output_tokens: int, model_config: dict) -> float:\n",
        "    \"\"\"Calculate cost based on token usage.\"\"\"\n",
        "    input_cost = (input_tokens / 1_000_000) * model_config[\"input_cost_per_1m\"]\n",
        "    output_cost = (output_tokens / 1_000_000) * model_config[\"output_cost_per_1m\"]\n",
        "    return input_cost + output_cost\n",
        "\n",
        "print(\"✓ Helper functions loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Client Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ API clients initialized\n"
          ]
        }
      ],
      "source": [
        "# Initialize API clients\n",
        "openai_client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "# Hugging Face client\n",
        "if os.environ.get(\"HF_TOKEN\"):\n",
        "    hf_client = OpenAI(\n",
        "        base_url=\"https://router.huggingface.co/v1\",\n",
        "        api_key=os.environ[\"HF_TOKEN\"]\n",
        "    )\n",
        "else:\n",
        "    hf_client = None\n",
        "\n",
        "\n",
        "print(\"✓ API clients initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prompt Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Prompt builders loaded\n"
          ]
        }
      ],
      "source": [
        "def build_scoring_prompt(full_transcript: str, role: str) -> str:\n",
        "    \"\"\"Build prompt for scoring candidate interview.\"\"\"\n",
        "    prompt = f\"\"\"You are evaluating a candidate interview for the role: {role}\n",
        "\n",
        "Analyze the candidate's responses using these six metrics (each scored 1-10):\n",
        "\n",
        "1. **Cognitive Ability (35%)**: Structured thinking, planning, logic, analytical reasoning\n",
        "2. **Experience (35%)**: Relevant work history (last 10 years), demonstrated skills, accomplishments\n",
        "3. **Problem Solving (15%)**: Resourcefulness, creative solutions, handling constraints\n",
        "4. **Reliability (5%)**: Punctuality, follow-through, dependability signals\n",
        "5. **Professionalism (5%)**: Respect for clients/rules, composure under stress\n",
        "6. **Communication (5%)**: Clarity and tone (ignore filler words like um, uh, like)\n",
        "\n",
        "CRITICAL INSTRUCTIONS:\n",
        "- Return ONLY a valid JSON object\n",
        "- Use these exact keys: cognitive_ability, experience, problem_solving, reliability, professionalism, communication\n",
        "- Each value must be an integer from 1 to 10\n",
        "- Do not include any explanations, just the JSON\n",
        "\n",
        "Interview Transcript:\n",
        "--- START TRANSCRIPT ---\n",
        "{full_transcript}\n",
        "--- END TRANSCRIPT ---\n",
        "\n",
        "Return your scores in this format:\n",
        "{{\"cognitive_ability\":7,\"experience\":6,\"problem_solving\":7,\"reliability\":6,\"professionalism\":7,\"communication\":8}}\"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "def build_rewrite_prompt(full_transcript: str, role: str, scores: Dict[str, int]) -> str:\n",
        "    \"\"\"Build prompt for generating justifications with locked scores.\"\"\"\n",
        "    prompt = f\"\"\"You are writing justifications for candidate evaluation scores.\n",
        "\n",
        "Role: {role}\n",
        "\n",
        "CRITICAL: Use these FIXED scores. DO NOT change them:\n",
        "- Cognitive Ability: {scores['cognitive_ability']}\n",
        "- Experience: {scores['experience']}\n",
        "- Problem Solving: {scores['problem_solving']}\n",
        "- Reliability: {scores['reliability']}\n",
        "- Professionalism: {scores['professionalism']}\n",
        "- Communication: {scores['communication']}\n",
        "\n",
        "Generate:\n",
        "1. A justification for each score (2-3 sentences)\n",
        "2. 3-4 general strengths (bullet points)\n",
        "3. 3-4 general weaknesses (bullet points)\n",
        "4. Overall summary (2-3 sentences)\n",
        "\n",
        "Return ONLY this JSON structure:\n",
        "{{\n",
        "  \"cognitive_ability_score\": {scores['cognitive_ability']},\n",
        "  \"cognitive_ability_justification\": \"...\",\n",
        "  \"experience_score\": {scores['experience']},\n",
        "  \"experience_justification\": \"...\",\n",
        "  \"problem_solving_score\": {scores['problem_solving']},\n",
        "  \"problem_solving_justification\": \"...\",\n",
        "  \"reliability_score\": {scores['reliability']},\n",
        "  \"reliability_justification\": \"...\",\n",
        "  \"professionalism_score\": {scores['professionalism']},\n",
        "  \"professionalism_justification\": \"...\",\n",
        "  \"communication_score\": {scores['communication']},\n",
        "  \"communication_justification\": \"...\",\n",
        "  \"general_strengths\": \"- ...\\\\n- ...\\\\n- ...\",\n",
        "  \"general_weaknesses\": \"- ...\\\\n- ...\\\\n- ...\",\n",
        "  \"general_summary\": \"...\"\n",
        "}}\n",
        "\n",
        "Interview Transcript:\n",
        "--- START TRANSCRIPT ---\n",
        "{full_transcript}\n",
        "--- END TRANSCRIPT ---\"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "print(\"✓ Prompt builders loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Call Functions with Cost Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ API call functions with cost tracking loaded\n"
          ]
        }
      ],
      "source": [
        "def call_llm_with_tracking(\n",
        "    prompt: str,\n",
        "    model: str,\n",
        "    temperature: float = 0.0,\n",
        "    max_tokens: int = 512,\n",
        "    json_mode: bool = True\n",
        ") -> Tuple[str, Dict[str, Any]]:\n",
        "    \"\"\"Call LLM and track tokens/cost.\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (response_text, metadata_dict)\n",
        "        metadata includes: input_tokens, output_tokens, cost, latency_ms\n",
        "    \"\"\"\n",
        "    model_config = MODELS_CONFIG[model]\n",
        "    provider = model_config[\"provider\"]\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        # Handle different providers\n",
        "        if provider == \"openai\":\n",
        "            # OpenAI API call\n",
        "            kwargs = {\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"temperature\": temperature,\n",
        "                \"max_tokens\": max_tokens,\n",
        "            }\n",
        "            \n",
        "            if json_mode:\n",
        "                kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
        "            \n",
        "            response = openai_client.chat.completions.create(**kwargs)\n",
        "            response_text = response.choices[0].message.content\n",
        "            \n",
        "            # Get token usage\n",
        "            if hasattr(response, 'usage') and response.usage:\n",
        "                input_tokens = response.usage.prompt_tokens\n",
        "                output_tokens = response.usage.completion_tokens\n",
        "            else:\n",
        "                input_tokens = estimate_tokens(prompt, model)\n",
        "                output_tokens = estimate_tokens(response_text, model)\n",
        "        \n",
        "        elif provider == \"huggingface\":\n",
        "            # Hugging Face Inference API call\n",
        "            if not hf_client:\n",
        "                raise ValueError(\"Hugging Face client not initialized. Set HF_TOKEN.\")\n",
        "            \n",
        "            # Add JSON instruction to prompt if needed\n",
        "            if json_mode and 'Return ONLY a valid JSON' not in prompt:\n",
        "                enhanced_prompt = prompt + \"\\n\\nIMPORTANT: Return ONLY valid JSON, no explanations.\"\n",
        "            else:\n",
        "                enhanced_prompt = prompt\n",
        "            \n",
        "            response = hf_client.text_generation(\n",
        "                model=model,\n",
        "                prompt=enhanced_prompt,\n",
        "                max_new_tokens=max_tokens,\n",
        "                temperature=temperature if temperature > 0 else 0.01,  # HF doesn't like exactly 0\n",
        "                return_full_text=False\n",
        "            )\n",
        "            \n",
        "            response_text = response\n",
        "            \n",
        "            # Estimate tokens for HF (no token counts provided)\n",
        "            input_tokens = estimate_tokens(prompt, model)\n",
        "            output_tokens = estimate_tokens(response_text, model)\n",
        "        \n",
        "        else:\n",
        "            raise ValueError(f\"Unknown provider: {provider}\")\n",
        "        \n",
        "        latency_ms = (time.time() - start_time) * 1000\n",
        "        \n",
        "        # Calculate cost\n",
        "        cost = calculate_cost(input_tokens, output_tokens, model_config)\n",
        "        \n",
        "        metadata = {\n",
        "            \"input_tokens\": input_tokens,\n",
        "            \"output_tokens\": output_tokens,\n",
        "            \"total_tokens\": input_tokens + output_tokens,\n",
        "            \"cost\": cost,\n",
        "            \"latency_ms\": latency_ms,\n",
        "            \"model\": model,\n",
        "            \"provider\": provider\n",
        "        }\n",
        "        \n",
        "        return response_text, metadata\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error calling {model}: {e}\")\n",
        "        raise\n",
        "\n",
        "def call_scoring(\n",
        "    full_transcript: str,\n",
        "    role: str,\n",
        "    model: str,\n",
        "    temperature: float = 0.0\n",
        ") -> Tuple[Dict[str, int], str, Dict[str, Any]]:\n",
        "    \"\"\"Call scoring API and parse response.\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (scores_dict, raw_text, metadata)\n",
        "    \"\"\"\n",
        "    prompt = build_scoring_prompt(full_transcript, role)\n",
        "    response_text, metadata = call_llm_with_tracking(\n",
        "        prompt=prompt,\n",
        "        model=model,\n",
        "        temperature=temperature,\n",
        "        max_tokens=512,\n",
        "        json_mode=True\n",
        "    )\n",
        "    \n",
        "    # Parse JSON response\n",
        "    try:\n",
        "        # Try to extract JSON from response\n",
        "        response_clean = response_text.strip()\n",
        "        \n",
        "        # Handle markdown code blocks\n",
        "        if response_clean.startswith('```'):\n",
        "            # Extract JSON from code block\n",
        "            lines = response_clean.split('\\n')\n",
        "            json_lines = []\n",
        "            in_json = False\n",
        "            for line in lines:\n",
        "                if line.strip().startswith('```'):\n",
        "                    in_json = not in_json\n",
        "                    continue\n",
        "                if in_json or (line.strip().startswith('{') or json_lines):\n",
        "                    json_lines.append(line)\n",
        "                    if line.strip().endswith('}') and json_lines[0].strip().startswith('{'):\n",
        "                        break\n",
        "            response_clean = '\\n'.join(json_lines)\n",
        "        \n",
        "        # Find JSON object\n",
        "        if '{' in response_clean:\n",
        "            start_idx = response_clean.index('{')\n",
        "            end_idx = response_clean.rindex('}') + 1\n",
        "            response_clean = response_clean[start_idx:end_idx]\n",
        "        \n",
        "        scores = json.loads(response_clean)\n",
        "        \n",
        "        # Clamp scores to valid range\n",
        "        for metric in METRICS:\n",
        "            if metric in scores:\n",
        "                scores[metric] = clamp_int(scores[metric], 1, 10)\n",
        "        return scores, response_text, metadata\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing scoring response: {e}\")\n",
        "        print(f\"Response text: {response_text[:200]}...\")\n",
        "        # Return default scores\n",
        "        default_scores = {metric: 5 for metric in METRICS}\n",
        "        return default_scores, response_text, metadata\n",
        "\n",
        "def call_rewrite(\n",
        "    full_transcript: str,\n",
        "    role: str,\n",
        "    scores: Dict[str, int],\n",
        "    model: str\n",
        ") -> Tuple[Dict[str, Any], str, Dict[str, Any]]:\n",
        "    \"\"\"Call rewrite API for justifications.\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (rewrite_dict, raw_text, metadata)\n",
        "    \"\"\"\n",
        "    prompt = build_rewrite_prompt(full_transcript, role, scores)\n",
        "    response_text, metadata = call_llm_with_tracking(\n",
        "        prompt=prompt,\n",
        "        model=model,\n",
        "        temperature=0.0,\n",
        "        max_tokens=1200,\n",
        "        json_mode=True\n",
        "    )\n",
        "    \n",
        "    # Parse JSON response\n",
        "    try:\n",
        "        # Clean response similar to call_scoring\n",
        "        response_clean = response_text.strip()\n",
        "        \n",
        "        if response_clean.startswith('```'):\n",
        "            lines = response_clean.split('\\n')\n",
        "            json_lines = []\n",
        "            in_json = False\n",
        "            for line in lines:\n",
        "                if line.strip().startswith('```'):\n",
        "                    in_json = not in_json\n",
        "                    continue\n",
        "                if in_json or (line.strip().startswith('{') or json_lines):\n",
        "                    json_lines.append(line)\n",
        "                    if line.strip().endswith('}') and json_lines[0].strip().startswith('{'):\n",
        "                        break\n",
        "            response_clean = '\\n'.join(json_lines)\n",
        "        \n",
        "        if '{' in response_clean:\n",
        "            start_idx = response_clean.index('{')\n",
        "            end_idx = response_clean.rindex('}') + 1\n",
        "            response_clean = response_clean[start_idx:end_idx]\n",
        "        \n",
        "        rewrite = json.loads(response_clean)\n",
        "        return rewrite, response_text, metadata\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing rewrite response: {e}\")\n",
        "        print(f\"Response text: {response_text[:200]}...\")\n",
        "        return {}, response_text, metadata\n",
        "\n",
        "print(\"✓ API call functions with cost tracking loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 250 interviews\n",
            "\n",
            "Sample interview structure:\n",
            "  Keys: ['interview_id', 'role', 'overall_score', 'quality', 'metric_scores', 'qa_pairs', 'full_transcript']\n",
            "  Role: Customer Service Representative\n",
            "  Interview ID: customer_service_representative_001\n",
            "\n",
            "Interviews by role:\n",
            "  Customer Service Representative: 50\n",
            "  Field Technician: 50\n",
            "  General Manager (Franchise): 50\n",
            "  Home Service Technician: 50\n",
            "  Sales Representative: 50\n"
          ]
        }
      ],
      "source": [
        "# Load interview dataset\n",
        "with open(DATASET_PATH, 'r') as f:\n",
        "    interviews_data = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(interviews_data)} interviews\")\n",
        "print(f\"\\nSample interview structure:\")\n",
        "print(f\"  Keys: {list(interviews_data[0].keys())}\")\n",
        "print(f\"  Role: {interviews_data[0]['role']}\")\n",
        "print(f\"  Interview ID: {interviews_data[0]['interview_id']}\")\n",
        "\n",
        "# Count interviews by role\n",
        "role_counts = {}\n",
        "for interview in interviews_data:\n",
        "    role = interview['role']\n",
        "    role_counts[role] = role_counts.get(role, 0) + 1\n",
        "\n",
        "print(f\"\\nInterviews by role:\")\n",
        "for role, count in sorted(role_counts.items()):\n",
        "    print(f\"  {role}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-Consistency Scoring Runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting self-consistency scoring with gpt-4o\n",
            "K = 7 samples per interview\n",
            "Total interviews: 250\n",
            "Expected API calls: 1750 scoring + 250 rewrite\n",
            "======================================================================\n",
            "Progress: 10/250 interviews | Cost so far: $0.2990\n",
            "Progress: 20/250 interviews | Cost so far: $0.5911\n",
            "Progress: 30/250 interviews | Cost so far: $0.9033\n",
            "Progress: 40/250 interviews | Cost so far: $1.2305\n",
            "Progress: 50/250 interviews | Cost so far: $1.5630\n",
            "Progress: 60/250 interviews | Cost so far: $1.7965\n",
            "Progress: 70/250 interviews | Cost so far: $2.0509\n",
            "Progress: 80/250 interviews | Cost so far: $2.2808\n",
            "Progress: 90/250 interviews | Cost so far: $2.5406\n",
            "Progress: 100/250 interviews | Cost so far: $2.7821\n",
            "Progress: 110/250 interviews | Cost so far: $3.0928\n",
            "Progress: 120/250 interviews | Cost so far: $3.4358\n",
            "Progress: 130/250 interviews | Cost so far: $3.8013\n",
            "Progress: 140/250 interviews | Cost so far: $4.1438\n",
            "Progress: 150/250 interviews | Cost so far: $4.4632\n",
            "Progress: 160/250 interviews | Cost so far: $4.8187\n",
            "Progress: 170/250 interviews | Cost so far: $5.1568\n",
            "Progress: 180/250 interviews | Cost so far: $5.5287\n",
            "Progress: 190/250 interviews | Cost so far: $5.8928\n",
            "Progress: 200/250 interviews | Cost so far: $6.2265\n",
            "Progress: 210/250 interviews | Cost so far: $6.5661\n",
            "Progress: 220/250 interviews | Cost so far: $6.9013\n",
            "Progress: 230/250 interviews | Cost so far: $7.2256\n",
            "Progress: 240/250 interviews | Cost so far: $7.5604\n",
            "Progress: 250/250 interviews | Cost so far: $7.9131\n",
            "\n",
            "======================================================================\n",
            "Scoring complete! Generated 1750 samples\n",
            "Total scoring cost: $7.9131\n"
          ]
        }
      ],
      "source": [
        "# Initialize results storage\n",
        "all_samples = []\n",
        "cost_tracker = {\n",
        "    \"total_cost\": 0.0,\n",
        "    \"total_input_tokens\": 0,\n",
        "    \"total_output_tokens\": 0,\n",
        "    \"scoring_cost\": 0.0,\n",
        "    \"rewrite_cost\": 0.0,\n",
        "    \"num_scoring_calls\": 0,\n",
        "    \"num_rewrite_calls\": 0,\n",
        "}\n",
        "\n",
        "print(f\"Starting self-consistency scoring with {CURRENT_MODEL}\")\n",
        "print(f\"K = {K_SAMPLES} samples per interview\")\n",
        "print(f\"Total interviews: {len(interviews_data)}\")\n",
        "print(f\"Expected API calls: {len(interviews_data) * K_SAMPLES} scoring + {len(interviews_data)} rewrite\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Process each interview\n",
        "for idx, interview in enumerate(interviews_data):\n",
        "    interview_id = interview['interview_id']\n",
        "    role = interview['role']\n",
        "    full_transcript = interview['full_transcript']\n",
        "    \n",
        "    # Generate K scoring samples\n",
        "    for k in range(K_SAMPLES):\n",
        "        try:\n",
        "            scores, raw_text, metadata = call_scoring(\n",
        "                full_transcript=full_transcript,\n",
        "                role=role,\n",
        "                model=CURRENT_MODEL,\n",
        "                temperature=0.0\n",
        "            )\n",
        "            \n",
        "            # Store sample result\n",
        "            sample_record = {\n",
        "                \"interview_id\": interview_id,\n",
        "                \"role\": role,\n",
        "                \"run_idx\": k,\n",
        "                \"model\": CURRENT_MODEL,\n",
        "                **{METRIC_ABBREV[m]: scores.get(m, 5) for m in METRICS},\n",
        "                \"latency_ms\": metadata[\"latency_ms\"],\n",
        "                \"input_tokens\": metadata[\"input_tokens\"],\n",
        "                \"output_tokens\": metadata[\"output_tokens\"],\n",
        "                \"cost\": metadata[\"cost\"],\n",
        "            }\n",
        "            all_samples.append(sample_record)\n",
        "            \n",
        "            # Update cost tracker\n",
        "            cost_tracker[\"total_cost\"] += metadata[\"cost\"]\n",
        "            cost_tracker[\"total_input_tokens\"] += metadata[\"input_tokens\"]\n",
        "            cost_tracker[\"total_output_tokens\"] += metadata[\"output_tokens\"]\n",
        "            cost_tracker[\"scoring_cost\"] += metadata[\"cost\"]\n",
        "            cost_tracker[\"num_scoring_calls\"] += 1\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error scoring interview {interview_id}, run {k}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    # Progress update\n",
        "    if (idx + 1) % 10 == 0 or (idx + 1) == len(interviews_data):\n",
        "        print(f\"Progress: {idx + 1}/{len(interviews_data)} interviews | \"\n",
        "              f\"Cost so far: ${cost_tracker['total_cost']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"Scoring complete! Generated {len(all_samples)} samples\")\n",
        "print(f\"Total scoring cost: ${cost_tracker['scoring_cost']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Aggregate Scores (Self-Consistency)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Aggregated 250 interviews\n",
            "\n",
            "Sample aggregated record:\n",
            "                          interview_id                             role  \\\n",
            "0  customer_service_representative_001  Customer Service Representative   \n",
            "1  customer_service_representative_002  Customer Service Representative   \n",
            "\n",
            "    model  num_samples  ca_score ca_confidence  ca_std  exp_score  \\\n",
            "0  gpt-4o            7       4.0          high     0.0        3.0   \n",
            "1  gpt-4o            7       3.0          high     0.0        2.0   \n",
            "\n",
            "  exp_confidence  exp_std  ...  prof_score prof_confidence  prof_std  \\\n",
            "0           high      0.0  ...         5.0            high       0.0   \n",
            "1           high      0.0  ...         4.0            high       0.0   \n",
            "\n",
            "   comm_score comm_confidence  comm_std  overall_weighted_score  \\\n",
            "0         4.0            high  0.000000                    3.90   \n",
            "1         3.0            high  0.451754                    2.75   \n",
            "\n",
            "  avg_latency_ms  total_cost  avg_cost_per_sample  \n",
            "0     993.438312    0.028770             0.004110  \n",
            "1     810.102122    0.025777             0.003682  \n",
            "\n",
            "[2 rows x 26 columns]\n"
          ]
        }
      ],
      "source": [
        "# Convert samples to DataFrame\n",
        "samples_df = pd.DataFrame(all_samples)\n",
        "\n",
        "# Aggregate scores for each interview\n",
        "aggregated_results = []\n",
        "\n",
        "for interview_id in samples_df['interview_id'].unique():\n",
        "    interview_samples = samples_df[samples_df['interview_id'] == interview_id]\n",
        "    \n",
        "    agg_record = {\n",
        "        \"interview_id\": interview_id,\n",
        "        \"role\": interview_samples.iloc[0]['role'],\n",
        "        \"model\": CURRENT_MODEL,\n",
        "        \"num_samples\": len(interview_samples),\n",
        "    }\n",
        "    \n",
        "    # Aggregate each metric\n",
        "    for abbrev in METRIC_ABBREV.values():\n",
        "        values = interview_samples[abbrev].tolist()\n",
        "        \n",
        "        # Median score (self-consistency)\n",
        "        agg_record[f\"{abbrev}_score\"] = np.median(values)\n",
        "        \n",
        "        # Confidence based on IQR\n",
        "        agg_record[f\"{abbrev}_confidence\"] = iqr_confidence(values)\n",
        "        \n",
        "        # Standard deviation\n",
        "        agg_record[f\"{abbrev}_std\"] = np.std(values)\n",
        "    \n",
        "    # Compute overall weighted score\n",
        "    metric_scores = {}\n",
        "    for metric, abbrev in METRIC_ABBREV.items():\n",
        "        metric_scores[metric] = agg_record[f\"{abbrev}_score\"]\n",
        "    \n",
        "    agg_record[\"overall_weighted_score\"] = compute_overall_weighted(metric_scores)\n",
        "    \n",
        "    # Cost statistics\n",
        "    agg_record[\"avg_latency_ms\"] = interview_samples[\"latency_ms\"].mean()\n",
        "    agg_record[\"total_cost\"] = interview_samples[\"cost\"].sum()\n",
        "    agg_record[\"avg_cost_per_sample\"] = interview_samples[\"cost\"].mean()\n",
        "    \n",
        "    aggregated_results.append(agg_record)\n",
        "\n",
        "aggregated_df = pd.DataFrame(aggregated_results)\n",
        "\n",
        "print(f\"Aggregated {len(aggregated_df)} interviews\")\n",
        "print(f\"\\nSample aggregated record:\")\n",
        "print(aggregated_df.head(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Justifications (Rewrite Step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating justifications for aggregated scores...\n",
            "======================================================================\n",
            "Progress: 10/250 justifications | Total cost: $8.0040\n",
            "Progress: 20/250 justifications | Total cost: $8.0955\n",
            "Progress: 30/250 justifications | Total cost: $8.1907\n",
            "Progress: 40/250 justifications | Total cost: $8.2862\n",
            "Progress: 50/250 justifications | Total cost: $8.3837\n",
            "Progress: 60/250 justifications | Total cost: $8.4652\n",
            "Progress: 70/250 justifications | Total cost: $8.5500\n",
            "Progress: 80/250 justifications | Total cost: $8.6329\n",
            "Progress: 90/250 justifications | Total cost: $8.7171\n",
            "Progress: 100/250 justifications | Total cost: $8.7999\n",
            "Progress: 110/250 justifications | Total cost: $8.8929\n",
            "Progress: 120/250 justifications | Total cost: $8.9902\n",
            "Progress: 130/250 justifications | Total cost: $9.0924\n",
            "Progress: 140/250 justifications | Total cost: $9.1893\n",
            "Progress: 150/250 justifications | Total cost: $9.2847\n",
            "Progress: 160/250 justifications | Total cost: $9.3866\n",
            "Progress: 170/250 justifications | Total cost: $9.4853\n",
            "Progress: 180/250 justifications | Total cost: $9.5879\n",
            "Progress: 190/250 justifications | Total cost: $9.6898\n",
            "Progress: 200/250 justifications | Total cost: $9.7868\n",
            "Progress: 210/250 justifications | Total cost: $9.8848\n",
            "Progress: 220/250 justifications | Total cost: $9.9822\n",
            "Progress: 230/250 justifications | Total cost: $10.0786\n",
            "Progress: 240/250 justifications | Total cost: $10.1759\n",
            "Progress: 250/250 justifications | Total cost: $10.2779\n",
            "\n",
            "======================================================================\n",
            "Justification generation complete!\n",
            "Final results: 250 interviews\n",
            "Rewrite cost: $2.3647\n"
          ]
        }
      ],
      "source": [
        "print(\"Generating justifications for aggregated scores...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "final_results = []\n",
        "\n",
        "for idx, agg_row in aggregated_df.iterrows():\n",
        "    interview_id = agg_row['interview_id']\n",
        "    \n",
        "    # Find original interview\n",
        "    interview = next(i for i in interviews_data if i['interview_id'] == interview_id)\n",
        "    \n",
        "    # Prepare scores for rewrite\n",
        "    locked_scores = {}\n",
        "    for metric, abbrev in METRIC_ABBREV.items():\n",
        "        locked_scores[metric] = int(round(agg_row[f\"{abbrev}_score\"]))\n",
        "    \n",
        "    try:\n",
        "        rewrite_result, raw_text, metadata = call_rewrite(\n",
        "            full_transcript=interview['full_transcript'],\n",
        "            role=interview['role'],\n",
        "            scores=locked_scores,\n",
        "            model=CURRENT_MODEL\n",
        "        )\n",
        "        \n",
        "        # Build final record\n",
        "        final_record = {\n",
        "            \"interview_id\": interview_id,\n",
        "            \"role\": interview['role'],\n",
        "            \"model\": CURRENT_MODEL,\n",
        "            **{f\"{abbrev}_score\": agg_row[f\"{abbrev}_score\"] for abbrev in METRIC_ABBREV.values()},\n",
        "            **{f\"{abbrev}_confidence\": agg_row[f\"{abbrev}_confidence\"] for abbrev in METRIC_ABBREV.values()},\n",
        "            \"overall_weighted_score\": agg_row[\"overall_weighted_score\"],\n",
        "            # Justifications\n",
        "            **{f\"{metric}_justification\": rewrite_result.get(f\"{metric}_justification\", \"\") \n",
        "               for metric in METRICS},\n",
        "            \"general_strengths\": rewrite_result.get(\"general_strengths\", \"\"),\n",
        "            \"general_weaknesses\": rewrite_result.get(\"general_weaknesses\", \"\"),\n",
        "            \"general_summary\": rewrite_result.get(\"general_summary\", \"\"),\n",
        "            # Cost tracking\n",
        "            \"scoring_cost\": agg_row[\"total_cost\"],\n",
        "            \"rewrite_cost\": metadata[\"cost\"],\n",
        "            \"total_cost\": agg_row[\"total_cost\"] + metadata[\"cost\"],\n",
        "            \"rewrite_latency_ms\": metadata[\"latency_ms\"],\n",
        "            \"rewrite_input_tokens\": metadata[\"input_tokens\"],\n",
        "            \"rewrite_output_tokens\": metadata[\"output_tokens\"],\n",
        "        }\n",
        "        \n",
        "        final_results.append(final_record)\n",
        "        \n",
        "        # Update cost tracker\n",
        "        cost_tracker[\"total_cost\"] += metadata[\"cost\"]\n",
        "        cost_tracker[\"total_input_tokens\"] += metadata[\"input_tokens\"]\n",
        "        cost_tracker[\"total_output_tokens\"] += metadata[\"output_tokens\"]\n",
        "        cost_tracker[\"rewrite_cost\"] += metadata[\"cost\"]\n",
        "        cost_tracker[\"num_rewrite_calls\"] += 1\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error rewriting interview {interview_id}: {e}\")\n",
        "        continue\n",
        "    \n",
        "    # Progress update\n",
        "    if (idx + 1) % 10 == 0 or (idx + 1) == len(aggregated_df):\n",
        "        print(f\"Progress: {idx + 1}/{len(aggregated_df)} justifications | \"\n",
        "              f\"Total cost: ${cost_tracker['total_cost']:.4f}\")\n",
        "\n",
        "final_df = pd.DataFrame(final_results)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"Justification generation complete!\")\n",
        "print(f\"Final results: {len(final_df)} interviews\")\n",
        "print(f\"Rewrite cost: ${cost_tracker['rewrite_cost']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cost Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "COST ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "Model: gpt-4o\n",
            "Provider: openai\n",
            "\n",
            "Pricing:\n",
            "  Input: $2.50 per 1M tokens\n",
            "  Output: $10.00 per 1M tokens\n",
            "\n",
            "API Calls:\n",
            "  Scoring calls: 1750\n",
            "  Rewrite calls: 250\n",
            "  Total calls: 2000\n",
            "\n",
            "Token Usage:\n",
            "  Total input tokens: 3,359,682\n",
            "  Total output tokens: 187,866\n",
            "  Total tokens: 3,547,548\n",
            "\n",
            "Cost Breakdown:\n",
            "  Scoring phase: $7.9131\n",
            "  Rewrite phase: $2.3647\n",
            "  TOTAL COST: $10.2779\n",
            "\n",
            "Per-Interview Averages:\n",
            "  Cost per interview: $0.0411\n",
            "  Scoring cost per interview: $0.0317\n",
            "  Rewrite cost per interview: $0.0095\n",
            "\n",
            "Per-Sample Averages:\n",
            "  Cost per scoring sample: $0.00452\n",
            "  Tokens per scoring sample: 1920 input, 107 output\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"COST ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nModel: {CURRENT_MODEL}\")\n",
        "print(f\"Provider: {MODELS_CONFIG[CURRENT_MODEL]['provider']}\")\n",
        "\n",
        "print(f\"\\nPricing:\")\n",
        "print(f\"  Input: ${MODELS_CONFIG[CURRENT_MODEL]['input_cost_per_1m']:.2f} per 1M tokens\")\n",
        "print(f\"  Output: ${MODELS_CONFIG[CURRENT_MODEL]['output_cost_per_1m']:.2f} per 1M tokens\")\n",
        "\n",
        "print(f\"\\nAPI Calls:\")\n",
        "print(f\"  Scoring calls: {cost_tracker['num_scoring_calls']}\")\n",
        "print(f\"  Rewrite calls: {cost_tracker['num_rewrite_calls']}\")\n",
        "print(f\"  Total calls: {cost_tracker['num_scoring_calls'] + cost_tracker['num_rewrite_calls']}\")\n",
        "\n",
        "print(f\"\\nToken Usage:\")\n",
        "print(f\"  Total input tokens: {cost_tracker['total_input_tokens']:,}\")\n",
        "print(f\"  Total output tokens: {cost_tracker['total_output_tokens']:,}\")\n",
        "print(f\"  Total tokens: {cost_tracker['total_input_tokens'] + cost_tracker['total_output_tokens']:,}\")\n",
        "\n",
        "print(f\"\\nCost Breakdown:\")\n",
        "print(f\"  Scoring phase: ${cost_tracker['scoring_cost']:.4f}\")\n",
        "print(f\"  Rewrite phase: ${cost_tracker['rewrite_cost']:.4f}\")\n",
        "print(f\"  TOTAL COST: ${cost_tracker['total_cost']:.4f}\")\n",
        "\n",
        "# Per-interview averages\n",
        "num_interviews = len(final_df)\n",
        "if num_interviews > 0:\n",
        "    print(f\"\\nPer-Interview Averages:\")\n",
        "    print(f\"  Cost per interview: ${cost_tracker['total_cost'] / num_interviews:.4f}\")\n",
        "    print(f\"  Scoring cost per interview: ${cost_tracker['scoring_cost'] / num_interviews:.4f}\")\n",
        "    print(f\"  Rewrite cost per interview: ${cost_tracker['rewrite_cost'] / num_interviews:.4f}\")\n",
        "\n",
        "# Per-sample averages\n",
        "num_samples = len(samples_df)\n",
        "if num_samples > 0:\n",
        "    print(f\"\\nPer-Sample Averages:\")\n",
        "    print(f\"  Cost per scoring sample: ${cost_tracker['scoring_cost'] / num_samples:.5f}\")\n",
        "    print(f\"  Tokens per scoring sample: {cost_tracker['total_input_tokens'] / cost_tracker['num_scoring_calls']:.0f} input, \"\n",
        "          f\"{cost_tracker['total_output_tokens'] / cost_tracker['num_scoring_calls']:.0f} output\")\n",
        "\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "COST ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "Model: gpt-4o\n",
            "Provider: openai\n",
            "\n",
            "Pricing:\n",
            "  Input: $2.50 per 1M tokens\n",
            "  Output: $10.00 per 1M tokens\n",
            "\n",
            "API Calls:\n",
            "  Scoring calls: 1750\n",
            "  Rewrite calls: 250\n",
            "  Total calls: 2000\n",
            "\n",
            "Token Usage:\n",
            "  Total input tokens: 3,359,682\n",
            "  Total output tokens: 187,866\n",
            "  Total tokens: 3,547,548\n",
            "\n",
            "Cost Breakdown:\n",
            "  Scoring phase: $7.9131\n",
            "  Rewrite phase: $2.3647\n",
            "  TOTAL COST: $10.2779\n",
            "\n",
            "Per-Interview Averages:\n",
            "  Cost per interview: $0.0411\n",
            "  Scoring cost per interview: $0.0317\n",
            "  Rewrite cost per interview: $0.0095\n",
            "\n",
            "Per-Sample Averages:\n",
            "  Cost per scoring sample: $0.00452\n",
            "  Tokens per scoring sample: 1920 input, 107 output\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"COST ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nModel: {CURRENT_MODEL}\")\n",
        "print(f\"Provider: {MODELS_CONFIG[CURRENT_MODEL]['provider']}\")\n",
        "\n",
        "print(f\"\\nPricing:\")\n",
        "print(f\"  Input: ${MODELS_CONFIG[CURRENT_MODEL]['input_cost_per_1m']:.2f} per 1M tokens\")\n",
        "print(f\"  Output: ${MODELS_CONFIG[CURRENT_MODEL]['output_cost_per_1m']:.2f} per 1M tokens\")\n",
        "\n",
        "print(f\"\\nAPI Calls:\")\n",
        "print(f\"  Scoring calls: {cost_tracker['num_scoring_calls']}\")\n",
        "print(f\"  Rewrite calls: {cost_tracker['num_rewrite_calls']}\")\n",
        "print(f\"  Total calls: {cost_tracker['num_scoring_calls'] + cost_tracker['num_rewrite_calls']}\")\n",
        "\n",
        "print(f\"\\nToken Usage:\")\n",
        "print(f\"  Total input tokens: {cost_tracker['total_input_tokens']:,}\")\n",
        "print(f\"  Total output tokens: {cost_tracker['total_output_tokens']:,}\")\n",
        "print(f\"  Total tokens: {cost_tracker['total_input_tokens'] + cost_tracker['total_output_tokens']:,}\")\n",
        "\n",
        "print(f\"\\nCost Breakdown:\")\n",
        "print(f\"  Scoring phase: ${cost_tracker['scoring_cost']:.4f}\")\n",
        "print(f\"  Rewrite phase: ${cost_tracker['rewrite_cost']:.4f}\")\n",
        "print(f\"  TOTAL COST: ${cost_tracker['total_cost']:.4f}\")\n",
        "\n",
        "# Per-interview averages\n",
        "num_interviews = len(final_df)\n",
        "if num_interviews > 0:\n",
        "    print(f\"\\nPer-Interview Averages:\")\n",
        "    print(f\"  Cost per interview: ${cost_tracker['total_cost'] / num_interviews:.4f}\")\n",
        "    print(f\"  Scoring cost per interview: ${cost_tracker['scoring_cost'] / num_interviews:.4f}\")\n",
        "    print(f\"  Rewrite cost per interview: ${cost_tracker['rewrite_cost'] / num_interviews:.4f}\")\n",
        "\n",
        "# Per-sample averages\n",
        "num_samples = len(samples_df)\n",
        "if num_samples > 0:\n",
        "    print(f\"\\nPer-Sample Averages:\")\n",
        "    print(f\"  Cost per scoring sample: ${cost_tracker['scoring_cost'] / num_samples:.5f}\")\n",
        "    print(f\"  Tokens per scoring sample: {cost_tracker['total_input_tokens'] / cost_tracker['num_scoring_calls']:.0f} input, \"\n",
        "          f\"{cost_tracker['total_output_tokens'] / cost_tracker['num_scoring_calls']:.0f} output\")\n",
        "\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "EVALUATION SUMMARY\n",
            "======================================================================\n",
            "\n",
            "Dataset:\n",
            "  Total interviews: 250\n",
            "  K samples per interview: 7\n",
            "  Total scoring samples: 1750\n",
            "\n",
            "Overall Weighted Score Statistics:\n",
            "  Mean:   5.90\n",
            "  Median: 7.60\n",
            "  Std:    2.41\n",
            "  Min:    2.50\n",
            "  Max:    8.80\n",
            "\n",
            "Metric Score Averages:\n",
            "  cognitive_ability   : Mean=5.96, Median=8.0\n",
            "  experience          : Mean=5.58, Median=7.0\n",
            "  problem_solving     : Mean=5.95, Median=8.0\n",
            "  reliability         : Mean=6.23, Median=7.0\n",
            "  professionalism     : Mean=6.56, Median=8.0\n",
            "  communication       : Mean=6.56, Median=8.0\n",
            "\n",
            "Confidence Distribution:\n",
            "  cognitive_ability   : High=250 (100%), Medium=0, Low=0\n",
            "  experience          : High=250 (100%), Medium=0, Low=0\n",
            "  problem_solving     : High=249 (100%), Medium=1, Low=0\n",
            "  reliability         : High=246 (98%), Medium=4, Low=0\n",
            "  professionalism     : High=250 (100%), Medium=0, Low=0\n",
            "  communication       : High=250 (100%), Medium=0, Low=0\n",
            "\n",
            "Score Distribution by Percentile:\n",
            "  P10: 2.80\n",
            "  P25: 3.25\n",
            "  P50: 7.60\n",
            "  P75: 8.07\n",
            "  P90: 8.50\n",
            "  P95: 8.50\n",
            "  P99: 8.50\n",
            "\n",
            "Statistics by Role:\n",
            "\n",
            "  Customer Service Representative:\n",
            "    Count: 50\n",
            "    Mean score: 6.19\n",
            "    Median score: 7.62\n",
            "    Avg cost per interview: $0.0407\n",
            "\n",
            "  Sales Representative:\n",
            "    Count: 50\n",
            "    Mean score: 5.50\n",
            "    Median score: 4.33\n",
            "    Avg cost per interview: $0.0327\n",
            "\n",
            "  Field Technician:\n",
            "    Count: 50\n",
            "    Mean score: 5.51\n",
            "    Median score: 3.92\n",
            "    Avg cost per interview: $0.0433\n",
            "\n",
            "  Home Service Technician:\n",
            "    Count: 50\n",
            "    Mean score: 6.30\n",
            "    Median score: 7.65\n",
            "    Avg cost per interview: $0.0453\n",
            "\n",
            "  General Manager (Franchise):\n",
            "    Count: 50\n",
            "    Mean score: 6.01\n",
            "    Median score: 7.60\n",
            "    Avg cost per interview: $0.0436\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"EVALUATION SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nDataset:\")\n",
        "print(f\"  Total interviews: {len(final_df)}\")\n",
        "print(f\"  K samples per interview: {K_SAMPLES}\")\n",
        "print(f\"  Total scoring samples: {len(samples_df)}\")\n",
        "\n",
        "print(f\"\\nOverall Weighted Score Statistics:\")\n",
        "print(f\"  Mean:   {final_df['overall_weighted_score'].mean():.2f}\")\n",
        "print(f\"  Median: {final_df['overall_weighted_score'].median():.2f}\")\n",
        "print(f\"  Std:    {final_df['overall_weighted_score'].std():.2f}\")\n",
        "print(f\"  Min:    {final_df['overall_weighted_score'].min():.2f}\")\n",
        "print(f\"  Max:    {final_df['overall_weighted_score'].max():.2f}\")\n",
        "\n",
        "print(f\"\\nMetric Score Averages:\")\n",
        "for metric, abbrev in METRIC_ABBREV.items():\n",
        "    score_col = f\"{abbrev}_score\"\n",
        "    if score_col in final_df.columns:\n",
        "        mean_score = final_df[score_col].mean()\n",
        "        median_score = final_df[score_col].median()\n",
        "        print(f\"  {metric:20s}: Mean={mean_score:.2f}, Median={median_score:.1f}\")\n",
        "\n",
        "print(f\"\\nConfidence Distribution:\")\n",
        "for metric, abbrev in METRIC_ABBREV.items():\n",
        "    conf_col = f\"{abbrev}_confidence\"\n",
        "    if conf_col in final_df.columns:\n",
        "        high = (final_df[conf_col] == \"high\").sum()\n",
        "        med = (final_df[conf_col] == \"medium\").sum()\n",
        "        low = (final_df[conf_col] == \"low\").sum()\n",
        "        total = high + med + low\n",
        "        high_pct = 100 * high / total if total > 0 else 0\n",
        "        print(f\"  {metric:20s}: High={high} ({high_pct:.0f}%), Medium={med}, Low={low}\")\n",
        "\n",
        "print(f\"\\nScore Distribution by Percentile:\")\n",
        "percentiles = [10, 25, 50, 75, 90, 95, 99]\n",
        "for p in percentiles:\n",
        "    val = final_df['overall_weighted_score'].quantile(p/100)\n",
        "    print(f\"  P{p:2d}: {val:.2f}\")\n",
        "\n",
        "# Statistics by role\n",
        "print(f\"\\nStatistics by Role:\")\n",
        "for role in final_df['role'].unique():\n",
        "    role_df = final_df[final_df['role'] == role]\n",
        "    print(f\"\\n  {role}:\")\n",
        "    print(f\"    Count: {len(role_df)}\")\n",
        "    print(f\"    Mean score: {role_df['overall_weighted_score'].mean():.2f}\")\n",
        "    print(f\"    Median score: {role_df['overall_weighted_score'].median():.2f}\")\n",
        "    print(f\"    Avg cost per interview: ${role_df['total_cost'].mean():.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "EVALUATION SUMMARY\n",
            "======================================================================\n",
            "\n",
            "Dataset:\n",
            "  Total interviews: 250\n",
            "  K samples per interview: 7\n",
            "  Total scoring samples: 1750\n",
            "\n",
            "Overall Weighted Score Statistics:\n",
            "  Mean:   5.90\n",
            "  Median: 7.60\n",
            "  Std:    2.41\n",
            "  Min:    2.50\n",
            "  Max:    8.80\n",
            "\n",
            "Metric Score Averages:\n",
            "  cognitive_ability   : Mean=5.96, Median=8.0\n",
            "  experience          : Mean=5.58, Median=7.0\n",
            "  problem_solving     : Mean=5.95, Median=8.0\n",
            "  reliability         : Mean=6.23, Median=7.0\n",
            "  professionalism     : Mean=6.56, Median=8.0\n",
            "  communication       : Mean=6.56, Median=8.0\n",
            "\n",
            "Confidence Distribution:\n",
            "  cognitive_ability   : High=250 (100%), Medium=0, Low=0\n",
            "  experience          : High=250 (100%), Medium=0, Low=0\n",
            "  problem_solving     : High=249 (100%), Medium=1, Low=0\n",
            "  reliability         : High=246 (98%), Medium=4, Low=0\n",
            "  professionalism     : High=250 (100%), Medium=0, Low=0\n",
            "  communication       : High=250 (100%), Medium=0, Low=0\n",
            "\n",
            "Score Distribution by Percentile:\n",
            "  P10: 2.80\n",
            "  P25: 3.25\n",
            "  P50: 7.60\n",
            "  P75: 8.07\n",
            "  P90: 8.50\n",
            "  P95: 8.50\n",
            "  P99: 8.50\n",
            "\n",
            "Statistics by Role:\n",
            "\n",
            "  Customer Service Representative:\n",
            "    Count: 50\n",
            "    Mean score: 6.19\n",
            "    Median score: 7.62\n",
            "    Avg cost per interview: $0.0407\n",
            "\n",
            "  Sales Representative:\n",
            "    Count: 50\n",
            "    Mean score: 5.50\n",
            "    Median score: 4.33\n",
            "    Avg cost per interview: $0.0327\n",
            "\n",
            "  Field Technician:\n",
            "    Count: 50\n",
            "    Mean score: 5.51\n",
            "    Median score: 3.92\n",
            "    Avg cost per interview: $0.0433\n",
            "\n",
            "  Home Service Technician:\n",
            "    Count: 50\n",
            "    Mean score: 6.30\n",
            "    Median score: 7.65\n",
            "    Avg cost per interview: $0.0453\n",
            "\n",
            "  General Manager (Franchise):\n",
            "    Count: 50\n",
            "    Mean score: 6.01\n",
            "    Median score: 7.60\n",
            "    Avg cost per interview: $0.0436\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"EVALUATION SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nDataset:\")\n",
        "print(f\"  Total interviews: {len(final_df)}\")\n",
        "print(f\"  K samples per interview: {K_SAMPLES}\")\n",
        "print(f\"  Total scoring samples: {len(samples_df)}\")\n",
        "\n",
        "print(f\"\\nOverall Weighted Score Statistics:\")\n",
        "print(f\"  Mean:   {final_df['overall_weighted_score'].mean():.2f}\")\n",
        "print(f\"  Median: {final_df['overall_weighted_score'].median():.2f}\")\n",
        "print(f\"  Std:    {final_df['overall_weighted_score'].std():.2f}\")\n",
        "print(f\"  Min:    {final_df['overall_weighted_score'].min():.2f}\")\n",
        "print(f\"  Max:    {final_df['overall_weighted_score'].max():.2f}\")\n",
        "\n",
        "print(f\"\\nMetric Score Averages:\")\n",
        "for metric, abbrev in METRIC_ABBREV.items():\n",
        "    score_col = f\"{abbrev}_score\"\n",
        "    if score_col in final_df.columns:\n",
        "        mean_score = final_df[score_col].mean()\n",
        "        median_score = final_df[score_col].median()\n",
        "        print(f\"  {metric:20s}: Mean={mean_score:.2f}, Median={median_score:.1f}\")\n",
        "\n",
        "print(f\"\\nConfidence Distribution:\")\n",
        "for metric, abbrev in METRIC_ABBREV.items():\n",
        "    conf_col = f\"{abbrev}_confidence\"\n",
        "    if conf_col in final_df.columns:\n",
        "        high = (final_df[conf_col] == \"high\").sum()\n",
        "        med = (final_df[conf_col] == \"medium\").sum()\n",
        "        low = (final_df[conf_col] == \"low\").sum()\n",
        "        total = high + med + low\n",
        "        high_pct = 100 * high / total if total > 0 else 0\n",
        "        print(f\"  {metric:20s}: High={high} ({high_pct:.0f}%), Medium={med}, Low={low}\")\n",
        "\n",
        "print(f\"\\nScore Distribution by Percentile:\")\n",
        "percentiles = [10, 25, 50, 75, 90, 95, 99]\n",
        "for p in percentiles:\n",
        "    val = final_df['overall_weighted_score'].quantile(p/100)\n",
        "    print(f\"  P{p:2d}: {val:.2f}\")\n",
        "\n",
        "# Statistics by role\n",
        "print(f\"\\nStatistics by Role:\")\n",
        "for role in final_df['role'].unique():\n",
        "    role_df = final_df[final_df['role'] == role]\n",
        "    print(f\"\\n  {role}:\")\n",
        "    print(f\"    Count: {len(role_df)}\")\n",
        "    print(f\"    Mean score: {role_df['overall_weighted_score'].mean():.2f}\")\n",
        "    print(f\"    Median score: {role_df['overall_weighted_score'].median():.2f}\")\n",
        "    print(f\"    Avg cost per interview: ${role_df['total_cost'].mean():.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Results saved successfully!\n",
            "\n",
            "Files saved:\n",
            "  - /Users/shreya_sudan/Desktop/ServiceAgent/CleanAgent/metaPromptOpt/data/k7RunsResults/samples_gpt-4o_20251120-163437.csv\n",
            "  - /Users/shreya_sudan/Desktop/ServiceAgent/CleanAgent/metaPromptOpt/data/k7RunsResults/aggregated_gpt-4o_20251120-163437.csv\n",
            "  - /Users/shreya_sudan/Desktop/ServiceAgent/CleanAgent/metaPromptOpt/data/k7RunsResults/final_gpt-4o_20251120-163437.csv\n",
            "  - /Users/shreya_sudan/Desktop/ServiceAgent/CleanAgent/metaPromptOpt/data/k7RunsResults/cost_report_gpt-4o_20251120-163437.json\n",
            "\n",
            "Total cost for this run: $10.2779\n"
          ]
        }
      ],
      "source": [
        "# Create timestamp for file naming\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "model_safe = CURRENT_MODEL.replace(\"/\", \"_\").replace(\":\", \"_\")\n",
        "\n",
        "# Save directory\n",
        "save_dir = \"/Users/shreya_sudan/Desktop/ServiceAgent/CleanAgent/metaPromptOpt/data/k7RunsResults\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save files\n",
        "samples_path = f\"{save_dir}/samples_{model_safe}_{timestamp}.csv\"\n",
        "aggregated_path = f\"{save_dir}/aggregated_{model_safe}_{timestamp}.csv\"\n",
        "final_path = f\"{save_dir}/final_{model_safe}_{timestamp}.csv\"\n",
        "cost_path = f\"{save_dir}/cost_report_{model_safe}_{timestamp}.json\"\n",
        "\n",
        "samples_df.to_csv(samples_path, index=False)\n",
        "aggregated_df.to_csv(aggregated_path, index=False)\n",
        "final_df.to_csv(final_path, index=False)\n",
        "\n",
        "# Save cost report\n",
        "cost_report = {\n",
        "    \"model\": CURRENT_MODEL,\n",
        "    \"timestamp\": timestamp,\n",
        "    \"k_samples\": K_SAMPLES,\n",
        "    \"num_interviews\": len(final_df),\n",
        "    \"num_samples\": len(samples_df),\n",
        "    **cost_tracker,\n",
        "    \"cost_per_interview\": cost_tracker['total_cost'] / len(final_df) if len(final_df) > 0 else 0,\n",
        "    \"cost_per_sample\": cost_tracker['scoring_cost'] / len(samples_df) if len(samples_df) > 0 else 0,\n",
        "}\n",
        "\n",
        "with open(cost_path, 'w') as f:\n",
        "    json.dump(cost_report, f, indent=2)\n",
        "\n",
        "print(\"✓ Results saved successfully!\")\n",
        "print(f\"\\nFiles saved:\")\n",
        "print(f\"  - {samples_path}\")\n",
        "print(f\"  - {aggregated_path}\")\n",
        "print(f\"  - {final_path}\")\n",
        "print(f\"  - {cost_path}\")\n",
        "print(f\"\\nTotal cost for this run: ${cost_tracker['total_cost']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset configuration\n",
        "DATASET_PATH = \"/content/drive/MyDrive/mvp/synthetic_interview_dataset.json\"\n",
        "K_SAMPLES = 3  # Number of self-consistency samples per interview\n",
        "\n",
        "# Models to evaluate\n",
        "MODELS_CONFIG = {\n",
        "    # OpenAI models\n",
        "    \"gpt-4o\": {\n",
        "        \"provider\": \"openai\",\n",
        "        \"input_cost_per_1m\": 2.50,  # $2.50 per 1M input tokens\n",
        "        \"output_cost_per_1m\": 10.00,  # $10.00 per 1M output tokens\n",
        "        \"temperature\": 0.0,\n",
        "        \"max_tokens\": 512\n",
        "    },\n",
        "    \"gpt-3.5-turbo\": {\n",
        "        \"provider\": \"openai\",\n",
        "        \"input_cost_per_1m\": 0.50,  # $0.50 per 1M input tokens\n",
        "        \"output_cost_per_1m\": 1.50,  # $1.50 per 1M output tokens\n",
        "        \"temperature\": 0.0,\n",
        "        \"max_tokens\": 512\n",
        "    },\n",
        "    # Llama via Hugging Face Inference API\n",
        "    \"meta-llama/Llama-3.1-8B-Instruct\": {\n",
        "        \"provider\": \"huggingface\",\n",
        "        \"input_cost_per_1m\": 0.00,  # Free with HF Pro subscription\n",
        "        \"output_cost_per_1m\": 0.00,  # Free with HF Pro subscription\n",
        "        \"temperature\": 0.0,\n",
        "        \"max_tokens\": 512\n",
        "    },\n",
        "}\n",
        "\n",
        "# Scoring weights and metrics\n",
        "WEIGHTS = {\n",
        "    \"cognitive_ability\": 0.35,\n",
        "    \"experience\": 0.35,\n",
        "    \"problem_solving\": 0.15,\n",
        "    \"reliability\": 0.05,\n",
        "    \"professionalism\": 0.05,\n",
        "    \"communication\": 0.05\n",
        "}\n",
        "\n",
        "METRICS = list(WEIGHTS.keys())\n",
        "METRIC_ABBREV = {\n",
        "    \"cognitive_ability\": \"ca\",\n",
        "    \"experience\": \"exp\",\n",
        "    \"problem_solving\": \"ps\",\n",
        "    \"reliability\": \"rel\",\n",
        "    \"professionalism\": \"prof\",\n",
        "    \"communication\": \"comm\"\n",
        "}\n",
        "\n",
        "# Model to run (change this to run different models)\n",
        "CURRENT_MODEL = \"gpt-3.5-turbo\"  # Options: gpt-4o, gpt-3.5-turbo, meta-llama/Llama-3.1-8B-Instruct\n",
        "\n",
        "print(f\"Configuration loaded for model: {CURRENT_MODEL}\")\n",
        "print(f\"Dataset path: {DATASET_PATH}\")\n",
        "print(f\"K samples per interview: {K_SAMPLES}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "MULTI-MODEL COST COMPARISON\n",
            "======================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>num_interviews</th>\n",
              "      <th>total_cost</th>\n",
              "      <th>cost_per_interview</th>\n",
              "      <th>scoring_cost</th>\n",
              "      <th>rewrite_cost</th>\n",
              "      <th>total_input_tokens</th>\n",
              "      <th>total_output_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>meta-llama/Llama-3.1-8B-Instruct:novita</td>\n",
              "      <td>250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3467268</td>\n",
              "      <td>190879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>250</td>\n",
              "      <td>1.995815</td>\n",
              "      <td>0.007983</td>\n",
              "      <td>1.623433</td>\n",
              "      <td>0.372382</td>\n",
              "      <td>3411268</td>\n",
              "      <td>193454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gpt-4o</td>\n",
              "      <td>250</td>\n",
              "      <td>10.277865</td>\n",
              "      <td>0.041111</td>\n",
              "      <td>7.913133</td>\n",
              "      <td>2.364732</td>\n",
              "      <td>3359682</td>\n",
              "      <td>187866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>gpt-4o</td>\n",
              "      <td>250</td>\n",
              "      <td>10.277865</td>\n",
              "      <td>0.041111</td>\n",
              "      <td>7.913133</td>\n",
              "      <td>2.364732</td>\n",
              "      <td>3359682</td>\n",
              "      <td>187866</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     model  num_interviews  total_cost  \\\n",
              "2  meta-llama/Llama-3.1-8B-Instruct:novita             250    0.000000   \n",
              "1                            gpt-3.5-turbo             250    1.995815   \n",
              "0                                   gpt-4o             250   10.277865   \n",
              "3                                   gpt-4o             250   10.277865   \n",
              "\n",
              "   cost_per_interview  scoring_cost  rewrite_cost  total_input_tokens  \\\n",
              "2            0.000000      0.000000      0.000000             3467268   \n",
              "1            0.007983      1.623433      0.372382             3411268   \n",
              "0            0.041111      7.913133      2.364732             3359682   \n",
              "3            0.041111      7.913133      2.364732             3359682   \n",
              "\n",
              "   total_output_tokens  \n",
              "2               190879  \n",
              "1               193454  \n",
              "0               187866  \n",
              "3               187866  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Cost Rankings (250 interviews):\n",
            "  meta-llama/Llama-3.1-8B-Instruct:novita : $0.00 total, $0.0000 per interview\n",
            "  gpt-3.5-turbo                           : $2.00 total, $0.0080 per interview\n",
            "  gpt-4o                                  : $10.28 total, $0.0411 per interview\n",
            "  gpt-4o                                  : $10.28 total, $0.0411 per interview\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "\n",
        "# Load all cost reports\n",
        "cost_reports = []\n",
        "for cost_file in glob.glob(f\"{save_dir}/cost_report_*.json\"):\n",
        "    with open(cost_file, 'r') as f:\n",
        "        cost_reports.append(json.load(f))\n",
        "\n",
        "if cost_reports:\n",
        "    cost_comparison_df = pd.DataFrame(cost_reports)\n",
        "    \n",
        "    print(\"=\" * 70)\n",
        "    print(\"MULTI-MODEL COST COMPARISON\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    display(cost_comparison_df[[\n",
        "        'model',\n",
        "        'num_interviews',\n",
        "        'total_cost',\n",
        "        'cost_per_interview',\n",
        "        'scoring_cost',\n",
        "        'rewrite_cost',\n",
        "        'total_input_tokens',\n",
        "        'total_output_tokens'\n",
        "    ]].sort_values('total_cost'))\n",
        "    \n",
        "    print(\"\\nCost Rankings (250 interviews):\")\n",
        "    for idx, row in cost_comparison_df.sort_values('total_cost').iterrows():\n",
        "        print(f\"  {row['model']:40s}: ${row['total_cost']:.2f} total, ${row['cost_per_interview']:.4f} per interview\")\n",
        "else:\n",
        "    print(\"No cost reports found for comparison\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
